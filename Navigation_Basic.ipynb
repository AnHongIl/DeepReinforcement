{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Start the environment\n",
    "- Prepare unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Get the default brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 37\n",
    "action_size = 4\n",
    "learning_rate = 0.001\n",
    "hidden_size = 256   \n",
    "\n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 100000            # memory capacity\n",
    "batch_size = 64                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, name='main'):\n",
    "        with tf.device(\"/device:GPU:0\"):\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            self.target = tf.placeholder(tf.float32, [None], name='target')\n",
    "            self.actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            self.one_hot_actions = tf.one_hot(self.actions, action_size)\n",
    "            \n",
    "            self.hidden = tf.layers.dense(inputs=self.inputs, units=hidden_size, activation=tf.nn.relu, name='hidden')\n",
    "            self.output = tf.layers.dense(inputs=self.hidden, units=action_size, activation=None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.one_hot_actions))\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target - self.Q))\n",
    "            #self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.opt = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.buffer.maxlen:\n",
    "            self.buffer.popleft()         \n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Pretrain in the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "for i in range(pretrain_length):    \n",
    "    action = np.random.choice(action_size)\n",
    "    \n",
    "    env_info = env.step(action)[brain_name]\n",
    "    \n",
    "    #Observe reward and next state\n",
    "    reward = env_info.rewards[0]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    done = env_info.local_done[0]                    \n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(37)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        continue\n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "    #S <- S'\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, step, state):    \n",
    "    explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "    if explore_p > np.random.rand():\n",
    "        action = np.random.choice(action_size)\n",
    "    else:\n",
    "        next_Q = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: state.reshape(1, state_size)})\n",
    "        action = np.asscalar(np.argmax(next_Q[0]))\n",
    "    return action, explore_p\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    tf.reset_default_graph()\n",
    "    mainDQN = DQNetwork(name=\"main\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 2.0 Training sum: 1.00 Training loss: 13.769 Explore P: 0.9970\n",
      "Episode: 1 Total reward: 1.0 Training sum: 0.00 Training loss: 0.188 Explore P: 0.9941\n",
      "Episode: 2 Total reward: 0.0 Training sum: 1.00 Training loss: 3.125 Explore P: 0.9911\n",
      "Episode: 3 Total reward: 0.0 Training sum: 0.00 Training loss: 2.828 Explore P: 0.9882\n",
      "Episode: 4 Total reward: -1.0 Training sum: 0.00 Training loss: 1.758 Explore P: 0.9853\n",
      "Episode: 5 Total reward: -2.0 Training sum: 0.00 Training loss: 3.010 Explore P: 0.9823\n",
      "Episode: 6 Total reward: 2.0 Training sum: 0.00 Training loss: 0.690 Explore P: 0.9794\n",
      "Episode: 7 Total reward: -1.0 Training sum: -1.00 Training loss: 0.753 Explore P: 0.9765\n",
      "Episode: 8 Total reward: 0.0 Training sum: 0.00 Training loss: 0.676 Explore P: 0.9736\n",
      "Episode: 9 Total reward: 1.0 Training sum: -1.00 Training loss: 2.126 Explore P: 0.9707\n",
      "Episode: 10 Total reward: 1.0 Training sum: 0.00 Training loss: 0.443 Explore P: 0.9679\n",
      "Episode: 11 Total reward: -1.0 Training sum: 0.00 Training loss: 0.600 Explore P: 0.9650\n",
      "Episode: 12 Total reward: -1.0 Training sum: 0.00 Training loss: 0.058 Explore P: 0.9621\n",
      "Episode: 13 Total reward: 0.0 Training sum: 0.00 Training loss: 2.471 Explore P: 0.9593\n",
      "Episode: 14 Total reward: -2.0 Training sum: -1.00 Training loss: 0.106 Explore P: 0.9564\n",
      "Episode: 15 Total reward: 2.0 Training sum: 0.00 Training loss: 0.021 Explore P: 0.9536\n",
      "Episode: 16 Total reward: 0.0 Training sum: 0.00 Training loss: 0.003 Explore P: 0.9508\n",
      "Episode: 17 Total reward: 1.0 Training sum: 1.00 Training loss: 0.062 Explore P: 0.9480\n",
      "Episode: 18 Total reward: 0.0 Training sum: 1.00 Training loss: 0.378 Explore P: 0.9451\n",
      "Episode: 19 Total reward: 0.0 Training sum: -1.00 Training loss: 0.825 Explore P: 0.9423\n",
      "Episode: 20 Total reward: 0.0 Training sum: 0.00 Training loss: 0.249 Explore P: 0.9396\n",
      "Episode: 21 Total reward: 0.0 Training sum: 0.00 Training loss: 0.392 Explore P: 0.9368\n",
      "Episode: 22 Total reward: 0.0 Training sum: 0.00 Training loss: 0.003 Explore P: 0.9340\n",
      "Episode: 23 Total reward: -3.0 Training sum: 0.00 Training loss: 0.058 Explore P: 0.9312\n",
      "Episode: 24 Total reward: 0.0 Training sum: 0.00 Training loss: 0.119 Explore P: 0.9285\n",
      "Episode: 25 Total reward: 0.0 Training sum: -1.00 Training loss: 0.059 Explore P: 0.9257\n",
      "Episode: 26 Total reward: -1.0 Training sum: 1.00 Training loss: 0.160 Explore P: 0.9230\n",
      "Episode: 27 Total reward: -2.0 Training sum: 0.00 Training loss: 0.006 Explore P: 0.9202\n",
      "Episode: 28 Total reward: 0.0 Training sum: 0.00 Training loss: 0.095 Explore P: 0.9175\n",
      "Episode: 29 Total reward: 0.0 Training sum: 0.00 Training loss: 0.331 Explore P: 0.9148\n",
      "Episode: 30 Total reward: 0.0 Training sum: -1.00 Training loss: 0.052 Explore P: 0.9121\n",
      "Episode: 31 Total reward: -1.0 Training sum: 0.00 Training loss: 0.029 Explore P: 0.9094\n",
      "Episode: 32 Total reward: -1.0 Training sum: 0.00 Training loss: 0.096 Explore P: 0.9067\n",
      "Episode: 33 Total reward: 0.0 Training sum: 0.00 Training loss: 0.056 Explore P: 0.9040\n",
      "Episode: 34 Total reward: -2.0 Training sum: -1.00 Training loss: 0.021 Explore P: 0.9013\n",
      "Episode: 35 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8987\n",
      "Episode: 36 Total reward: 0.0 Training sum: -1.00 Training loss: 0.019 Explore P: 0.8960\n",
      "Episode: 37 Total reward: 0.0 Training sum: 1.00 Training loss: 0.031 Explore P: 0.8933\n",
      "Episode: 38 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8907\n",
      "Episode: 39 Total reward: 0.0 Training sum: 0.00 Training loss: 0.092 Explore P: 0.8881\n",
      "Episode: 40 Total reward: 0.0 Training sum: 0.00 Training loss: 0.087 Explore P: 0.8854\n",
      "Episode: 41 Total reward: -1.0 Training sum: 1.00 Training loss: 0.023 Explore P: 0.8828\n",
      "Episode: 42 Total reward: 0.0 Training sum: -1.00 Training loss: 0.033 Explore P: 0.8802\n",
      "Episode: 43 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.8776\n",
      "Episode: 44 Total reward: 0.0 Training sum: 0.00 Training loss: 0.003 Explore P: 0.8750\n",
      "Episode: 45 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8724\n",
      "Episode: 46 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8698\n",
      "Episode: 47 Total reward: -2.0 Training sum: 0.00 Training loss: 0.004 Explore P: 0.8672\n",
      "Episode: 48 Total reward: 0.0 Training sum: 0.00 Training loss: 0.005 Explore P: 0.8647\n",
      "Episode: 49 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.8621\n",
      "Episode: 50 Total reward: -1.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.8595\n",
      "Episode: 51 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8570\n",
      "Episode: 52 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8545\n",
      "Episode: 53 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8519\n",
      "Episode: 54 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8494\n",
      "Episode: 55 Total reward: 1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.8469\n",
      "Episode: 56 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8444\n",
      "Episode: 57 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.8419\n",
      "Episode: 58 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.8394\n",
      "Episode: 59 Total reward: 1.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.8369\n",
      "Episode: 60 Total reward: 0.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.8344\n",
      "Episode: 61 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8320\n",
      "Episode: 62 Total reward: 0.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.8295\n",
      "Episode: 63 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8271\n",
      "Episode: 64 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8246\n",
      "Episode: 65 Total reward: 2.0 Training sum: 2.00 Training loss: 0.030 Explore P: 0.8222\n",
      "Episode: 66 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.8197\n",
      "Episode: 67 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8173\n",
      "Episode: 68 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8149\n",
      "Episode: 69 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.8125\n",
      "Episode: 70 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8101\n",
      "Episode: 71 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8077\n",
      "Episode: 72 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8053\n",
      "Episode: 73 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8029\n",
      "Episode: 74 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.8005\n",
      "Episode: 75 Total reward: -1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.7982\n",
      "Episode: 76 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7958\n",
      "Episode: 77 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7934\n",
      "Episode: 78 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7911\n",
      "Episode: 79 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7888\n",
      "Episode: 80 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7864\n",
      "Episode: 81 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7841\n",
      "Episode: 82 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7818\n",
      "Episode: 83 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7795\n",
      "Episode: 84 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7772\n",
      "Episode: 85 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7749\n",
      "Episode: 86 Total reward: -1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.7726\n",
      "Episode: 87 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7703\n",
      "Episode: 88 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7680\n",
      "Episode: 89 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7657\n",
      "Episode: 90 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7635\n",
      "Episode: 91 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7612\n",
      "Episode: 92 Total reward: -4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 93 Total reward: -2.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.7567\n",
      "Episode: 94 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7545\n",
      "Episode: 95 Total reward: 0.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.7523\n",
      "Episode: 96 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7500\n",
      "Episode: 97 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7478\n",
      "Episode: 98 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7456\n",
      "Episode: 99 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7434\n",
      "Episode: 100 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.7412\n",
      "Episode: 101 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7390\n",
      "Episode: 102 Total reward: -1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.7368\n",
      "Episode: 103 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.7347\n",
      "Episode: 104 Total reward: 2.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.7325\n",
      "Episode: 105 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7303\n",
      "Episode: 106 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.7282\n",
      "Episode: 107 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7260\n",
      "Episode: 108 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7239\n",
      "Episode: 109 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7217\n",
      "Episode: 110 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7196\n",
      "Episode: 111 Total reward: -1.0 Training sum: 1.00 Training loss: 0.018 Explore P: 0.7175\n",
      "Episode: 112 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.7154\n",
      "Episode: 113 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7132\n",
      "Episode: 114 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7111\n",
      "Episode: 115 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.7090\n",
      "Episode: 116 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.7069\n",
      "Episode: 117 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.7049\n",
      "Episode: 118 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.7028\n",
      "Episode: 119 Total reward: 1.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.7007\n",
      "Episode: 120 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6986\n",
      "Episode: 121 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.6966\n",
      "Episode: 122 Total reward: 0.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.6945\n",
      "Episode: 123 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6925\n",
      "Episode: 124 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6904\n",
      "Episode: 125 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.6884\n",
      "Episode: 126 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.6863\n",
      "Episode: 127 Total reward: -2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.6843\n",
      "Episode: 128 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.6823\n",
      "Episode: 129 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6803\n",
      "Episode: 130 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.6783\n",
      "Episode: 131 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.6763\n",
      "Episode: 132 Total reward: 1.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.6743\n",
      "Episode: 133 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6723\n",
      "Episode: 134 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6703\n",
      "Episode: 135 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6683\n",
      "Episode: 136 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6664\n",
      "Episode: 137 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6644\n",
      "Episode: 138 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6624\n",
      "Episode: 139 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.6605\n",
      "Episode: 140 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6585\n",
      "Episode: 141 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6566\n",
      "Episode: 142 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6546\n",
      "Episode: 143 Total reward: -1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.6527\n",
      "Episode: 144 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.6508\n",
      "Episode: 145 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6489\n",
      "Episode: 146 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.6470\n",
      "Episode: 147 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6451\n",
      "Episode: 148 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6431\n",
      "Episode: 149 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6413\n",
      "Episode: 150 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6394\n",
      "Episode: 151 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6375\n",
      "Episode: 152 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6356\n",
      "Episode: 153 Total reward: -2.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.6337\n",
      "Episode: 154 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6319\n",
      "Episode: 155 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6300\n",
      "Episode: 156 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6281\n",
      "Episode: 157 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6263\n",
      "Episode: 158 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6244\n",
      "Episode: 159 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6226\n",
      "Episode: 160 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6208\n",
      "Episode: 161 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6189\n",
      "Episode: 162 Total reward: 1.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.6171\n",
      "Episode: 163 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6153\n",
      "Episode: 164 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6135\n",
      "Episode: 165 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6117\n",
      "Episode: 166 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.6099\n",
      "Episode: 167 Total reward: -1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.6081\n",
      "Episode: 168 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6063\n",
      "Episode: 169 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6045\n",
      "Episode: 170 Total reward: 2.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.6027\n",
      "Episode: 171 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.6009\n",
      "Episode: 172 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5992\n",
      "Episode: 173 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5974\n",
      "Episode: 174 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5956\n",
      "Episode: 175 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.5939\n",
      "Episode: 176 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5921\n",
      "Episode: 177 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5904\n",
      "Episode: 178 Total reward: 1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.5887\n",
      "Episode: 179 Total reward: 2.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.5869\n",
      "Episode: 180 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5852\n",
      "Episode: 181 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5835\n",
      "Episode: 182 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5818\n",
      "Episode: 183 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5800\n",
      "Episode: 184 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 185 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5766\n",
      "Episode: 186 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5749\n",
      "Episode: 187 Total reward: 2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5732\n",
      "Episode: 188 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5716\n",
      "Episode: 189 Total reward: 2.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.5699\n",
      "Episode: 190 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5682\n",
      "Episode: 191 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5665\n",
      "Episode: 192 Total reward: 1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.5649\n",
      "Episode: 193 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5632\n",
      "Episode: 194 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5615\n",
      "Episode: 195 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5599\n",
      "Episode: 196 Total reward: 3.0 Training sum: -1.00 Training loss: 0.017 Explore P: 0.5582\n",
      "Episode: 197 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5566\n",
      "Episode: 198 Total reward: -3.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5550\n",
      "Episode: 199 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5533\n",
      "Episode: 200 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5517\n",
      "Episode: 201 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5501\n",
      "Episode: 202 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5485\n",
      "Episode: 203 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.5468\n",
      "Episode: 204 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5452\n",
      "Episode: 205 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5436\n",
      "Episode: 206 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5420\n",
      "Episode: 207 Total reward: -1.0 Training sum: 0.00 Training loss: 0.004 Explore P: 0.5404\n",
      "Episode: 208 Total reward: 2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5389\n",
      "Episode: 209 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5373\n",
      "Episode: 210 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5357\n",
      "Episode: 211 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.5341\n",
      "Episode: 212 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5325\n",
      "Episode: 213 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5310\n",
      "Episode: 214 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5294\n",
      "Episode: 215 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5279\n",
      "Episode: 216 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5263\n",
      "Episode: 217 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.5248\n",
      "Episode: 218 Total reward: 1.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.5232\n",
      "Episode: 219 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5217\n",
      "Episode: 220 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5202\n",
      "Episode: 221 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5186\n",
      "Episode: 222 Total reward: 1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.5171\n",
      "Episode: 223 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5156\n",
      "Episode: 224 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5141\n",
      "Episode: 225 Total reward: 1.0 Training sum: 0.00 Training loss: 0.032 Explore P: 0.5126\n",
      "Episode: 226 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5110\n",
      "Episode: 227 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.5095\n",
      "Episode: 228 Total reward: -1.0 Training sum: 0.00 Training loss: 0.003 Explore P: 0.5081\n",
      "Episode: 229 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5066\n",
      "Episode: 230 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5051\n",
      "Episode: 231 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5036\n",
      "Episode: 232 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5021\n",
      "Episode: 233 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.5006\n",
      "Episode: 234 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4992\n",
      "Episode: 235 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4977\n",
      "Episode: 236 Total reward: -2.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.4962\n",
      "Episode: 237 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4948\n",
      "Episode: 238 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4933\n",
      "Episode: 239 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4919\n",
      "Episode: 240 Total reward: -2.0 Training sum: 0.00 Training loss: 0.004 Explore P: 0.4904\n",
      "Episode: 241 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4890\n",
      "Episode: 242 Total reward: -2.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.4876\n",
      "Episode: 243 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4861\n",
      "Episode: 244 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4847\n",
      "Episode: 245 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4833\n",
      "Episode: 246 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4819\n",
      "Episode: 247 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4805\n",
      "Episode: 248 Total reward: 1.0 Training sum: -1.00 Training loss: 0.017 Explore P: 0.4790\n",
      "Episode: 249 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4776\n",
      "Episode: 250 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4762\n",
      "Episode: 251 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4748\n",
      "Episode: 252 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4735\n",
      "Episode: 253 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4721\n",
      "Episode: 254 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4707\n",
      "Episode: 255 Total reward: 1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.4693\n",
      "Episode: 256 Total reward: -1.0 Training sum: 1.00 Training loss: 0.049 Explore P: 0.4679\n",
      "Episode: 257 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4666\n",
      "Episode: 258 Total reward: 0.0 Training sum: -1.00 Training loss: 0.017 Explore P: 0.4652\n",
      "Episode: 259 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.4638\n",
      "Episode: 260 Total reward: 3.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.4625\n",
      "Episode: 261 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4611\n",
      "Episode: 262 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.4598\n",
      "Episode: 263 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4584\n",
      "Episode: 264 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4571\n",
      "Episode: 265 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4557\n",
      "Episode: 266 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4544\n",
      "Episode: 267 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4531\n",
      "Episode: 268 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.4517\n",
      "Episode: 269 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4504\n",
      "Episode: 270 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4491\n",
      "Episode: 271 Total reward: -3.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4478\n",
      "Episode: 272 Total reward: -2.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.4465\n",
      "Episode: 273 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.4452\n",
      "Episode: 274 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4439\n",
      "Episode: 275 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.4426\n",
      "Episode: 276 Total reward: 0.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.4413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 277 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4400\n",
      "Episode: 278 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4387\n",
      "Episode: 279 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4374\n",
      "Episode: 280 Total reward: 2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.4361\n",
      "Episode: 281 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4348\n",
      "Episode: 282 Total reward: 0.0 Training sum: 0.00 Training loss: 0.032 Explore P: 0.4336\n",
      "Episode: 283 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4323\n",
      "Episode: 284 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4310\n",
      "Episode: 285 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4298\n",
      "Episode: 286 Total reward: 0.0 Training sum: -2.00 Training loss: 0.032 Explore P: 0.4285\n",
      "Episode: 287 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.4273\n",
      "Episode: 288 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4260\n",
      "Episode: 289 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.4248\n",
      "Episode: 290 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4235\n",
      "Episode: 291 Total reward: -4.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.4223\n",
      "Episode: 292 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4210\n",
      "Episode: 293 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4198\n",
      "Episode: 294 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4186\n",
      "Episode: 295 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4174\n",
      "Episode: 296 Total reward: -2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.4161\n",
      "Episode: 297 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4149\n",
      "Episode: 298 Total reward: -1.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.4137\n",
      "Episode: 299 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4125\n",
      "Episode: 300 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4113\n",
      "Episode: 301 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4101\n",
      "Episode: 302 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4089\n",
      "Episode: 303 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4077\n",
      "Episode: 304 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.4065\n",
      "Episode: 305 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4053\n",
      "Episode: 306 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4041\n",
      "Episode: 307 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4030\n",
      "Episode: 308 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4018\n",
      "Episode: 309 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.4006\n",
      "Episode: 310 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3994\n",
      "Episode: 311 Total reward: -1.0 Training sum: -2.00 Training loss: 0.031 Explore P: 0.3983\n",
      "Episode: 312 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3971\n",
      "Episode: 313 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3959\n",
      "Episode: 314 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3948\n",
      "Episode: 315 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3936\n",
      "Episode: 316 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3925\n",
      "Episode: 317 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3913\n",
      "Episode: 318 Total reward: -4.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.3902\n",
      "Episode: 319 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3891\n",
      "Episode: 320 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3879\n",
      "Episode: 321 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3868\n",
      "Episode: 322 Total reward: 0.0 Training sum: -1.00 Training loss: 0.017 Explore P: 0.3857\n",
      "Episode: 323 Total reward: -1.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.3845\n",
      "Episode: 324 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3834\n",
      "Episode: 325 Total reward: 0.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.3823\n",
      "Episode: 326 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3812\n",
      "Episode: 327 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3801\n",
      "Episode: 328 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3790\n",
      "Episode: 329 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3779\n",
      "Episode: 330 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3768\n",
      "Episode: 331 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3757\n",
      "Episode: 332 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3746\n",
      "Episode: 333 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3735\n",
      "Episode: 334 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3724\n",
      "Episode: 335 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3713\n",
      "Episode: 336 Total reward: -1.0 Training sum: -1.00 Training loss: 0.047 Explore P: 0.3702\n",
      "Episode: 337 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3691\n",
      "Episode: 338 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3681\n",
      "Episode: 339 Total reward: -1.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.3670\n",
      "Episode: 340 Total reward: -3.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3659\n",
      "Episode: 341 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.3649\n",
      "Episode: 342 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3638\n",
      "Episode: 343 Total reward: -1.0 Training sum: 2.00 Training loss: 0.032 Explore P: 0.3627\n",
      "Episode: 344 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3617\n",
      "Episode: 345 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3606\n",
      "Episode: 346 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3596\n",
      "Episode: 347 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3585\n",
      "Episode: 348 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3575\n",
      "Episode: 349 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3564\n",
      "Episode: 350 Total reward: -1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.3554\n",
      "Episode: 351 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3544\n",
      "Episode: 352 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3533\n",
      "Episode: 353 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3523\n",
      "Episode: 354 Total reward: 4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3513\n",
      "Episode: 355 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3503\n",
      "Episode: 356 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3492\n",
      "Episode: 357 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3482\n",
      "Episode: 358 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3472\n",
      "Episode: 359 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3462\n",
      "Episode: 360 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3452\n",
      "Episode: 361 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3442\n",
      "Episode: 362 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3432\n",
      "Episode: 363 Total reward: 0.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.3422\n",
      "Episode: 364 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3412\n",
      "Episode: 365 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3402\n",
      "Episode: 366 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3392\n",
      "Episode: 367 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3382\n",
      "Episode: 368 Total reward: 2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 369 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3363\n",
      "Episode: 370 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3353\n",
      "Episode: 371 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3343\n",
      "Episode: 372 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3333\n",
      "Episode: 373 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3324\n",
      "Episode: 374 Total reward: 2.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3314\n",
      "Episode: 375 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3304\n",
      "Episode: 376 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3295\n",
      "Episode: 377 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3285\n",
      "Episode: 378 Total reward: 4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3276\n",
      "Episode: 379 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3266\n",
      "Episode: 380 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3257\n",
      "Episode: 381 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3247\n",
      "Episode: 382 Total reward: -1.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.3238\n",
      "Episode: 383 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3228\n",
      "Episode: 384 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3219\n",
      "Episode: 385 Total reward: -1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3210\n",
      "Episode: 386 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3200\n",
      "Episode: 387 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3191\n",
      "Episode: 388 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3182\n",
      "Episode: 389 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.3173\n",
      "Episode: 390 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3163\n",
      "Episode: 391 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3154\n",
      "Episode: 392 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3145\n",
      "Episode: 393 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3136\n",
      "Episode: 394 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3127\n",
      "Episode: 395 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3118\n",
      "Episode: 396 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3109\n",
      "Episode: 397 Total reward: -1.0 Training sum: 2.00 Training loss: 0.032 Explore P: 0.3100\n",
      "Episode: 398 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3091\n",
      "Episode: 399 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3082\n",
      "Episode: 400 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3073\n",
      "Episode: 401 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3064\n",
      "Episode: 402 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3055\n",
      "Episode: 403 Total reward: 4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3046\n",
      "Episode: 404 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3037\n",
      "Episode: 405 Total reward: 2.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.3029\n",
      "Episode: 406 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3020\n",
      "Episode: 407 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.3011\n",
      "Episode: 408 Total reward: 1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.3002\n",
      "Episode: 409 Total reward: 0.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.2994\n",
      "Episode: 410 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2985\n",
      "Episode: 411 Total reward: 1.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.2976\n",
      "Episode: 412 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2968\n",
      "Episode: 413 Total reward: -1.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.2959\n",
      "Episode: 414 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2951\n",
      "Episode: 415 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2942\n",
      "Episode: 416 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2934\n",
      "Episode: 417 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2925\n",
      "Episode: 418 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2917\n",
      "Episode: 419 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2908\n",
      "Episode: 420 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2900\n",
      "Episode: 421 Total reward: -5.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2891\n",
      "Episode: 422 Total reward: -1.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.2883\n",
      "Episode: 423 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.2875\n",
      "Episode: 424 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2866\n",
      "Episode: 425 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2858\n",
      "Episode: 426 Total reward: -3.0 Training sum: 0.00 Training loss: 0.004 Explore P: 0.2850\n",
      "Episode: 427 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2842\n",
      "Episode: 428 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2833\n",
      "Episode: 429 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.2825\n",
      "Episode: 430 Total reward: 2.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.2817\n",
      "Episode: 431 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2809\n",
      "Episode: 432 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2801\n",
      "Episode: 433 Total reward: -3.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.2793\n",
      "Episode: 434 Total reward: 0.0 Training sum: 1.00 Training loss: 0.015 Explore P: 0.2785\n",
      "Episode: 435 Total reward: -2.0 Training sum: 0.00 Training loss: 0.002 Explore P: 0.2777\n",
      "Episode: 436 Total reward: 4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2769\n",
      "Episode: 437 Total reward: 2.0 Training sum: -1.00 Training loss: 0.018 Explore P: 0.2761\n",
      "Episode: 438 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2753\n",
      "Episode: 439 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2745\n",
      "Episode: 440 Total reward: 2.0 Training sum: 1.00 Training loss: 0.018 Explore P: 0.2737\n",
      "Episode: 441 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2729\n",
      "Episode: 442 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2721\n",
      "Episode: 443 Total reward: -1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.2713\n",
      "Episode: 444 Total reward: 0.0 Training sum: 1.00 Training loss: 0.017 Explore P: 0.2705\n",
      "Episode: 445 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2697\n",
      "Episode: 446 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2690\n",
      "Episode: 447 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2682\n",
      "Episode: 448 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2674\n",
      "Episode: 449 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2666\n",
      "Episode: 450 Total reward: 2.0 Training sum: 0.00 Training loss: 0.032 Explore P: 0.2659\n",
      "Episode: 451 Total reward: 2.0 Training sum: -1.00 Training loss: 0.015 Explore P: 0.2651\n",
      "Episode: 452 Total reward: 1.0 Training sum: 1.00 Training loss: 0.047 Explore P: 0.2643\n",
      "Episode: 453 Total reward: 1.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.2636\n",
      "Episode: 454 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2628\n",
      "Episode: 455 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2621\n",
      "Episode: 456 Total reward: 3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2613\n",
      "Episode: 457 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2606\n",
      "Episode: 458 Total reward: 4.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2598\n",
      "Episode: 459 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2591\n",
      "Episode: 460 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 461 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2576\n",
      "Episode: 462 Total reward: 2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2568\n",
      "Episode: 463 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2561\n",
      "Episode: 464 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2554\n",
      "Episode: 465 Total reward: 0.0 Training sum: 1.00 Training loss: 0.016 Explore P: 0.2546\n",
      "Episode: 466 Total reward: -1.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.2539\n",
      "Episode: 467 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2532\n",
      "Episode: 468 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2524\n",
      "Episode: 469 Total reward: -2.0 Training sum: -1.00 Training loss: 0.017 Explore P: 0.2517\n",
      "Episode: 470 Total reward: 0.0 Training sum: -1.00 Training loss: 0.016 Explore P: 0.2510\n",
      "Episode: 471 Total reward: -3.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2503\n",
      "Episode: 472 Total reward: 1.0 Training sum: 0.00 Training loss: 0.001 Explore P: 0.2495\n",
      "Episode: 473 Total reward: 3.0 Training sum: 0.00 Training loss: 0.031 Explore P: 0.2488\n",
      "Episode: 474 Total reward: -2.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2481\n",
      "Episode: 475 Total reward: 1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2474\n",
      "Episode: 476 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2467\n",
      "Episode: 477 Total reward: -5.0 Training sum: -2.00 Training loss: 0.032 Explore P: 0.2460\n",
      "Episode: 478 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2453\n",
      "Episode: 479 Total reward: -1.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2446\n",
      "Episode: 480 Total reward: 0.0 Training sum: 0.00 Training loss: 0.000 Explore P: 0.2439\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3cba485ac936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#Select an action with probability e\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplore_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplore_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#Execute action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e5dab63c8e78>\u001b[0m in \u001b[0;36mpredict_action\u001b[1;34m(explore_start, explore_stop, decay_rate, step, state)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mnext_Q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplore_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "save_file = './Model/basic.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    sess.run(init_op)\n",
    "   \n",
    "    step = 0\n",
    "    for ep in range(0, train_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]\n",
    "                      \n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            \n",
    "            #Select an action with probability e\n",
    "            action, ex_p = predict_action(explore_start, explore_stop, decay_rate, step, state)\n",
    "            \n",
    "            #Execute action\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            #Observe reward and next state\n",
    "            reward = env_info.rewards[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                next_state = np.zeros(state_size)\n",
    "\n",
    "            #Store transition (st, at, rt, st+1, done) in D\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            \n",
    "            states = [each[0] for each in batch]\n",
    "            actions = [each[1] for each in batch]\n",
    "            rewards = [each[2] for each in batch]\n",
    "            next_states = [each[3] for each in batch]\n",
    "            dones = [each[4] for each in batch]\n",
    "            targets= []\n",
    "                        \n",
    "            next_Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: next_states})\n",
    "            batch_sum = 0\n",
    "            for i in range(len(batch)):\n",
    "                if dones[i]:\n",
    "                    target = rewards[i]\n",
    "                else:\n",
    "                    target = rewards[i] + gamma * np.max(next_Qs[i])                    \n",
    "                targets.append(target)\n",
    "                batch_sum += rewards[i]\n",
    "\n",
    "            loss, _ = sess.run([mainDQN.loss, mainDQN.opt], feed_dict={mainDQN.inputs: states, mainDQN.target: targets, mainDQN.actions: actions})                   \n",
    "\n",
    "            if done:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training sum: {:.2f}'.format(batch_sum),\n",
    "                      'Training loss: {:.3f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(ex_p))\n",
    "                #if ep % 50 == 0:\n",
    "                    #Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: states})\n",
    "                    #print(Qs)\n",
    "                #rewards_list.append((ep, total_reward))              \n",
    "                break\n",
    "            state = next_state\n",
    "    \n",
    "    save_path = saver.save(sess, save_file)\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "    with tf.variable_scope('hidden', reuse=True):\n",
    "        w = tf.get_variable('kernel')\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Model/basic.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./Model/basic.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 359, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-5b92b678fa1f>\", line 6, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1311, in __init__\n    self.build()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1541, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./Model/basic.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./Model/basic.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5b92b678fa1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#with tf.device(\"/device:GPU:0\") as sess:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_soft_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1775\u001b[1;33m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./Model/basic.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 359, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-5b92b678fa1f>\", line 6, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1311, in __init__\n    self.build()\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1541, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\AHI\\Anaconda3\\envs\\P1_Navigation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./Model/basic.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "save_file = './Model/basic.ckpt'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "DQN = DQNetwork(name=\"main\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#with tf.device(\"/device:GPU:0\") as sess:\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    scores = []\n",
    "    print(\"Validation Start\")\n",
    "    for ep in range(1, 101):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        score = 0                                          # initialize the score        \n",
    "        while True:\n",
    "            #Choose action\n",
    "            \"\"\"\n",
    "            # Get action from Q-network            \n",
    "            Qs = sess.run(DQN.output, feed_dict={DQN.inputs: state.reshape((1, *state.shape))})\n",
    "            action = np.asscalar(np.argmax(Qs[0]))\n",
    "            next_Qs = sess.run(DQN.output, feed_dict={DQN.inputs: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            Qs[0] = [q / sum(Qs[0]) for q in Qs[0]]\n",
    "\n",
    "            if Qs[0, action] > np.random.uniform():\n",
    "                pass\n",
    "            else:\n",
    "                action = np.random.choice(4) # roll the dice!\n",
    "            \"\"\"\n",
    "            Qs = sess.run(DQN.output, feed_dict={DQN.inputs: state.reshape((1, *state.shape))})\n",
    "            action = np.asscalar(np.argmax(Qs[0]))\n",
    "            #print(\"action: \", action, \", Q : \",Qs[0])\n",
    "            #sleep(0.1)\n",
    "            \n",
    "            #Execute action\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            #Observe reward and next state\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "\n",
    "            #Store reward\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \"\"\"\n",
    "            next_Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: next_state})\n",
    "            target = reward + gamma * np.max(next_Qs[0])\n",
    "            loss, _ = sess.run([mainDQN.loss, mainDQN.opt], feed_dict={mainDQN.inputs: state.reshape((1, *state.shape)), mainDQN.target: target, mainDQN.actions: action})\n",
    "            \"\"\"\n",
    "            state = next_state\n",
    "        print(\"episode: {}, Score : {}\".format(ep, score))\n",
    "        scores.append(score)\n",
    "    print(\"Average score in 100 episodes: {}\".format(np.sum(scores) / 100.0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Code\n",
    "일단 Hidden size 팍 줄이고, Layer 늘려볼것.\n",
    "그리고 나서 Target network 적용\n",
    "그리고 나서 prioritized 방식 적용\n",
    "\n",
    "메모\n",
    "1. Validation 시 좌우, 앞뒤와 같은 반복움직임;\n",
    "   - Local maxima인거 같은데 음;;;\n",
    "2. Memory replay 개선 : prioritized 방식\n",
    "   - 학습 Convergence 속도 때문\n",
    "3. Target Q 다시 적용\n",
    "   - 이게 가장 시급한거 같음;\n",
    "4. Dueling:\n",
    "5. 4 stack\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1",
   "language": "python",
   "name": "p1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
