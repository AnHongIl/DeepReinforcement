{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Start the environment\n",
    "- Prepare unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Get the default brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training through DQN and tensorflow\n",
    "  - Implement DQN with experience memory   \n",
    "  - number of hidden layers and hidden nodes\n",
    "  - hidden layers = 2 and hidden nodes = 2/3 * input(37) + output(4) = 27.xx(28)\n",
    "    https://web.archive.org/web/20140721050413/http://www.heatonresearch.com/node/707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=37, hidden_size = 256, action_size=4, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.device(\"/device:GPU:0\"):\n",
    "        #with tf.variable_scope(name):\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            self.actions = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            self.one_hot_actions = tf.one_hot(self.actions, action_size)\n",
    "            \n",
    "            self.hidden1 = tf.layers.dense(inputs=self.inputs, units=hidden_size, activation=tf.nn.relu)\n",
    "            #self.hidden2 = tf.layers.dense(inputs=self.hidden1, units=hidden_size, activation=tf.nn.relu)\n",
    "\n",
    "            self.output = tf.layers.dense(inputs=self.hidden1, units=action_size, activation=None)\n",
    "\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target')\n",
    "                        \n",
    "            self.Qmul = tf.multiply(self.output, self.one_hot_actions)\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.one_hot_actions), axis=1)\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            #self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 37\n",
    "action_size = 4\n",
    "\n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 1000000            # memory capacity\n",
    "batch_size = 64                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory\n",
    "\n",
    "#Target network update\n",
    "#C = 1000\n",
    "C = 10000\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define the experience memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) >= self.buffer.maxlen:\n",
    "            self.buffer.popleft()         \n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pretrain the experience memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocate memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "#Reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "\n",
    "for i in range(pretrain_length):    \n",
    "    #Choose an action using random policy\n",
    "    action = np.random.choice(4)\n",
    "    \n",
    "    #Take the action\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    \n",
    "    #Observe reward and next state\n",
    "    reward = env_info.rewards[0]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    done = env_info.local_done[0]                    \n",
    "\n",
    "    if done:\n",
    "        print('Done')\n",
    "        next_state = np.zeros(state_size)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        #Reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        continue\n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "    #S <- S'\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    with tf.device(\"/device:GPU:0\"):        \n",
    "        op_holder = []\n",
    "\n",
    "        src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "        dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "        for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "            #op_holder.append(dest_var.assign(src_var.value()))\n",
    "            op_holder.append(dest_var.assign(src_var))\n",
    "        return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, step, state):\n",
    "    explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "    if explore_p > np.random.rand():\n",
    "        # Make a random action\n",
    "        action = np.random.choice(action_size)\n",
    "    else:\n",
    "        # Get action from Q-network\n",
    "        next_Q = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: state.reshape(1, state_size)})\n",
    "        action = np.asscalar(np.argmax(next_Q[0]))\n",
    "        #action = np.random.choice(action_size)\n",
    "        \n",
    "    return action, explore_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:    \n",
    "        scores = []\n",
    "        for ep in range(100):\n",
    "            env_info = env.reset(train_mode=False)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "\n",
    "            score = 0                                          # initialize the score        \n",
    "            while True:\n",
    "                Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: state.reshape((1, *state.shape))})\n",
    "                print(Qs)\n",
    "                action = np.asscalar(np.argmax(Qs[0]))\n",
    "                env_info = env.step(action)[brain_name]\n",
    "\n",
    "                #Observe reward and next state\n",
    "                reward = env_info.rewards[0]\n",
    "                next_state = env_info.vector_observations[0]\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            print(\"episode: {}\".format(ep+1), \"Score: {}\".format(score))\n",
    "            scores.append(score)\n",
    "        print(\"Average score in 100 episodes: {}\".format(np.sum(scores) / 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value dense_1/bias\n\t [[Node: dense_1/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_1/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/bias)]]\n\nCaused by op 'dense_1/bias/read', defined at:\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-13e2520f986f>\", line 3, in <module>\n    mainDQN = DQNetwork(name=\"main\", hidden_size=hidden_size, learning_rate=learning_rate)\n  File \"<ipython-input-3-a1182b3f2970>\", line 18, in __init__\n    self.output = tf.layers.dense(inputs=self.hidden1, units=action_size, activation=None)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 146, in build\n    trainable=True)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\n    partitioner=partitioner)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 415, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1297, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1093, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 439, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 408, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 800, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 391, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3658, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_1/bias\n\t [[Node: dense_1/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_1/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/bias)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_1/bias\n\t [[Node: dense_1/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_1/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/bias)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d262532c960b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrewards_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_soft_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-ee8e0329fa82>\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m                                          \u001b[1;31m# initialize the score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_1/bias\n\t [[Node: dense_1/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_1/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/bias)]]\n\nCaused by op 'dense_1/bias/read', defined at:\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-13e2520f986f>\", line 3, in <module>\n    mainDQN = DQNetwork(name=\"main\", hidden_size=hidden_size, learning_rate=learning_rate)\n  File \"<ipython-input-3-a1182b3f2970>\", line 18, in __init__\n    self.output = tf.layers.dense(inputs=self.hidden1, units=action_size, activation=None)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 146, in build\n    trainable=True)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\n    partitioner=partitioner)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 415, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1297, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1093, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 439, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 408, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 800, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 391, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3658, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Lab\\Anaconda3\\envs\\p1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_1/bias\n\t [[Node: dense_1/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_1/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/bias)]]\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    tf.reset_default_graph()\n",
    "    mainDQN = DQNetwork(name=\"main\", hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "    #targetDQN = DQNetwork(name=\"target\", hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Step: 300 Total reward: 1.0 Training loss: 2.9699 Explore P: 0.9970\n",
      "[[4.5943675 5.3294854 3.2046866 3.1603777]\n",
      " [7.944023  4.3194623 4.7484026 4.498185 ]\n",
      " [6.846814  3.5674865 4.3562427 3.3723652]\n",
      " [7.2145023 3.9676092 4.6181912 3.7733212]\n",
      " [5.008889  3.1177785 2.8903613 2.3879185]\n",
      " [3.9745803 3.3708763 2.5378127 2.3254042]\n",
      " [5.1605244 7.303274  4.261751  4.508261 ]\n",
      " [4.4340906 4.9364247 3.0416093 3.6619143]\n",
      " [9.137946  4.8083463 5.81684   4.9352255]\n",
      " [4.384675  5.3765717 3.296164  3.0554485]\n",
      " [4.1635075 4.5745883 2.9408143 2.9118452]\n",
      " [9.079639  4.7138877 5.6797037 4.733161 ]\n",
      " [3.3951483 3.3976843 2.1190996 1.9494985]\n",
      " [3.6946673 4.547998  2.5740614 2.6980965]\n",
      " [3.3560114 4.05257   2.2425976 2.2120473]\n",
      " [5.6104727 3.279546  2.939683  3.047233 ]\n",
      " [6.804757  3.6524189 4.0024967 3.514903 ]\n",
      " [3.8017979 3.2434516 2.5931134 2.1953077]\n",
      " [5.6742673 8.323836  4.417793  4.562245 ]\n",
      " [5.1507683 7.4517713 4.1765275 4.2849364]\n",
      " [3.919662  5.137943  2.7202942 2.6587715]\n",
      " [4.9110594 6.7109604 3.6116004 4.040262 ]\n",
      " [5.423848  6.9167914 3.8842113 4.708993 ]\n",
      " [3.7934747 2.9176118 2.2994406 2.3091447]\n",
      " [2.8436098 3.7016912 2.215766  2.208457 ]\n",
      " [4.943951  7.2256103 3.8114204 3.9770575]\n",
      " [5.3665547 3.3636823 3.138374  2.9708567]\n",
      " [9.419989  5.0608172 5.742168  5.2408767]\n",
      " [3.3295503 2.6602585 1.85803   2.1439319]\n",
      " [4.331213  3.3835433 2.754797  2.3754432]\n",
      " [6.541091  3.6722095 4.129222  3.6565483]\n",
      " [7.967257  4.1488557 4.8313775 4.283364 ]\n",
      " [4.6552863 6.51037   3.4822571 3.5675366]\n",
      " [5.598389  8.209213  4.5893736 4.668686 ]\n",
      " [4.448048  3.4881737 2.621671  2.6488526]\n",
      " [4.764485  4.6046224 3.0911732 3.2067647]\n",
      " [4.3438253 4.4081655 2.9534836 2.6432934]\n",
      " [7.5781565 4.1457906 4.638926  4.0884943]\n",
      " [7.7213306 4.4596143 4.7154365 4.3180914]\n",
      " [4.824813  4.7637057 3.1147673 2.9156928]\n",
      " [4.1395035 4.5730243 2.713039  2.6361308]\n",
      " [3.3721094 2.8357632 2.0366642 2.0483303]\n",
      " [3.9405665 3.883917  2.5248365 2.390204 ]\n",
      " [4.5309606 3.3412707 2.5862937 2.4690065]\n",
      " [3.7651768 3.3413856 2.1261084 2.153243 ]\n",
      " [6.9164667 3.545537  4.2600837 3.4462535]\n",
      " [2.6372685 2.9446907 1.8452194 1.8764364]\n",
      " [7.7310133 4.070083  4.401036  4.2104673]\n",
      " [3.5744042 3.0329988 2.132319  1.9618334]\n",
      " [5.2791777 6.719118  3.919832  4.3032584]\n",
      " [5.3155885 3.3441982 3.0631518 2.9261956]\n",
      " [4.464403  3.0730848 2.5427213 2.5185196]\n",
      " [3.2203374 3.1154313 2.037149  2.1820893]\n",
      " [7.531621  4.455489  4.826486  3.8948169]\n",
      " [5.8542356 8.546156  4.7668395 5.2536297]\n",
      " [8.980295  5.0005813 5.7250495 4.9099016]\n",
      " [4.176321  4.5472207 2.7301202 2.67203  ]\n",
      " [7.0179467 3.9989667 4.6852617 3.9889827]\n",
      " [8.204257  4.7063293 4.9975953 4.322502 ]\n",
      " [6.976069  3.6004705 4.300254  3.463294 ]\n",
      " [7.1381097 3.7971253 4.4068313 4.067995 ]\n",
      " [4.272119  5.54308   3.02163   2.8084407]\n",
      " [4.7638516 2.5594714 2.2665505 2.3772478]\n",
      " [3.912219  3.1569824 2.387701  2.2329407]]\n",
      "Episode: 1 Step: 600 Total reward: 0.0 Training loss: 14.0483 Explore P: 0.9941\n",
      "Episode: 2 Step: 900 Total reward: -2.0 Training loss: 40.0784 Explore P: 0.9911\n",
      "Episode: 3 Step: 1200 Total reward: 0.0 Training loss: 124.7620 Explore P: 0.9882\n",
      "Episode: 4 Step: 1500 Total reward: -1.0 Training loss: 62.1641 Explore P: 0.9853\n",
      "Episode: 5 Step: 1800 Total reward: 0.0 Training loss: 167.6518 Explore P: 0.9823\n",
      "Episode: 6 Step: 2100 Total reward: -1.0 Training loss: 188.6476 Explore P: 0.9794\n",
      "Episode: 7 Step: 2400 Total reward: -2.0 Training loss: 452.6425 Explore P: 0.9765\n",
      "Episode: 8 Step: 2700 Total reward: 1.0 Training loss: 1392.7799 Explore P: 0.9736\n",
      "Episode: 9 Step: 3000 Total reward: 0.0 Training loss: 496.4193 Explore P: 0.9707\n",
      "Episode: 10 Step: 3300 Total reward: 1.0 Training loss: 170.3813 Explore P: 0.9679\n",
      "Episode: 11 Step: 3600 Total reward: 1.0 Training loss: 420.0244 Explore P: 0.9650\n",
      "Episode: 12 Step: 3900 Total reward: -4.0 Training loss: 1884.8750 Explore P: 0.9621\n",
      "Episode: 13 Step: 4200 Total reward: 0.0 Training loss: 147.9962 Explore P: 0.9593\n",
      "Episode: 14 Step: 4500 Total reward: 1.0 Training loss: 167.9986 Explore P: 0.9564\n",
      "Episode: 15 Step: 4800 Total reward: 0.0 Training loss: 137.4778 Explore P: 0.9536\n",
      "Episode: 16 Step: 5100 Total reward: -1.0 Training loss: 2007.5680 Explore P: 0.9508\n",
      "Episode: 17 Step: 5400 Total reward: 1.0 Training loss: 2057.5925 Explore P: 0.9480\n",
      "Episode: 18 Step: 5700 Total reward: -1.0 Training loss: 189.8315 Explore P: 0.9451\n",
      "Episode: 19 Step: 6000 Total reward: 1.0 Training loss: 1784.2736 Explore P: 0.9423\n",
      "Episode: 20 Step: 6300 Total reward: 0.0 Training loss: 84.2617 Explore P: 0.9396\n",
      "Episode: 21 Step: 6600 Total reward: 1.0 Training loss: 1426.6155 Explore P: 0.9368\n",
      "Episode: 22 Step: 6900 Total reward: -1.0 Training loss: 53.6748 Explore P: 0.9340\n",
      "Episode: 23 Step: 7200 Total reward: 1.0 Training loss: 42.0192 Explore P: 0.9312\n",
      "Episode: 24 Step: 7500 Total reward: 2.0 Training loss: 36.5998 Explore P: 0.9285\n",
      "Episode: 25 Step: 7800 Total reward: -3.0 Training loss: 46.5821 Explore P: 0.9257\n",
      "Episode: 26 Step: 8100 Total reward: -1.0 Training loss: 1311.2936 Explore P: 0.9230\n",
      "Episode: 27 Step: 8400 Total reward: 1.0 Training loss: 23.9314 Explore P: 0.9202\n",
      "Episode: 28 Step: 8700 Total reward: -1.0 Training loss: 20.9747 Explore P: 0.9175\n",
      "Episode: 29 Step: 9000 Total reward: 1.0 Training loss: 21.7058 Explore P: 0.9148\n",
      "Episode: 30 Step: 9300 Total reward: -1.0 Training loss: 11.7648 Explore P: 0.9121\n",
      "Episode: 31 Step: 9600 Total reward: -1.0 Training loss: 9.0486 Explore P: 0.9094\n",
      "Episode: 32 Step: 9900 Total reward: -1.0 Training loss: 10.2272 Explore P: 0.9067\n",
      "Episode: 33 Step: 10200 Total reward: -1.0 Training loss: 11.9097 Explore P: 0.9040\n",
      "Episode: 34 Step: 10500 Total reward: 0.0 Training loss: 6.1355 Explore P: 0.9013\n",
      "Episode: 35 Step: 10800 Total reward: 1.0 Training loss: 3.0794 Explore P: 0.8987\n",
      "Episode: 36 Step: 11100 Total reward: -1.0 Training loss: 5.0483 Explore P: 0.8960\n",
      "Episode: 37 Step: 11400 Total reward: 0.0 Training loss: 4.5508 Explore P: 0.8933\n",
      "Episode: 38 Step: 11700 Total reward: -1.0 Training loss: 7.0737 Explore P: 0.8907\n",
      "Episode: 39 Step: 12000 Total reward: -1.0 Training loss: 4.8314 Explore P: 0.8881\n",
      "Episode: 40 Step: 12300 Total reward: -1.0 Training loss: 585.4800 Explore P: 0.8854\n",
      "Episode: 41 Step: 12600 Total reward: 0.0 Training loss: 3.3681 Explore P: 0.8828\n",
      "Episode: 42 Step: 12900 Total reward: 1.0 Training loss: 4.8392 Explore P: 0.8802\n",
      "Episode: 43 Step: 13200 Total reward: -1.0 Training loss: 3.8516 Explore P: 0.8776\n",
      "Episode: 44 Step: 13500 Total reward: 2.0 Training loss: 3.8432 Explore P: 0.8750\n",
      "Episode: 45 Step: 13800 Total reward: -1.0 Training loss: 3.0528 Explore P: 0.8724\n",
      "Episode: 46 Step: 14100 Total reward: 0.0 Training loss: 1.7458 Explore P: 0.8698\n",
      "Episode: 47 Step: 14400 Total reward: 0.0 Training loss: 433.3219 Explore P: 0.8672\n",
      "Episode: 48 Step: 14700 Total reward: 0.0 Training loss: 867.3367 Explore P: 0.8647\n",
      "Episode: 49 Step: 15000 Total reward: -1.0 Training loss: 2.4243 Explore P: 0.8621\n",
      "Episode: 50 Step: 15300 Total reward: -2.0 Training loss: 773.8341 Explore P: 0.8595\n",
      "Episode: 51 Step: 15600 Total reward: 0.0 Training loss: 3.4195 Explore P: 0.8570\n",
      "Episode: 52 Step: 15900 Total reward: 0.0 Training loss: 2.4296 Explore P: 0.8545\n",
      "Episode: 53 Step: 16200 Total reward: -2.0 Training loss: 696.8436 Explore P: 0.8519\n",
      "Episode: 54 Step: 16500 Total reward: -1.0 Training loss: 2.2863 Explore P: 0.8494\n",
      "Episode: 55 Step: 16800 Total reward: 0.0 Training loss: 2.4535 Explore P: 0.8469\n",
      "Episode: 56 Step: 17100 Total reward: 0.0 Training loss: 2.0348 Explore P: 0.8444\n",
      "Episode: 57 Step: 17400 Total reward: 0.0 Training loss: 3.3601 Explore P: 0.8419\n",
      "Episode: 58 Step: 17700 Total reward: 1.0 Training loss: 2.0999 Explore P: 0.8394\n",
      "Episode: 59 Step: 18000 Total reward: 0.0 Training loss: 1.3421 Explore P: 0.8369\n",
      "Episode: 60 Step: 18300 Total reward: -1.0 Training loss: 2.1837 Explore P: 0.8344\n",
      "Episode: 61 Step: 18600 Total reward: -2.0 Training loss: 223.9183 Explore P: 0.8320\n",
      "Episode: 62 Step: 18900 Total reward: 4.0 Training loss: 1.7204 Explore P: 0.8295\n",
      "Episode: 63 Step: 19200 Total reward: 0.0 Training loss: 227.6666 Explore P: 0.8271\n",
      "Episode: 64 Step: 19500 Total reward: 2.0 Training loss: 1.8717 Explore P: 0.8246\n",
      "Episode: 65 Step: 19800 Total reward: 1.0 Training loss: 173.3858 Explore P: 0.8222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 66 Step: 20100 Total reward: 1.0 Training loss: 546.0077 Explore P: 0.8197\n",
      "Episode: 67 Step: 20400 Total reward: -1.0 Training loss: 1.1745 Explore P: 0.8173\n",
      "Episode: 68 Step: 20700 Total reward: 1.0 Training loss: 1.3673 Explore P: 0.8149\n",
      "Episode: 69 Step: 21000 Total reward: -4.0 Training loss: 172.5473 Explore P: 0.8125\n",
      "Episode: 70 Step: 21300 Total reward: -3.0 Training loss: 1.1160 Explore P: 0.8101\n",
      "Episode: 71 Step: 21600 Total reward: -1.0 Training loss: 0.9799 Explore P: 0.8077\n",
      "Episode: 72 Step: 21900 Total reward: 0.0 Training loss: 1.0722 Explore P: 0.8053\n",
      "Episode: 73 Step: 22200 Total reward: 0.0 Training loss: 0.8191 Explore P: 0.8029\n",
      "Episode: 74 Step: 22500 Total reward: -1.0 Training loss: 0.8246 Explore P: 0.8005\n",
      "Episode: 75 Step: 22800 Total reward: 1.0 Training loss: 0.9014 Explore P: 0.7982\n",
      "Episode: 76 Step: 23100 Total reward: 0.0 Training loss: 0.7501 Explore P: 0.7958\n",
      "Episode: 77 Step: 23400 Total reward: 1.0 Training loss: 0.9764 Explore P: 0.7934\n",
      "Episode: 78 Step: 23700 Total reward: -1.0 Training loss: 0.8246 Explore P: 0.7911\n",
      "Episode: 79 Step: 24000 Total reward: -1.0 Training loss: 108.8252 Explore P: 0.7888\n",
      "Episode: 80 Step: 24300 Total reward: 2.0 Training loss: 2.1166 Explore P: 0.7864\n",
      "Episode: 81 Step: 24600 Total reward: 0.0 Training loss: 0.9044 Explore P: 0.7841\n",
      "Episode: 82 Step: 24900 Total reward: -1.0 Training loss: 0.5644 Explore P: 0.7818\n",
      "Episode: 83 Step: 25200 Total reward: -1.0 Training loss: 0.8309 Explore P: 0.7795\n",
      "Episode: 84 Step: 25500 Total reward: 2.0 Training loss: 0.4528 Explore P: 0.7772\n",
      "Episode: 85 Step: 25800 Total reward: 1.0 Training loss: 0.6726 Explore P: 0.7749\n",
      "Episode: 86 Step: 26100 Total reward: 1.0 Training loss: 0.6886 Explore P: 0.7726\n",
      "Episode: 87 Step: 26400 Total reward: -2.0 Training loss: 68.2714 Explore P: 0.7703\n",
      "Episode: 88 Step: 26700 Total reward: 0.0 Training loss: 0.5888 Explore P: 0.7680\n",
      "Episode: 89 Step: 27000 Total reward: 0.0 Training loss: 60.1236 Explore P: 0.7657\n",
      "Episode: 90 Step: 27300 Total reward: 2.0 Training loss: 56.8881 Explore P: 0.7635\n",
      "Episode: 91 Step: 27600 Total reward: 2.0 Training loss: 52.5110 Explore P: 0.7612\n",
      "Episode: 92 Step: 27900 Total reward: 0.0 Training loss: 0.4895 Explore P: 0.7590\n",
      "Episode: 93 Step: 28200 Total reward: 1.0 Training loss: 0.5406 Explore P: 0.7567\n",
      "Episode: 94 Step: 28500 Total reward: 0.0 Training loss: 41.8899 Explore P: 0.7545\n",
      "Episode: 95 Step: 28800 Total reward: -1.0 Training loss: 0.4283 Explore P: 0.7523\n",
      "Episode: 96 Step: 29100 Total reward: 0.0 Training loss: 0.3966 Explore P: 0.7500\n",
      "Episode: 97 Step: 29400 Total reward: 0.0 Training loss: 0.3365 Explore P: 0.7478\n",
      "Episode: 98 Step: 29700 Total reward: -3.0 Training loss: 0.4280 Explore P: 0.7456\n",
      "Episode: 99 Step: 30000 Total reward: 0.0 Training loss: 0.3798 Explore P: 0.7434\n",
      "Episode: 100 Step: 30300 Total reward: -1.0 Training loss: 0.2511 Explore P: 0.7412\n",
      "[[46.30875  46.44408  46.260258 46.19907 ]\n",
      " [47.060055 47.950428 47.105522 47.62517 ]\n",
      " [48.703907 49.438026 48.101597 48.44786 ]\n",
      " [46.40269  46.3997   46.791607 46.05482 ]\n",
      " [47.25381  48.003883 46.98449  47.387245]\n",
      " [45.84889  46.087303 46.50304  45.428074]\n",
      " [47.291096 47.293488 46.662056 46.958126]\n",
      " [46.38814  47.672203 46.759174 46.506397]\n",
      " [46.09502  47.145744 46.475525 46.693607]\n",
      " [46.345394 46.739353 46.044575 47.221333]\n",
      " [45.898624 45.95614  45.816143 46.74937 ]\n",
      " [46.69321  47.280552 46.623684 47.792747]\n",
      " [46.325665 46.277435 46.62837  45.857395]\n",
      " [45.789654 45.77463  46.268513 46.512177]\n",
      " [45.72247  47.12479  46.075165 46.61289 ]\n",
      " [45.568832 44.848915 45.45142  44.98656 ]\n",
      " [47.451954 47.73504  47.254063 46.860256]\n",
      " [47.155903 48.176636 47.02943  46.956367]\n",
      " [47.18234  47.608826 46.583084 46.358498]\n",
      " [45.71192  45.095673 46.06148  45.99296 ]\n",
      " [46.68482  47.3294   46.935947 47.41779 ]\n",
      " [46.866177 48.376965 46.356636 47.071476]\n",
      " [46.95917  47.365936 46.96146  46.061863]\n",
      " [47.080997 48.318207 46.85374  46.822445]\n",
      " [47.999058 48.56606  47.930428 47.123108]\n",
      " [46.86191  46.541534 46.19311  46.88161 ]\n",
      " [46.423496 46.497383 47.03056  46.373264]\n",
      " [45.379486 45.313454 45.507076 45.476227]\n",
      " [46.135975 45.937683 46.395138 46.39542 ]\n",
      " [46.637306 46.78934  46.896954 46.32545 ]\n",
      " [46.835823 47.150475 46.98951  47.09178 ]\n",
      " [45.456104 45.269794 46.368427 45.383434]\n",
      " [47.026577 47.2257   47.680973 46.78697 ]\n",
      " [47.38599  48.10446  47.207184 47.08169 ]\n",
      " [47.89514  49.070564 47.539116 47.41772 ]\n",
      " [47.961002 47.32339  47.538868 46.572292]\n",
      " [45.55138  45.577675 45.93317  46.061127]\n",
      " [46.887726 47.201088 46.85174  46.938152]\n",
      " [47.28194  47.402298 47.19464  46.63701 ]\n",
      " [47.12344  48.84613  46.700314 46.78003 ]\n",
      " [47.40367  48.01451  47.261806 47.220177]\n",
      " [45.85197  45.904438 45.90922  46.469948]\n",
      " [46.654823 46.7151   46.50033  46.15317 ]\n",
      " [47.851063 49.80455  47.609333 48.081264]\n",
      " [45.83569  46.837234 45.843327 45.85743 ]\n",
      " [47.300434 47.304596 47.18018  47.211514]\n",
      " [48.227207 48.799046 48.104168 48.207752]\n",
      " [47.65918  48.285496 47.798336 47.855434]\n",
      " [46.962482 47.914078 47.227364 47.052567]\n",
      " [48.1002   50.03288  47.809517 47.99854 ]\n",
      " [47.358177 48.148376 47.601326 47.551624]\n",
      " [46.73035  47.285362 47.33894  46.997547]\n",
      " [44.795063 44.77082  45.856167 45.29889 ]\n",
      " [47.530685 48.267498 47.494537 47.361935]\n",
      " [45.109383 44.88903  45.50293  44.88631 ]\n",
      " [47.22629  47.568523 47.330246 47.179977]\n",
      " [45.747723 45.44854  46.146965 44.933064]\n",
      " [45.89918  46.183296 46.391033 45.31647 ]\n",
      " [47.177822 47.71344  46.73869  47.720547]\n",
      " [46.30343  46.858093 46.60581  47.322765]\n",
      " [45.50797  46.642216 45.5175   46.08668 ]\n",
      " [46.904186 47.049717 47.223923 46.479115]\n",
      " [47.043056 47.23852  47.27957  46.82754 ]\n",
      " [46.60332  47.224472 46.34843  46.28864 ]]\n",
      "Episode: 101 Step: 30600 Total reward: 0.0 Training loss: 0.2937 Explore P: 0.7390\n",
      "Episode: 102 Step: 30900 Total reward: 1.0 Training loss: 0.1942 Explore P: 0.7368\n",
      "Episode: 103 Step: 31200 Total reward: 1.0 Training loss: 0.3329 Explore P: 0.7347\n",
      "Episode: 104 Step: 31500 Total reward: 0.0 Training loss: 0.1341 Explore P: 0.7325\n",
      "Episode: 105 Step: 31800 Total reward: 0.0 Training loss: 0.3028 Explore P: 0.7303\n",
      "Episode: 106 Step: 32100 Total reward: -3.0 Training loss: 0.1298 Explore P: 0.7282\n",
      "Episode: 107 Step: 32400 Total reward: 1.0 Training loss: 0.1819 Explore P: 0.7260\n",
      "Episode: 108 Step: 32700 Total reward: 0.0 Training loss: 0.1642 Explore P: 0.7239\n",
      "Episode: 109 Step: 33000 Total reward: 0.0 Training loss: 24.4956 Explore P: 0.7217\n",
      "Episode: 110 Step: 33300 Total reward: 1.0 Training loss: 22.4750 Explore P: 0.7196\n",
      "Episode: 111 Step: 33600 Total reward: 0.0 Training loss: 0.2595 Explore P: 0.7175\n",
      "Episode: 112 Step: 33900 Total reward: 0.0 Training loss: 0.2828 Explore P: 0.7154\n",
      "Episode: 113 Step: 34200 Total reward: 0.0 Training loss: 0.2027 Explore P: 0.7132\n",
      "Episode: 114 Step: 34500 Total reward: 0.0 Training loss: 0.1300 Explore P: 0.7111\n",
      "Episode: 115 Step: 34800 Total reward: 0.0 Training loss: 38.1767 Explore P: 0.7090\n",
      "Episode: 116 Step: 35100 Total reward: 0.0 Training loss: 0.1210 Explore P: 0.7069\n",
      "Episode: 117 Step: 35400 Total reward: -1.0 Training loss: 0.1791 Explore P: 0.7049\n",
      "Episode: 118 Step: 35700 Total reward: -1.0 Training loss: 0.0753 Explore P: 0.7028\n",
      "Episode: 119 Step: 36000 Total reward: 1.0 Training loss: 0.0861 Explore P: 0.7007\n",
      "Episode: 120 Step: 36300 Total reward: 0.0 Training loss: 0.0644 Explore P: 0.6986\n",
      "Episode: 121 Step: 36600 Total reward: 3.0 Training loss: 0.0665 Explore P: 0.6966\n",
      "Episode: 122 Step: 36900 Total reward: 1.0 Training loss: 0.1304 Explore P: 0.6945\n",
      "Episode: 123 Step: 37200 Total reward: -1.0 Training loss: 0.0944 Explore P: 0.6925\n",
      "Episode: 124 Step: 37500 Total reward: 0.0 Training loss: 0.0654 Explore P: 0.6904\n",
      "Episode: 125 Step: 37800 Total reward: -1.0 Training loss: 0.0540 Explore P: 0.6884\n",
      "Episode: 126 Step: 38100 Total reward: 1.0 Training loss: 0.0906 Explore P: 0.6863\n",
      "Episode: 127 Step: 38400 Total reward: 0.0 Training loss: 0.0330 Explore P: 0.6843\n",
      "Episode: 128 Step: 38700 Total reward: 1.0 Training loss: 0.0691 Explore P: 0.6823\n",
      "Episode: 129 Step: 39000 Total reward: 0.0 Training loss: 6.9607 Explore P: 0.6803\n",
      "Episode: 130 Step: 39300 Total reward: 0.0 Training loss: 5.9614 Explore P: 0.6783\n",
      "Episode: 131 Step: 39600 Total reward: 1.0 Training loss: 6.6100 Explore P: 0.6763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 132 Step: 39900 Total reward: 1.0 Training loss: 0.0225 Explore P: 0.6743\n",
      "Episode: 133 Step: 40200 Total reward: 0.0 Training loss: 0.0349 Explore P: 0.6723\n",
      "Episode: 134 Step: 40500 Total reward: 0.0 Training loss: 0.0254 Explore P: 0.6703\n",
      "Episode: 135 Step: 40800 Total reward: 1.0 Training loss: 0.0289 Explore P: 0.6683\n",
      "Episode: 136 Step: 41100 Total reward: 1.0 Training loss: 0.0340 Explore P: 0.6664\n",
      "Episode: 137 Step: 41400 Total reward: 0.0 Training loss: 0.0101 Explore P: 0.6644\n",
      "Episode: 138 Step: 41700 Total reward: 0.0 Training loss: 0.0414 Explore P: 0.6624\n",
      "Episode: 139 Step: 42000 Total reward: 1.0 Training loss: 0.0245 Explore P: 0.6605\n",
      "Episode: 140 Step: 42300 Total reward: 2.0 Training loss: 0.0078 Explore P: 0.6585\n",
      "Episode: 141 Step: 42600 Total reward: -1.0 Training loss: 0.0094 Explore P: 0.6566\n",
      "Episode: 142 Step: 42900 Total reward: 0.0 Training loss: 0.0082 Explore P: 0.6546\n",
      "Episode: 143 Step: 43200 Total reward: 0.0 Training loss: 0.0079 Explore P: 0.6527\n",
      "Episode: 144 Step: 43500 Total reward: 0.0 Training loss: 0.0146 Explore P: 0.6508\n",
      "Episode: 145 Step: 43800 Total reward: 0.0 Training loss: 0.0130 Explore P: 0.6489\n",
      "Episode: 146 Step: 44100 Total reward: 1.0 Training loss: 0.0167 Explore P: 0.6470\n",
      "Episode: 147 Step: 44400 Total reward: -1.0 Training loss: 0.0093 Explore P: 0.6451\n",
      "Episode: 148 Step: 44700 Total reward: 0.0 Training loss: 0.0072 Explore P: 0.6431\n",
      "Episode: 149 Step: 45000 Total reward: 0.0 Training loss: 0.0247 Explore P: 0.6413\n",
      "Episode: 150 Step: 45300 Total reward: 0.0 Training loss: 0.7805 Explore P: 0.6394\n",
      "Episode: 151 Step: 45600 Total reward: 1.0 Training loss: 0.0068 Explore P: 0.6375\n",
      "Episode: 152 Step: 45900 Total reward: 1.0 Training loss: 0.0059 Explore P: 0.6356\n",
      "Episode: 153 Step: 46200 Total reward: 1.0 Training loss: 0.0108 Explore P: 0.6337\n",
      "Episode: 154 Step: 46500 Total reward: 0.0 Training loss: 0.4768 Explore P: 0.6319\n",
      "Episode: 155 Step: 46800 Total reward: 0.0 Training loss: 0.0147 Explore P: 0.6300\n",
      "Episode: 156 Step: 47100 Total reward: 1.0 Training loss: 0.0057 Explore P: 0.6281\n",
      "Episode: 157 Step: 47400 Total reward: 1.0 Training loss: 0.4578 Explore P: 0.6263\n",
      "Episode: 158 Step: 47700 Total reward: 0.0 Training loss: 0.0051 Explore P: 0.6244\n",
      "Episode: 159 Step: 48000 Total reward: 0.0 Training loss: 0.0070 Explore P: 0.6226\n",
      "Episode: 160 Step: 48300 Total reward: 2.0 Training loss: 0.0064 Explore P: 0.6208\n",
      "Episode: 161 Step: 48600 Total reward: 1.0 Training loss: 0.0039 Explore P: 0.6189\n",
      "Episode: 162 Step: 48900 Total reward: 0.0 Training loss: 0.0071 Explore P: 0.6171\n",
      "Episode: 163 Step: 49200 Total reward: 4.0 Training loss: 0.0133 Explore P: 0.6153\n",
      "Episode: 164 Step: 49500 Total reward: -1.0 Training loss: 0.0040 Explore P: 0.6135\n",
      "Episode: 165 Step: 49800 Total reward: 1.0 Training loss: 0.0151 Explore P: 0.6117\n",
      "Episode: 166 Step: 50100 Total reward: 0.0 Training loss: 0.0039 Explore P: 0.6099\n",
      "Episode: 167 Step: 50400 Total reward: 0.0 Training loss: 0.0142 Explore P: 0.6081\n",
      "Episode: 168 Step: 50700 Total reward: -1.0 Training loss: 0.4055 Explore P: 0.6063\n",
      "Episode: 169 Step: 51000 Total reward: 2.0 Training loss: 0.0062 Explore P: 0.6045\n",
      "Episode: 170 Step: 51300 Total reward: 0.0 Training loss: 0.3844 Explore P: 0.6027\n",
      "Episode: 171 Step: 51600 Total reward: 0.0 Training loss: 0.0185 Explore P: 0.6009\n",
      "Episode: 172 Step: 51900 Total reward: 2.0 Training loss: 0.0456 Explore P: 0.5992\n",
      "Episode: 173 Step: 52200 Total reward: -1.0 Training loss: 0.0027 Explore P: 0.5974\n",
      "Episode: 174 Step: 52500 Total reward: -1.0 Training loss: 0.0083 Explore P: 0.5956\n",
      "Episode: 175 Step: 52800 Total reward: 0.0 Training loss: 0.3534 Explore P: 0.5939\n",
      "Episode: 176 Step: 53100 Total reward: -1.0 Training loss: 0.0141 Explore P: 0.5921\n",
      "Episode: 177 Step: 53400 Total reward: 0.0 Training loss: 0.0050 Explore P: 0.5904\n",
      "Episode: 178 Step: 53700 Total reward: 0.0 Training loss: 0.0042 Explore P: 0.5887\n",
      "Episode: 179 Step: 54000 Total reward: 2.0 Training loss: 0.0136 Explore P: 0.5869\n",
      "Episode: 180 Step: 54300 Total reward: 3.0 Training loss: 0.0171 Explore P: 0.5852\n",
      "Episode: 181 Step: 54600 Total reward: 1.0 Training loss: 0.0092 Explore P: 0.5835\n",
      "Episode: 182 Step: 54900 Total reward: 0.0 Training loss: 0.0049 Explore P: 0.5818\n",
      "Episode: 183 Step: 55200 Total reward: 2.0 Training loss: 0.3366 Explore P: 0.5800\n",
      "Episode: 184 Step: 55500 Total reward: 0.0 Training loss: 0.4051 Explore P: 0.5783\n",
      "Episode: 185 Step: 55800 Total reward: 0.0 Training loss: 0.0019 Explore P: 0.5766\n",
      "Episode: 186 Step: 56100 Total reward: 1.0 Training loss: 0.0030 Explore P: 0.5749\n",
      "Episode: 187 Step: 56400 Total reward: 1.0 Training loss: 0.0284 Explore P: 0.5732\n",
      "Episode: 188 Step: 56700 Total reward: 2.0 Training loss: 0.0050 Explore P: 0.5716\n",
      "Episode: 189 Step: 57000 Total reward: 0.0 Training loss: 0.0035 Explore P: 0.5699\n",
      "Episode: 190 Step: 57300 Total reward: 0.0 Training loss: 0.0060 Explore P: 0.5682\n",
      "Episode: 191 Step: 57600 Total reward: 1.0 Training loss: 0.0029 Explore P: 0.5665\n",
      "Episode: 192 Step: 57900 Total reward: 0.0 Training loss: 0.0039 Explore P: 0.5649\n",
      "Episode: 193 Step: 58200 Total reward: 1.0 Training loss: 0.0149 Explore P: 0.5632\n",
      "Episode: 194 Step: 58500 Total reward: 1.0 Training loss: 0.0043 Explore P: 0.5615\n",
      "Episode: 195 Step: 58800 Total reward: 3.0 Training loss: 0.0042 Explore P: 0.5599\n",
      "Episode: 196 Step: 59100 Total reward: -1.0 Training loss: 0.0034 Explore P: 0.5582\n",
      "Episode: 197 Step: 59400 Total reward: -1.0 Training loss: 0.0054 Explore P: 0.5566\n",
      "Episode: 198 Step: 59700 Total reward: 1.0 Training loss: 0.3407 Explore P: 0.5550\n",
      "Episode: 199 Step: 60000 Total reward: 1.0 Training loss: 0.0032 Explore P: 0.5533\n",
      "Episode: 200 Step: 60300 Total reward: 1.0 Training loss: 0.0046 Explore P: 0.5517\n",
      "[[4.4822183 4.4081664 4.4608126 4.4205894]\n",
      " [4.5277004 4.411854  4.4940343 4.3630023]\n",
      " [4.3592696 4.37424   4.421166  4.3648677]\n",
      " [4.230611  4.2297444 4.2497973 4.2041526]\n",
      " [4.3033175 4.2932353 4.3578987 4.286006 ]\n",
      " [4.5424013 4.4603295 4.449973  4.4866757]\n",
      " [4.443037  4.3337035 4.455946  4.3628044]\n",
      " [4.448416  4.307911  4.4693346 4.343758 ]\n",
      " [4.3331137 4.3414426 4.446081  4.4212475]\n",
      " [4.459992  4.4177976 4.47197   4.376777 ]\n",
      " [4.4568357 4.3698335 4.492312  4.467613 ]\n",
      " [4.227009  4.222476  4.26597   4.280269 ]\n",
      " [4.473266  4.4250603 4.504316  4.5364556]\n",
      " [4.3704963 4.379367  4.3709135 4.4034996]\n",
      " [4.3636627 4.4237876 4.4080462 4.3693185]\n",
      " [4.5169373 4.4479413 4.5015874 4.407705 ]\n",
      " [4.353244  4.3192806 4.487746  4.4179506]\n",
      " [4.4531784 4.3987136 4.5709796 4.442168 ]\n",
      " [4.3741174 4.3788924 4.50499   4.395407 ]\n",
      " [4.4384155 4.413107  4.3661346 4.3981595]\n",
      " [4.367847  4.4659877 4.4643674 4.4765625]\n",
      " [4.5009727 4.420841  4.519982  4.504695 ]\n",
      " [4.3002734 4.3309684 4.4503336 4.4204936]\n",
      " [4.5682397 4.392249  4.532495  4.438932 ]\n",
      " [4.4705114 4.2824974 4.405928  4.4102416]\n",
      " [4.4206543 4.4036946 4.560477  4.476615 ]\n",
      " [4.4636555 4.3630548 4.4303    4.4049134]\n",
      " [4.2987556 4.333158  4.4176955 4.433046 ]\n",
      " [4.43046   4.415892  4.505911  4.3989067]\n",
      " [4.3085747 4.274927  4.3436427 4.3265367]\n",
      " [4.3119936 4.3306212 4.4551387 4.3741546]\n",
      " [4.4390454 4.383865  4.451863  4.4093003]\n",
      " [4.275271  4.2396207 4.2278585 4.3158617]\n",
      " [4.4539213 4.4262214 4.444322  4.418966 ]\n",
      " [4.4722815 4.3687406 4.4462557 4.3741364]\n",
      " [4.534857  4.3989377 4.5233245 4.46224  ]\n",
      " [4.461478  4.321401  4.3950186 4.401101 ]\n",
      " [4.3592    4.3025346 4.3416066 4.3793664]\n",
      " [4.323799  4.2965074 4.3789186 4.2761893]\n",
      " [4.456192  4.3230176 4.411642  4.4395537]\n",
      " [4.3593884 4.442442  4.391085  4.299607 ]\n",
      " [4.554219  4.427384  4.456241  4.4102798]\n",
      " [4.432961  4.3691406 4.480765  4.4568877]\n",
      " [4.5400577 4.4647274 4.6160283 4.557445 ]\n",
      " [4.283481  4.3544526 4.3449607 4.257493 ]\n",
      " [4.3141713 4.309812  4.4013376 4.289554 ]\n",
      " [4.351088  4.2815733 4.432667  4.3406186]\n",
      " [4.390553  4.339375  4.4891267 4.3751044]\n",
      " [4.3068266 4.3250628 4.4164987 4.2757177]\n",
      " [4.420144  4.431     4.5547595 4.377282 ]\n",
      " [4.5510807 4.4759088 4.52681   4.489589 ]\n",
      " [4.4329877 4.4672694 4.499385  4.424542 ]\n",
      " [4.397849  4.372287  4.4345937 4.355416 ]\n",
      " [4.3571877 4.392958  4.4668407 4.4611845]\n",
      " [4.7164936 4.4909196 4.568224  4.5478444]\n",
      " [4.397181  4.3626676 4.5099816 4.4124894]\n",
      " [4.3528657 4.2776337 4.3222885 4.367429 ]\n",
      " [4.3567247 4.385324  4.441167  4.430082 ]\n",
      " [4.3611026 4.3128963 4.4092255 4.445502 ]\n",
      " [4.4919415 4.487231  4.5180593 4.4616857]\n",
      " [4.5178595 4.491317  4.493423  4.4607725]\n",
      " [4.1965723 4.2063856 4.253412  4.28849  ]\n",
      " [4.260462  4.2553997 4.301995  4.290973 ]\n",
      " [4.32895   4.296678  4.3055944 4.376983 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 201 Step: 60600 Total reward: 0.0 Training loss: 0.0047 Explore P: 0.5501\n",
      "Episode: 202 Step: 60900 Total reward: 3.0 Training loss: 0.0049 Explore P: 0.5485\n",
      "Episode: 203 Step: 61200 Total reward: 3.0 Training loss: 0.0064 Explore P: 0.5468\n",
      "Episode: 204 Step: 61500 Total reward: -1.0 Training loss: 0.0045 Explore P: 0.5452\n",
      "Episode: 205 Step: 61800 Total reward: 1.0 Training loss: 0.0048 Explore P: 0.5436\n",
      "Episode: 206 Step: 62100 Total reward: 3.0 Training loss: 0.3298 Explore P: 0.5420\n",
      "Episode: 207 Step: 62400 Total reward: 1.0 Training loss: 0.3317 Explore P: 0.5404\n",
      "Episode: 208 Step: 62700 Total reward: 2.0 Training loss: 0.0053 Explore P: 0.5389\n",
      "Episode: 209 Step: 63000 Total reward: 2.0 Training loss: 0.0055 Explore P: 0.5373\n",
      "Episode: 210 Step: 63300 Total reward: 5.0 Training loss: 0.0055 Explore P: 0.5357\n",
      "Episode: 211 Step: 63600 Total reward: 5.0 Training loss: 0.0056 Explore P: 0.5341\n",
      "Episode: 212 Step: 63900 Total reward: 1.0 Training loss: 0.0095 Explore P: 0.5325\n",
      "Episode: 213 Step: 64200 Total reward: 1.0 Training loss: 0.0039 Explore P: 0.5310\n",
      "Episode: 214 Step: 64500 Total reward: 2.0 Training loss: 0.0041 Explore P: 0.5294\n",
      "Episode: 215 Step: 64800 Total reward: 1.0 Training loss: 0.0100 Explore P: 0.5279\n",
      "Episode: 216 Step: 65100 Total reward: 6.0 Training loss: 0.0084 Explore P: 0.5263\n",
      "Episode: 217 Step: 65400 Total reward: 4.0 Training loss: 0.0040 Explore P: 0.5248\n",
      "Episode: 218 Step: 65700 Total reward: 5.0 Training loss: 0.0259 Explore P: 0.5232\n",
      "Episode: 219 Step: 66000 Total reward: 1.0 Training loss: 0.0047 Explore P: 0.5217\n",
      "Episode: 220 Step: 66300 Total reward: 0.0 Training loss: 0.0055 Explore P: 0.5202\n",
      "Episode: 221 Step: 66600 Total reward: 5.0 Training loss: 0.0046 Explore P: 0.5186\n",
      "Episode: 222 Step: 66900 Total reward: 2.0 Training loss: 0.0082 Explore P: 0.5171\n",
      "Episode: 223 Step: 67200 Total reward: 6.0 Training loss: 0.0042 Explore P: 0.5156\n",
      "Episode: 224 Step: 67500 Total reward: 2.0 Training loss: 0.0127 Explore P: 0.5141\n",
      "Episode: 225 Step: 67800 Total reward: 0.0 Training loss: 0.0192 Explore P: 0.5126\n",
      "Episode: 226 Step: 68100 Total reward: 0.0 Training loss: 0.0317 Explore P: 0.5110\n",
      "Episode: 227 Step: 68400 Total reward: 3.0 Training loss: 0.0040 Explore P: 0.5095\n",
      "Episode: 228 Step: 68700 Total reward: 2.0 Training loss: 0.0062 Explore P: 0.5081\n",
      "Episode: 229 Step: 69000 Total reward: 5.0 Training loss: 0.4314 Explore P: 0.5066\n",
      "Episode: 230 Step: 69300 Total reward: 2.0 Training loss: 0.0049 Explore P: 0.5051\n",
      "Episode: 231 Step: 69600 Total reward: 2.0 Training loss: 0.4263 Explore P: 0.5036\n",
      "Episode: 232 Step: 69900 Total reward: 1.0 Training loss: 0.4110 Explore P: 0.5021\n",
      "Episode: 233 Step: 70200 Total reward: 0.0 Training loss: 0.0041 Explore P: 0.5006\n",
      "Episode: 234 Step: 70500 Total reward: 6.0 Training loss: 0.0120 Explore P: 0.4992\n",
      "Episode: 235 Step: 70800 Total reward: 3.0 Training loss: 0.3948 Explore P: 0.4977\n",
      "Episode: 236 Step: 71100 Total reward: 0.0 Training loss: 0.0221 Explore P: 0.4962\n",
      "Episode: 237 Step: 71400 Total reward: 1.0 Training loss: 0.0046 Explore P: 0.4948\n",
      "Episode: 238 Step: 71700 Total reward: 5.0 Training loss: 0.0185 Explore P: 0.4933\n",
      "Episode: 239 Step: 72000 Total reward: 2.0 Training loss: 0.0040 Explore P: 0.4919\n",
      "Episode: 240 Step: 72300 Total reward: -2.0 Training loss: 0.0049 Explore P: 0.4904\n",
      "Episode: 241 Step: 72600 Total reward: 2.0 Training loss: 0.0056 Explore P: 0.4890\n",
      "Episode: 242 Step: 72900 Total reward: 2.0 Training loss: 0.0033 Explore P: 0.4876\n",
      "Episode: 243 Step: 73200 Total reward: 7.0 Training loss: 0.0047 Explore P: 0.4861\n",
      "Episode: 244 Step: 73500 Total reward: -1.0 Training loss: 0.0370 Explore P: 0.4847\n",
      "Episode: 245 Step: 73800 Total reward: 4.0 Training loss: 0.4122 Explore P: 0.4833\n",
      "Episode: 246 Step: 74100 Total reward: 0.0 Training loss: 0.0081 Explore P: 0.4819\n",
      "Episode: 247 Step: 74400 Total reward: 0.0 Training loss: 0.0040 Explore P: 0.4805\n",
      "Episode: 248 Step: 74700 Total reward: 4.0 Training loss: 0.0047 Explore P: 0.4790\n",
      "Episode: 249 Step: 75000 Total reward: 0.0 Training loss: 0.0042 Explore P: 0.4776\n",
      "Episode: 250 Step: 75300 Total reward: 1.0 Training loss: 0.4620 Explore P: 0.4762\n",
      "Episode: 251 Step: 75600 Total reward: 0.0 Training loss: 0.0110 Explore P: 0.4748\n",
      "Episode: 252 Step: 75900 Total reward: 4.0 Training loss: 0.4130 Explore P: 0.4735\n",
      "Episode: 253 Step: 76200 Total reward: 1.0 Training loss: 0.0079 Explore P: 0.4721\n",
      "Episode: 254 Step: 76500 Total reward: 4.0 Training loss: 0.0416 Explore P: 0.4707\n",
      "Episode: 255 Step: 76800 Total reward: 4.0 Training loss: 0.4150 Explore P: 0.4693\n",
      "Episode: 256 Step: 77100 Total reward: 2.0 Training loss: 0.0050 Explore P: 0.4679\n",
      "Episode: 257 Step: 77400 Total reward: 1.0 Training loss: 0.4451 Explore P: 0.4666\n",
      "Episode: 258 Step: 77700 Total reward: 1.0 Training loss: 0.4252 Explore P: 0.4652\n",
      "Episode: 259 Step: 78000 Total reward: 3.0 Training loss: 0.0063 Explore P: 0.4638\n",
      "Episode: 260 Step: 78300 Total reward: 6.0 Training loss: 0.0068 Explore P: 0.4625\n",
      "Episode: 261 Step: 78600 Total reward: 3.0 Training loss: 0.6033 Explore P: 0.4611\n",
      "Episode: 262 Step: 78900 Total reward: 3.0 Training loss: 0.0053 Explore P: 0.4598\n",
      "Episode: 263 Step: 79200 Total reward: 6.0 Training loss: 0.0082 Explore P: 0.4584\n",
      "Episode: 264 Step: 79500 Total reward: 3.0 Training loss: 0.0079 Explore P: 0.4571\n",
      "Episode: 265 Step: 79800 Total reward: 1.0 Training loss: 0.0064 Explore P: 0.4557\n",
      "Episode: 266 Step: 80100 Total reward: 2.0 Training loss: 0.0111 Explore P: 0.4544\n",
      "Episode: 267 Step: 80400 Total reward: -1.0 Training loss: 0.0021 Explore P: 0.4531\n",
      "Episode: 268 Step: 80700 Total reward: 2.0 Training loss: 0.0047 Explore P: 0.4517\n",
      "Episode: 269 Step: 81000 Total reward: 5.0 Training loss: 0.0220 Explore P: 0.4504\n",
      "Episode: 270 Step: 81300 Total reward: 6.0 Training loss: 0.5305 Explore P: 0.4491\n",
      "Episode: 271 Step: 81600 Total reward: 2.0 Training loss: 0.0061 Explore P: 0.4478\n",
      "Episode: 272 Step: 81900 Total reward: -1.0 Training loss: 0.0086 Explore P: 0.4465\n",
      "Episode: 273 Step: 82200 Total reward: 2.0 Training loss: 0.0047 Explore P: 0.4452\n",
      "Episode: 274 Step: 82500 Total reward: 3.0 Training loss: 0.0069 Explore P: 0.4439\n",
      "Episode: 275 Step: 82800 Total reward: 6.0 Training loss: 0.0253 Explore P: 0.4426\n",
      "Episode: 276 Step: 83100 Total reward: 3.0 Training loss: 0.0107 Explore P: 0.4413\n",
      "Episode: 277 Step: 83400 Total reward: 6.0 Training loss: 0.0040 Explore P: 0.4400\n",
      "Episode: 278 Step: 83700 Total reward: 7.0 Training loss: 0.0182 Explore P: 0.4387\n",
      "Episode: 279 Step: 84000 Total reward: 0.0 Training loss: 0.0090 Explore P: 0.4374\n",
      "Episode: 280 Step: 84300 Total reward: 3.0 Training loss: 0.0076 Explore P: 0.4361\n",
      "Episode: 281 Step: 84600 Total reward: 5.0 Training loss: 0.0050 Explore P: 0.4348\n",
      "Episode: 282 Step: 84900 Total reward: 1.0 Training loss: 0.0236 Explore P: 0.4336\n",
      "Episode: 283 Step: 85200 Total reward: 1.0 Training loss: 0.0044 Explore P: 0.4323\n",
      "Episode: 284 Step: 85500 Total reward: 3.0 Training loss: 0.4294 Explore P: 0.4310\n",
      "Episode: 285 Step: 85800 Total reward: 1.0 Training loss: 0.0104 Explore P: 0.4298\n",
      "Episode: 286 Step: 86100 Total reward: 5.0 Training loss: 0.0046 Explore P: 0.4285\n",
      "Episode: 287 Step: 86400 Total reward: 2.0 Training loss: 0.5522 Explore P: 0.4273\n",
      "Episode: 288 Step: 86700 Total reward: 3.0 Training loss: 0.0125 Explore P: 0.4260\n",
      "Episode: 289 Step: 87000 Total reward: 3.0 Training loss: 0.5911 Explore P: 0.4248\n",
      "Episode: 290 Step: 87300 Total reward: 5.0 Training loss: 0.5344 Explore P: 0.4235\n",
      "Episode: 291 Step: 87600 Total reward: 2.0 Training loss: 0.0196 Explore P: 0.4223\n",
      "Episode: 292 Step: 87900 Total reward: 3.0 Training loss: 0.0047 Explore P: 0.4210\n",
      "Episode: 293 Step: 88200 Total reward: 10.0 Training loss: 0.0043 Explore P: 0.4198\n",
      "Episode: 294 Step: 88500 Total reward: 3.0 Training loss: 0.4708 Explore P: 0.4186\n",
      "Episode: 295 Step: 88800 Total reward: 3.0 Training loss: 0.0133 Explore P: 0.4174\n",
      "Episode: 296 Step: 89100 Total reward: 5.0 Training loss: 0.0113 Explore P: 0.4161\n",
      "Episode: 297 Step: 89400 Total reward: 9.0 Training loss: 0.0112 Explore P: 0.4149\n",
      "Episode: 298 Step: 89700 Total reward: 5.0 Training loss: 0.6089 Explore P: 0.4137\n",
      "Episode: 299 Step: 90000 Total reward: 4.0 Training loss: 0.0064 Explore P: 0.4125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300 Step: 90300 Total reward: 3.0 Training loss: 0.0093 Explore P: 0.4113\n",
      "[[5.4182587 5.4903746 5.420751  5.3342366]\n",
      " [5.112575  5.1782117 5.172853  5.169282 ]\n",
      " [5.1260657 5.074095  5.1716204 5.1305065]\n",
      " [5.146574  5.268119  5.3786025 5.2481422]\n",
      " [5.1045094 5.1741505 5.225743  5.1806955]\n",
      " [5.1949167 5.2543674 5.317104  5.2322626]\n",
      " [5.200549  5.189224  5.2944713 5.1808763]\n",
      " [5.1207094 5.0574408 5.2026215 5.138609 ]\n",
      " [5.5345097 5.5321803 5.3916063 5.3933787]\n",
      " [5.2493687 5.319669  5.2346635 5.39453  ]\n",
      " [5.038951  5.1189265 5.1189528 5.17536  ]\n",
      " [5.4635453 5.4307284 5.520057  5.462821 ]\n",
      " [5.2949014 5.3066764 5.371245  5.34429  ]\n",
      " [5.0859895 5.2005553 5.2810287 5.1281986]\n",
      " [5.1208005 5.21648   5.1959176 5.3149695]\n",
      " [5.339264  5.3397727 5.2850056 5.315044 ]\n",
      " [5.4625406 5.2748647 5.4850183 5.240383 ]\n",
      " [5.3015347 5.486179  5.3895926 5.39131  ]\n",
      " [5.3391414 5.3737316 5.4860682 5.3829784]\n",
      " [5.1650753 5.22003   5.323519  5.214961 ]\n",
      " [5.0704594 5.1434636 5.1416144 5.2086763]\n",
      " [5.0902157 5.098717  5.1226645 5.2062798]\n",
      " [5.377557  5.513045  5.388357  5.431638 ]\n",
      " [5.128237  5.1547256 5.183153  5.1243277]\n",
      " [5.3414965 5.4460077 5.3416605 5.3884325]\n",
      " [5.6447854 5.481053  5.3901343 5.522193 ]\n",
      " [5.472604  5.4679456 5.47102   5.4740434]\n",
      " [5.127182  5.1945915 5.16334   5.126718 ]\n",
      " [5.8096843 5.447141  5.633293  5.4216948]\n",
      " [5.3147936 5.407732  5.3589644 5.388325 ]\n",
      " [5.133362  5.216922  5.1788645 5.371929 ]\n",
      " [5.789669  5.58532   5.551954  5.480529 ]\n",
      " [5.5944366 5.4975576 5.541756  5.391683 ]\n",
      " [5.2117467 5.2364697 5.2946854 5.2647815]\n",
      " [5.1564693 5.1519713 5.249877  5.1230574]\n",
      " [5.270606  5.269633  5.3703437 5.28706  ]\n",
      " [5.185356  5.227414  5.208597  5.274867 ]\n",
      " [5.119399  5.157144  5.2840624 5.2117987]\n",
      " [5.131364  5.254963  5.2819085 5.158846 ]\n",
      " [5.2900157 5.3719764 5.3246117 5.2620955]\n",
      " [5.1454706 5.206053  5.1768417 5.229122 ]\n",
      " [5.2037625 5.2731028 5.3458977 5.216141 ]\n",
      " [5.326741  5.438338  5.348135  5.390362 ]\n",
      " [5.1059227 5.159022  5.255652  5.2133417]\n",
      " [5.099902  5.1851544 5.2154384 5.128296 ]\n",
      " [5.3287086 5.3856473 5.5086417 5.2604227]\n",
      " [5.1752996 5.3370776 5.220412  5.258195 ]\n",
      " [5.360262  5.3813624 5.3750396 5.2668147]\n",
      " [5.1373262 5.1854978 5.244686  5.2334313]\n",
      " [5.3893642 5.397592  5.4033318 5.449244 ]\n",
      " [5.064477  5.188593  5.242672  5.19535  ]\n",
      " [5.23922   5.203264  5.3094754 5.1628714]\n",
      " [5.1680045 5.1795754 5.279574  5.242501 ]\n",
      " [5.3273187 5.4318056 5.334495  5.3452854]\n",
      " [5.2501974 5.3273616 5.218327  5.3090014]\n",
      " [5.143451  5.22605   5.2716575 5.3839445]\n",
      " [5.411105  5.2681317 5.345815  5.3634214]\n",
      " [5.189725  5.1692076 5.2701197 5.164295 ]\n",
      " [5.2698364 5.3171134 5.3785033 5.292596 ]\n",
      " [5.2063007 5.3431845 5.321735  5.396689 ]\n",
      " [6.0318046 5.765127  5.663125  5.796588 ]\n",
      " [5.0999775 5.080053  5.2129817 5.206236 ]\n",
      " [5.3364964 5.3348947 5.285358  5.4639425]\n",
      " [5.253727  5.373688  5.38291   5.3789787]]\n",
      "Episode: 301 Step: 90600 Total reward: 5.0 Training loss: 0.0210 Explore P: 0.4101\n",
      "Episode: 302 Step: 90900 Total reward: 1.0 Training loss: 0.9569 Explore P: 0.4089\n",
      "Episode: 303 Step: 91200 Total reward: 0.0 Training loss: 0.0090 Explore P: 0.4077\n",
      "Episode: 304 Step: 91500 Total reward: 5.0 Training loss: 0.0098 Explore P: 0.4065\n",
      "Episode: 305 Step: 91800 Total reward: 6.0 Training loss: 0.0296 Explore P: 0.4053\n",
      "Episode: 306 Step: 92100 Total reward: 3.0 Training loss: 0.0061 Explore P: 0.4041\n",
      "Episode: 307 Step: 92400 Total reward: 6.0 Training loss: 0.0053 Explore P: 0.4030\n",
      "Episode: 308 Step: 92700 Total reward: 3.0 Training loss: 0.0086 Explore P: 0.4018\n",
      "Episode: 309 Step: 93000 Total reward: 0.0 Training loss: 0.0125 Explore P: 0.4006\n",
      "Episode: 310 Step: 93300 Total reward: 4.0 Training loss: 0.0098 Explore P: 0.3994\n",
      "Episode: 311 Step: 93600 Total reward: 4.0 Training loss: 0.0092 Explore P: 0.3983\n",
      "Episode: 312 Step: 93900 Total reward: 4.0 Training loss: 0.0059 Explore P: 0.3971\n",
      "Episode: 313 Step: 94200 Total reward: 3.0 Training loss: 0.0147 Explore P: 0.3959\n",
      "Episode: 314 Step: 94500 Total reward: 0.0 Training loss: 0.0038 Explore P: 0.3948\n",
      "Episode: 315 Step: 94800 Total reward: 4.0 Training loss: 0.0045 Explore P: 0.3936\n",
      "Episode: 316 Step: 95100 Total reward: 3.0 Training loss: 0.0182 Explore P: 0.3925\n",
      "Episode: 317 Step: 95400 Total reward: 1.0 Training loss: 0.0096 Explore P: 0.3913\n",
      "Episode: 318 Step: 95700 Total reward: 2.0 Training loss: 0.0034 Explore P: 0.3902\n",
      "Episode: 319 Step: 96000 Total reward: 11.0 Training loss: 0.0052 Explore P: 0.3891\n",
      "Episode: 320 Step: 96300 Total reward: 9.0 Training loss: 0.5152 Explore P: 0.3879\n",
      "Episode: 321 Step: 96600 Total reward: 8.0 Training loss: 0.0217 Explore P: 0.3868\n",
      "Episode: 322 Step: 96900 Total reward: 0.0 Training loss: 0.0154 Explore P: 0.3857\n",
      "Episode: 323 Step: 97200 Total reward: 2.0 Training loss: 0.6268 Explore P: 0.3845\n",
      "Episode: 324 Step: 97500 Total reward: 6.0 Training loss: 0.0222 Explore P: 0.3834\n",
      "Episode: 325 Step: 97800 Total reward: 1.0 Training loss: 0.5651 Explore P: 0.3823\n",
      "Episode: 326 Step: 98100 Total reward: 1.0 Training loss: 0.4912 Explore P: 0.3812\n",
      "Episode: 327 Step: 98400 Total reward: 1.0 Training loss: 0.0065 Explore P: 0.3801\n",
      "Episode: 328 Step: 98700 Total reward: 9.0 Training loss: 0.0067 Explore P: 0.3790\n",
      "Episode: 329 Step: 99000 Total reward: 9.0 Training loss: 1.1759 Explore P: 0.3779\n",
      "Episode: 330 Step: 99300 Total reward: 6.0 Training loss: 0.0065 Explore P: 0.3768\n",
      "Episode: 331 Step: 99600 Total reward: 9.0 Training loss: 0.0072 Explore P: 0.3757\n",
      "Episode: 332 Step: 99900 Total reward: 3.0 Training loss: 0.9755 Explore P: 0.3746\n",
      "Episode: 333 Step: 100200 Total reward: 11.0 Training loss: 0.0059 Explore P: 0.3735\n",
      "Episode: 334 Step: 100500 Total reward: 5.0 Training loss: 0.0068 Explore P: 0.3724\n",
      "Episode: 335 Step: 100800 Total reward: 4.0 Training loss: 0.0045 Explore P: 0.3713\n",
      "Episode: 336 Step: 101100 Total reward: 2.0 Training loss: 0.4457 Explore P: 0.3702\n",
      "Episode: 337 Step: 101400 Total reward: 3.0 Training loss: 0.0059 Explore P: 0.3691\n",
      "Episode: 338 Step: 101700 Total reward: 3.0 Training loss: 0.0129 Explore P: 0.3681\n",
      "Episode: 339 Step: 102000 Total reward: 4.0 Training loss: 0.4857 Explore P: 0.3670\n",
      "Episode: 340 Step: 102300 Total reward: 1.0 Training loss: 0.0054 Explore P: 0.3659\n",
      "Episode: 341 Step: 102600 Total reward: 7.0 Training loss: 0.5618 Explore P: 0.3649\n",
      "Episode: 342 Step: 102900 Total reward: 9.0 Training loss: 0.0188 Explore P: 0.3638\n",
      "Episode: 343 Step: 103200 Total reward: 3.0 Training loss: 0.0151 Explore P: 0.3627\n",
      "Episode: 344 Step: 103500 Total reward: 7.0 Training loss: 0.0089 Explore P: 0.3617\n",
      "Episode: 345 Step: 103800 Total reward: 8.0 Training loss: 0.5460 Explore P: 0.3606\n",
      "Episode: 346 Step: 104100 Total reward: 6.0 Training loss: 0.0085 Explore P: 0.3596\n",
      "Episode: 347 Step: 104400 Total reward: 4.0 Training loss: 0.0219 Explore P: 0.3585\n",
      "Episode: 348 Step: 104700 Total reward: 5.0 Training loss: 0.0088 Explore P: 0.3575\n",
      "Episode: 349 Step: 105000 Total reward: 9.0 Training loss: 0.0148 Explore P: 0.3564\n",
      "Episode: 350 Step: 105300 Total reward: 6.0 Training loss: 0.4937 Explore P: 0.3554\n",
      "Episode: 351 Step: 105600 Total reward: 4.0 Training loss: 0.0049 Explore P: 0.3544\n",
      "Episode: 352 Step: 105900 Total reward: 5.0 Training loss: 0.0302 Explore P: 0.3533\n",
      "Episode: 353 Step: 106200 Total reward: 14.0 Training loss: 0.0120 Explore P: 0.3523\n",
      "Episode: 354 Step: 106500 Total reward: 8.0 Training loss: 0.0127 Explore P: 0.3513\n",
      "Episode: 355 Step: 106800 Total reward: 2.0 Training loss: 0.0205 Explore P: 0.3503\n",
      "Episode: 356 Step: 107100 Total reward: 3.0 Training loss: 0.5584 Explore P: 0.3492\n",
      "Episode: 357 Step: 107400 Total reward: 5.0 Training loss: 0.0220 Explore P: 0.3482\n",
      "Episode: 358 Step: 107700 Total reward: 7.0 Training loss: 0.6827 Explore P: 0.3472\n",
      "Episode: 359 Step: 108000 Total reward: 0.0 Training loss: 0.0187 Explore P: 0.3462\n",
      "Episode: 360 Step: 108300 Total reward: 2.0 Training loss: 0.0116 Explore P: 0.3452\n",
      "Episode: 361 Step: 108600 Total reward: 1.0 Training loss: 0.0084 Explore P: 0.3442\n",
      "Episode: 362 Step: 108900 Total reward: 7.0 Training loss: 0.0068 Explore P: 0.3432\n",
      "Episode: 363 Step: 109200 Total reward: 3.0 Training loss: 0.0243 Explore P: 0.3422\n",
      "Episode: 364 Step: 109500 Total reward: 5.0 Training loss: 0.0066 Explore P: 0.3412\n",
      "Episode: 365 Step: 109800 Total reward: 2.0 Training loss: 0.0313 Explore P: 0.3402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 366 Step: 110100 Total reward: 7.0 Training loss: 0.0103 Explore P: 0.3392\n",
      "Episode: 367 Step: 110400 Total reward: 2.0 Training loss: 0.0227 Explore P: 0.3382\n",
      "Episode: 368 Step: 110700 Total reward: 1.0 Training loss: 0.5493 Explore P: 0.3372\n",
      "Episode: 369 Step: 111000 Total reward: 4.0 Training loss: 0.0088 Explore P: 0.3363\n",
      "Episode: 370 Step: 111300 Total reward: 6.0 Training loss: 0.0062 Explore P: 0.3353\n",
      "Episode: 371 Step: 111600 Total reward: 2.0 Training loss: 0.0362 Explore P: 0.3343\n",
      "Episode: 372 Step: 111900 Total reward: 4.0 Training loss: 0.6433 Explore P: 0.3333\n",
      "Episode: 373 Step: 112200 Total reward: 6.0 Training loss: 0.0058 Explore P: 0.3324\n",
      "Episode: 374 Step: 112500 Total reward: 6.0 Training loss: 0.0143 Explore P: 0.3314\n",
      "Episode: 375 Step: 112800 Total reward: 13.0 Training loss: 0.5001 Explore P: 0.3304\n",
      "Episode: 376 Step: 113100 Total reward: 6.0 Training loss: 0.5683 Explore P: 0.3295\n",
      "Episode: 377 Step: 113400 Total reward: 5.0 Training loss: 0.0101 Explore P: 0.3285\n",
      "Episode: 378 Step: 113700 Total reward: 6.0 Training loss: 0.0290 Explore P: 0.3276\n",
      "Episode: 379 Step: 114000 Total reward: 7.0 Training loss: 0.0066 Explore P: 0.3266\n",
      "Episode: 380 Step: 114300 Total reward: 2.0 Training loss: 0.0088 Explore P: 0.3257\n",
      "Episode: 381 Step: 114600 Total reward: 1.0 Training loss: 0.0221 Explore P: 0.3247\n",
      "Episode: 382 Step: 114900 Total reward: 3.0 Training loss: 0.0110 Explore P: 0.3238\n",
      "Episode: 383 Step: 115200 Total reward: 2.0 Training loss: 0.0086 Explore P: 0.3228\n",
      "Episode: 384 Step: 115500 Total reward: 8.0 Training loss: 0.0201 Explore P: 0.3219\n",
      "Episode: 385 Step: 115800 Total reward: 3.0 Training loss: 0.0153 Explore P: 0.3210\n",
      "Episode: 386 Step: 116100 Total reward: 7.0 Training loss: 0.6689 Explore P: 0.3200\n",
      "Episode: 387 Step: 116400 Total reward: 7.0 Training loss: 0.6272 Explore P: 0.3191\n",
      "Episode: 388 Step: 116700 Total reward: 5.0 Training loss: 0.0161 Explore P: 0.3182\n",
      "Episode: 389 Step: 117000 Total reward: 1.0 Training loss: 0.0213 Explore P: 0.3173\n",
      "Episode: 390 Step: 117300 Total reward: 5.0 Training loss: 0.0096 Explore P: 0.3163\n",
      "Episode: 391 Step: 117600 Total reward: 0.0 Training loss: 1.1592 Explore P: 0.3154\n",
      "Episode: 392 Step: 117900 Total reward: 10.0 Training loss: 0.6030 Explore P: 0.3145\n",
      "Episode: 393 Step: 118200 Total reward: 0.0 Training loss: 0.0096 Explore P: 0.3136\n",
      "Episode: 394 Step: 118500 Total reward: 7.0 Training loss: 0.0126 Explore P: 0.3127\n",
      "Episode: 395 Step: 118800 Total reward: 2.0 Training loss: 1.2149 Explore P: 0.3118\n",
      "Episode: 396 Step: 119100 Total reward: 4.0 Training loss: 0.0081 Explore P: 0.3109\n",
      "Episode: 397 Step: 119400 Total reward: 14.0 Training loss: 0.0129 Explore P: 0.3100\n",
      "Episode: 398 Step: 119700 Total reward: 8.0 Training loss: 0.5557 Explore P: 0.3091\n",
      "Episode: 399 Step: 120000 Total reward: 7.0 Training loss: 0.6317 Explore P: 0.3082\n",
      "Episode: 400 Step: 120300 Total reward: 5.0 Training loss: 0.0318 Explore P: 0.3073\n",
      "[[6.231669  6.2195396 6.218599  6.1885867]\n",
      " [6.147629  6.178416  6.1126065 6.138276 ]\n",
      " [6.2289596 6.2586207 6.162155  6.2409368]\n",
      " [6.489712  6.4975615 6.6917186 6.260947 ]\n",
      " [6.2499743 6.230789  6.190423  6.2551312]\n",
      " [6.08598   6.1426697 6.0753717 6.0951486]\n",
      " [6.134166  6.1314335 6.039348  6.1682553]\n",
      " [6.1006985 6.1296597 5.888238  6.205292 ]\n",
      " [6.233559  6.270005  6.1409693 6.286413 ]\n",
      " [6.097232  6.203327  6.0666785 6.1157427]\n",
      " [6.262749  6.3435764 6.105072  6.266786 ]\n",
      " [6.2019587 6.1826086 6.0681987 6.3061   ]\n",
      " [6.283887  6.204794  6.173641  6.1955495]\n",
      " [6.219178  6.228685  6.128041  6.156674 ]\n",
      " [6.161599  6.131868  6.2589917 6.1385775]\n",
      " [6.270055  6.384008  6.2672706 6.201209 ]\n",
      " [6.273193  6.2041173 6.1659284 6.2457004]\n",
      " [6.6902137 6.5014877 6.316271  6.3558345]\n",
      " [6.156897  6.1786594 6.041558  6.170486 ]\n",
      " [6.3770394 6.36676   6.2310424 6.3249836]\n",
      " [6.170431  6.170912  6.0989933 6.199758 ]\n",
      " [6.044706  6.05223   6.14302   6.1313396]\n",
      " [6.3255873 6.283166  6.2184677 6.2404923]\n",
      " [6.81369   6.6134872 6.3799825 6.5655074]\n",
      " [6.0902452 6.1473227 6.0136056 6.2048182]\n",
      " [6.298838  6.2886567 6.236014  6.2426333]\n",
      " [6.225074  6.1676064 6.162529  6.1429725]\n",
      " [6.2205796 6.273107  6.1471944 6.226026 ]\n",
      " [6.2898355 6.2700977 6.1845365 6.234221 ]\n",
      " [6.270563  6.2189093 6.1288896 6.3037744]\n",
      " [6.2073994 6.2689066 6.0919237 6.282494 ]\n",
      " [6.181396  6.2016144 6.0035305 6.2138715]\n",
      " [6.1402493 6.265398  6.2332177 6.044887 ]\n",
      " [6.1715374 6.157488  6.2281384 6.206222 ]\n",
      " [6.117716  6.074808  6.1227484 6.126903 ]\n",
      " [6.0696654 6.12278   6.0388045 6.136171 ]\n",
      " [6.393869  6.4382153 6.2421565 6.3466334]\n",
      " [6.228285  6.2523956 6.101438  6.2383647]\n",
      " [6.431087  6.4426622 6.321     6.2638884]\n",
      " [6.076661  6.1144342 6.0537887 6.0635266]\n",
      " [6.2948117 6.2667856 6.3205156 6.3488145]\n",
      " [6.1067543 6.0712395 6.155914  6.106117 ]\n",
      " [6.1618147 6.1721134 6.095317  6.157152 ]\n",
      " [6.2266593 6.187171  6.1873846 6.1530337]\n",
      " [6.1676226 6.226912  6.1407166 6.1454654]\n",
      " [6.1051226 6.1767697 6.078387  6.2487807]\n",
      " [6.06443   6.0517836 6.037498  6.163323 ]\n",
      " [6.378935  6.4774446 6.436252  6.261084 ]\n",
      " [6.1904826 6.2520585 6.358925  6.164145 ]\n",
      " [6.0737295 6.10564   6.05848   6.0656843]\n",
      " [6.0495443 6.0871572 6.102423  6.099644 ]\n",
      " [6.1057873 6.1152096 6.12747   6.108946 ]\n",
      " [6.125614  6.107042  6.0900626 6.118288 ]\n",
      " [6.1522436 6.216034  6.171226  6.1634083]\n",
      " [6.644225  6.5230455 6.3613286 6.46861  ]\n",
      " [6.487195  6.32891   6.236696  6.281319 ]\n",
      " [6.2419896 6.271332  6.2659574 6.2493644]\n",
      " [6.3523674 6.3147097 6.228203  6.2633376]\n",
      " [6.109951  6.1460114 5.990704  6.1832967]\n",
      " [6.09341   6.153323  6.052857  6.0851564]\n",
      " [6.165019  6.144189  6.210791  6.123902 ]\n",
      " [6.4555087 6.3801703 6.2408013 6.2960305]\n",
      " [6.1762767 6.165944  6.034123  6.210445 ]\n",
      " [6.1621737 6.2999897 6.1648345 6.304559 ]]\n",
      "Episode: 401 Step: 120600 Total reward: 6.0 Training loss: 0.0204 Explore P: 0.3064\n",
      "Episode: 402 Step: 120900 Total reward: 7.0 Training loss: 0.0145 Explore P: 0.3055\n",
      "Episode: 403 Step: 121200 Total reward: 4.0 Training loss: 0.0058 Explore P: 0.3046\n",
      "Episode: 404 Step: 121500 Total reward: 7.0 Training loss: 0.0119 Explore P: 0.3037\n",
      "Episode: 405 Step: 121800 Total reward: 6.0 Training loss: 0.0198 Explore P: 0.3029\n",
      "Episode: 406 Step: 122100 Total reward: 4.0 Training loss: 0.0066 Explore P: 0.3020\n",
      "Episode: 407 Step: 122400 Total reward: 3.0 Training loss: 0.5139 Explore P: 0.3011\n",
      "Episode: 408 Step: 122700 Total reward: 6.0 Training loss: 0.0237 Explore P: 0.3002\n",
      "Episode: 409 Step: 123000 Total reward: 1.0 Training loss: 0.0221 Explore P: 0.2994\n",
      "Episode: 410 Step: 123300 Total reward: 2.0 Training loss: 0.0099 Explore P: 0.2985\n",
      "Episode: 411 Step: 123600 Total reward: 8.0 Training loss: 0.0101 Explore P: 0.2976\n",
      "Episode: 412 Step: 123900 Total reward: 1.0 Training loss: 0.0164 Explore P: 0.2968\n",
      "Episode: 413 Step: 124200 Total reward: 6.0 Training loss: 1.0546 Explore P: 0.2959\n",
      "Episode: 414 Step: 124500 Total reward: -1.0 Training loss: 0.0174 Explore P: 0.2951\n",
      "Episode: 415 Step: 124800 Total reward: 2.0 Training loss: 0.0076 Explore P: 0.2942\n",
      "Episode: 416 Step: 125100 Total reward: 4.0 Training loss: 0.0140 Explore P: 0.2934\n",
      "Episode: 417 Step: 125400 Total reward: 10.0 Training loss: 0.0192 Explore P: 0.2925\n",
      "Episode: 418 Step: 125700 Total reward: 5.0 Training loss: 0.6164 Explore P: 0.2917\n",
      "Episode: 419 Step: 126000 Total reward: 10.0 Training loss: 0.0160 Explore P: 0.2908\n",
      "Episode: 420 Step: 126300 Total reward: 7.0 Training loss: 0.5650 Explore P: 0.2900\n",
      "Episode: 421 Step: 126600 Total reward: 1.0 Training loss: 0.0244 Explore P: 0.2891\n",
      "Episode: 422 Step: 126900 Total reward: 3.0 Training loss: 0.0238 Explore P: 0.2883\n",
      "Episode: 423 Step: 127200 Total reward: 8.0 Training loss: 0.7048 Explore P: 0.2875\n",
      "Episode: 424 Step: 127500 Total reward: 2.0 Training loss: 0.0137 Explore P: 0.2866\n",
      "Episode: 425 Step: 127800 Total reward: 10.0 Training loss: 0.0277 Explore P: 0.2858\n",
      "Episode: 426 Step: 128100 Total reward: 5.0 Training loss: 0.0097 Explore P: 0.2850\n",
      "Episode: 427 Step: 128400 Total reward: 4.0 Training loss: 0.0118 Explore P: 0.2842\n",
      "Episode: 428 Step: 128700 Total reward: 15.0 Training loss: 0.0075 Explore P: 0.2833\n",
      "Episode: 429 Step: 129000 Total reward: 9.0 Training loss: 0.0068 Explore P: 0.2825\n",
      "Episode: 430 Step: 129300 Total reward: 13.0 Training loss: 0.0114 Explore P: 0.2817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 431 Step: 129600 Total reward: 12.0 Training loss: 0.0104 Explore P: 0.2809\n",
      "Episode: 432 Step: 129900 Total reward: 10.0 Training loss: 0.0116 Explore P: 0.2801\n",
      "Episode: 433 Step: 130200 Total reward: 3.0 Training loss: 0.0077 Explore P: 0.2793\n",
      "Episode: 434 Step: 130500 Total reward: 2.0 Training loss: 0.0213 Explore P: 0.2785\n",
      "Episode: 435 Step: 130800 Total reward: 5.0 Training loss: 0.6209 Explore P: 0.2777\n",
      "Episode: 436 Step: 131100 Total reward: 6.0 Training loss: 0.0092 Explore P: 0.2769\n",
      "Episode: 437 Step: 131400 Total reward: 3.0 Training loss: 0.0184 Explore P: 0.2761\n",
      "Episode: 438 Step: 131700 Total reward: 8.0 Training loss: 0.0085 Explore P: 0.2753\n",
      "Episode: 439 Step: 132000 Total reward: 6.0 Training loss: 0.0160 Explore P: 0.2745\n",
      "Episode: 440 Step: 132300 Total reward: 1.0 Training loss: 0.0207 Explore P: 0.2737\n",
      "Episode: 441 Step: 132600 Total reward: 6.0 Training loss: 0.0180 Explore P: 0.2729\n",
      "Episode: 442 Step: 132900 Total reward: 2.0 Training loss: 0.0127 Explore P: 0.2721\n",
      "Episode: 443 Step: 133200 Total reward: 3.0 Training loss: 0.0240 Explore P: 0.2713\n",
      "Episode: 444 Step: 133500 Total reward: 5.0 Training loss: 0.0185 Explore P: 0.2705\n",
      "Episode: 445 Step: 133800 Total reward: 3.0 Training loss: 0.5708 Explore P: 0.2697\n",
      "Episode: 446 Step: 134100 Total reward: 6.0 Training loss: 0.0052 Explore P: 0.2690\n",
      "Episode: 447 Step: 134400 Total reward: 3.0 Training loss: 0.0434 Explore P: 0.2682\n",
      "Episode: 448 Step: 134700 Total reward: 4.0 Training loss: 0.0094 Explore P: 0.2674\n",
      "Episode: 449 Step: 135000 Total reward: 3.0 Training loss: 0.0089 Explore P: 0.2666\n",
      "Episode: 450 Step: 135300 Total reward: 8.0 Training loss: 0.0119 Explore P: 0.2659\n",
      "Episode: 451 Step: 135600 Total reward: 6.0 Training loss: 0.0288 Explore P: 0.2651\n",
      "Episode: 452 Step: 135900 Total reward: 7.0 Training loss: 0.0116 Explore P: 0.2643\n",
      "Episode: 453 Step: 136200 Total reward: 7.0 Training loss: 0.0109 Explore P: 0.2636\n",
      "Episode: 454 Step: 136500 Total reward: 11.0 Training loss: 0.0057 Explore P: 0.2628\n",
      "Episode: 455 Step: 136800 Total reward: 5.0 Training loss: 0.0072 Explore P: 0.2621\n",
      "Episode: 456 Step: 137100 Total reward: 6.0 Training loss: 0.0153 Explore P: 0.2613\n",
      "Episode: 457 Step: 137400 Total reward: 3.0 Training loss: 0.0149 Explore P: 0.2606\n",
      "Episode: 458 Step: 137700 Total reward: 12.0 Training loss: 0.0170 Explore P: 0.2598\n",
      "Episode: 459 Step: 138000 Total reward: 10.0 Training loss: 0.0111 Explore P: 0.2591\n",
      "Episode: 460 Step: 138300 Total reward: 9.0 Training loss: 0.0087 Explore P: 0.2583\n",
      "Episode: 461 Step: 138600 Total reward: 4.0 Training loss: 0.5014 Explore P: 0.2576\n",
      "Episode: 462 Step: 138900 Total reward: 3.0 Training loss: 0.5533 Explore P: 0.2568\n",
      "Episode: 463 Step: 139200 Total reward: 5.0 Training loss: 0.0118 Explore P: 0.2561\n",
      "Episode: 464 Step: 139500 Total reward: 13.0 Training loss: 0.0120 Explore P: 0.2554\n",
      "Episode: 465 Step: 139800 Total reward: 3.0 Training loss: 0.0272 Explore P: 0.2546\n",
      "Episode: 466 Step: 140100 Total reward: 4.0 Training loss: 0.0143 Explore P: 0.2539\n",
      "Episode: 467 Step: 140400 Total reward: 4.0 Training loss: 0.0178 Explore P: 0.2532\n",
      "Episode: 468 Step: 140700 Total reward: 11.0 Training loss: 0.0194 Explore P: 0.2524\n",
      "Episode: 469 Step: 141000 Total reward: 8.0 Training loss: 0.5690 Explore P: 0.2517\n",
      "Episode: 470 Step: 141300 Total reward: 9.0 Training loss: 0.0322 Explore P: 0.2510\n",
      "Episode: 471 Step: 141600 Total reward: 7.0 Training loss: 0.6949 Explore P: 0.2503\n",
      "Episode: 472 Step: 141900 Total reward: 11.0 Training loss: 0.7045 Explore P: 0.2495\n",
      "Episode: 473 Step: 142200 Total reward: 4.0 Training loss: 0.0425 Explore P: 0.2488\n",
      "Episode: 474 Step: 142500 Total reward: 8.0 Training loss: 0.0131 Explore P: 0.2481\n",
      "Episode: 475 Step: 142800 Total reward: 6.0 Training loss: 0.6713 Explore P: 0.2474\n",
      "Episode: 476 Step: 143100 Total reward: 4.0 Training loss: 0.0201 Explore P: 0.2467\n",
      "Episode: 477 Step: 143400 Total reward: 2.0 Training loss: 0.0382 Explore P: 0.2460\n",
      "Episode: 478 Step: 143700 Total reward: 9.0 Training loss: 0.0212 Explore P: 0.2453\n",
      "Episode: 479 Step: 144000 Total reward: 7.0 Training loss: 0.0096 Explore P: 0.2446\n",
      "Episode: 480 Step: 144300 Total reward: 7.0 Training loss: 0.7635 Explore P: 0.2439\n",
      "Episode: 481 Step: 144600 Total reward: 3.0 Training loss: 0.0180 Explore P: 0.2432\n",
      "Episode: 482 Step: 144900 Total reward: 9.0 Training loss: 0.0293 Explore P: 0.2425\n",
      "Episode: 483 Step: 145200 Total reward: 3.0 Training loss: 0.0159 Explore P: 0.2418\n",
      "Episode: 484 Step: 145500 Total reward: 1.0 Training loss: 0.0071 Explore P: 0.2411\n",
      "Episode: 485 Step: 145800 Total reward: 11.0 Training loss: 0.7318 Explore P: 0.2404\n",
      "Episode: 486 Step: 146100 Total reward: 2.0 Training loss: 0.0103 Explore P: 0.2397\n",
      "Episode: 487 Step: 146400 Total reward: 10.0 Training loss: 0.0135 Explore P: 0.2390\n",
      "Episode: 488 Step: 146700 Total reward: 7.0 Training loss: 0.0239 Explore P: 0.2383\n",
      "Episode: 489 Step: 147000 Total reward: 5.0 Training loss: 0.0092 Explore P: 0.2376\n",
      "Episode: 490 Step: 147300 Total reward: 7.0 Training loss: 0.0431 Explore P: 0.2369\n",
      "Episode: 491 Step: 147600 Total reward: 6.0 Training loss: 0.0107 Explore P: 0.2363\n",
      "Episode: 492 Step: 147900 Total reward: 12.0 Training loss: 0.0073 Explore P: 0.2356\n",
      "Episode: 493 Step: 148200 Total reward: 5.0 Training loss: 0.0073 Explore P: 0.2349\n",
      "Episode: 494 Step: 148500 Total reward: 11.0 Training loss: 0.6181 Explore P: 0.2342\n",
      "Episode: 495 Step: 148800 Total reward: 13.0 Training loss: 0.0094 Explore P: 0.2336\n",
      "Episode: 496 Step: 149100 Total reward: 8.0 Training loss: 0.0208 Explore P: 0.2329\n",
      "Episode: 497 Step: 149400 Total reward: 12.0 Training loss: 0.0188 Explore P: 0.2322\n",
      "Episode: 498 Step: 149700 Total reward: 4.0 Training loss: 0.0085 Explore P: 0.2316\n",
      "Episode: 499 Step: 150000 Total reward: 3.0 Training loss: 0.0080 Explore P: 0.2309\n",
      "Episode: 500 Step: 150300 Total reward: 7.0 Training loss: 0.0279 Explore P: 0.2302\n",
      "[[6.572728  6.5935187 6.5867763 6.7101045]\n",
      " [6.8601084 6.7762237 6.932552  6.7206674]\n",
      " [6.6147966 6.63063   6.696398  6.6296687]\n",
      " [6.5498953 6.633637  6.70398   6.610847 ]\n",
      " [6.487043  6.5065656 6.6147943 6.499988 ]\n",
      " [7.361766  7.068229  7.193562  6.8240147]\n",
      " [7.348734  7.029074  7.0056586 7.020786 ]\n",
      " [6.5203824 6.5702505 6.6076617 6.526173 ]\n",
      " [6.9354606 6.908756  7.043834  6.847669 ]\n",
      " [6.686561  6.6655645 6.6701427 6.646122 ]\n",
      " [6.716112  6.6562953 6.750818  6.5810914]\n",
      " [6.829386  6.6769195 6.888937  6.7877936]\n",
      " [6.646469  6.5727177 6.641769  6.6218443]\n",
      " [7.089554  6.900692  6.7705154 6.8429794]\n",
      " [6.5716915 6.5063825 6.687532  6.4948797]\n",
      " [6.626417  6.5575147 6.61048   6.584487 ]\n",
      " [6.6826935 6.643748  6.7685204 6.532528 ]\n",
      " [6.57028   6.5557256 6.571138  6.6361175]\n",
      " [6.6355376 6.603528  6.638371  6.6537566]\n",
      " [6.756112  6.622556  6.61591   6.702149 ]\n",
      " [7.0560565 6.9318857 6.8562326 6.699363 ]\n",
      " [6.5462456 6.556975  6.6795583 6.489267 ]\n",
      " [6.7400737 6.744638  6.7015266 6.906624 ]\n",
      " [6.8563743 6.772506  6.960949  6.7130103]\n",
      " [6.1433496 6.5398154 6.394525  6.228225 ]\n",
      " [7.123258  6.980516  6.789628  6.8387175]\n",
      " [6.816625  6.6661077 6.976553  6.598101 ]\n",
      " [6.627142  6.6522965 6.674211  6.5845323]\n",
      " [6.679626  6.553968  6.611541  6.6533704]\n",
      " [7.200199  6.886395  6.8592763 6.90892  ]\n",
      " [6.7648478 6.710647  6.973214  6.576022 ]\n",
      " [6.7871084 6.637258  6.7115564 6.7488174]\n",
      " [6.6834774 6.742883  6.7480454 6.6787143]\n",
      " [6.9372396 6.913905  6.8285484 6.7866845]\n",
      " [6.521842  6.602306  6.5449066 6.5429664]\n",
      " [6.885826  6.8228383 6.739556  6.6599536]\n",
      " [6.494295  6.5665383 6.5899653 6.5497847]\n",
      " [6.5824738 6.545548  6.662043  6.606263 ]\n",
      " [6.6971345 6.620443  6.647302  6.606549 ]\n",
      " [6.763757  6.648173  6.7473893 6.584679 ]\n",
      " [6.664253  6.4950323 6.7037663 6.569403 ]\n",
      " [6.4701996 6.4652057 6.566706  6.5110364]\n",
      " [6.8806376 6.7724934 6.7213283 6.7268887]\n",
      " [7.5119205 7.127423  7.024952  7.07716  ]\n",
      " [7.657154  7.247906  7.4844365 7.009574 ]\n",
      " [6.7912445 6.688459  6.732387  6.7098327]\n",
      " [7.25262   6.9729834 7.0198298 6.7237096]\n",
      " [6.7097316 6.705445  6.652146  6.782595 ]\n",
      " [6.53543   6.53533   6.6195273 6.5401096]\n",
      " [6.6494646 6.6082273 6.649694  6.6550584]\n",
      " [6.6071854 6.645014  6.4766173 6.5308065]\n",
      " [6.9111032 6.766559  6.7448826 6.65266  ]\n",
      " [7.0607023 6.835893  6.8024592 6.83498  ]\n",
      " [6.5709887 6.519448  6.58526   6.514209 ]\n",
      " [6.6482797 6.660609  6.6555324 6.561175 ]\n",
      " [6.78909   6.7454076 6.737943  6.6358366]\n",
      " [6.8175797 6.734465  6.651558  6.641953 ]\n",
      " [6.7434626 6.6875544 6.7855916 6.62244  ]\n",
      " [6.742189  6.7168717 6.846976  6.6184926]\n",
      " [6.53138   6.5973296 6.5757155 6.6210895]\n",
      " [6.9893003 6.832706  6.985156  6.7295413]\n",
      " [7.6764617 7.251859  7.250708  7.244097 ]\n",
      " [6.6292534 6.619401  6.6227255 6.677495 ]\n",
      " [6.545261  6.564834  6.555817  6.6440415]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 501 Step: 150600 Total reward: 13.0 Training loss: 0.0106 Explore P: 0.2296\n",
      "Episode: 502 Step: 150900 Total reward: 7.0 Training loss: 0.0272 Explore P: 0.2289\n",
      "Episode: 503 Step: 151200 Total reward: 13.0 Training loss: 0.0071 Explore P: 0.2283\n",
      "Episode: 504 Step: 151500 Total reward: 7.0 Training loss: 0.0380 Explore P: 0.2276\n",
      "Episode: 505 Step: 151800 Total reward: 6.0 Training loss: 0.0149 Explore P: 0.2270\n",
      "Episode: 506 Step: 152100 Total reward: 8.0 Training loss: 0.0092 Explore P: 0.2263\n",
      "Episode: 507 Step: 152400 Total reward: 8.0 Training loss: 0.0483 Explore P: 0.2257\n",
      "Episode: 508 Step: 152700 Total reward: 4.0 Training loss: 0.0206 Explore P: 0.2250\n",
      "Episode: 509 Step: 153000 Total reward: 7.0 Training loss: 0.0266 Explore P: 0.2244\n",
      "Episode: 510 Step: 153300 Total reward: 5.0 Training loss: 1.2601 Explore P: 0.2237\n",
      "Episode: 511 Step: 153600 Total reward: 1.0 Training loss: 0.0191 Explore P: 0.2231\n",
      "Episode: 512 Step: 153900 Total reward: 5.0 Training loss: 0.5553 Explore P: 0.2224\n",
      "Episode: 513 Step: 154200 Total reward: 7.0 Training loss: 0.7388 Explore P: 0.2218\n",
      "Episode: 514 Step: 154500 Total reward: 6.0 Training loss: 0.0281 Explore P: 0.2212\n",
      "Episode: 515 Step: 154800 Total reward: 13.0 Training loss: 1.1786 Explore P: 0.2205\n",
      "Episode: 516 Step: 155100 Total reward: 5.0 Training loss: 0.5182 Explore P: 0.2199\n",
      "Episode: 517 Step: 155400 Total reward: 6.0 Training loss: 0.0164 Explore P: 0.2193\n",
      "Episode: 518 Step: 155700 Total reward: 2.0 Training loss: 0.0102 Explore P: 0.2187\n",
      "Episode: 519 Step: 156000 Total reward: 4.0 Training loss: 0.0158 Explore P: 0.2180\n",
      "Episode: 520 Step: 156300 Total reward: 10.0 Training loss: 0.5300 Explore P: 0.2174\n",
      "Episode: 521 Step: 156600 Total reward: 7.0 Training loss: 0.0152 Explore P: 0.2168\n",
      "Episode: 522 Step: 156900 Total reward: 6.0 Training loss: 0.0229 Explore P: 0.2162\n",
      "Episode: 523 Step: 157200 Total reward: 7.0 Training loss: 0.0326 Explore P: 0.2156\n",
      "Episode: 524 Step: 157500 Total reward: 4.0 Training loss: 0.0182 Explore P: 0.2149\n",
      "Episode: 525 Step: 157800 Total reward: 7.0 Training loss: 0.0121 Explore P: 0.2143\n",
      "Episode: 526 Step: 158100 Total reward: 13.0 Training loss: 0.0138 Explore P: 0.2137\n",
      "Episode: 527 Step: 158400 Total reward: 6.0 Training loss: 0.0408 Explore P: 0.2131\n",
      "Episode: 528 Step: 158700 Total reward: 3.0 Training loss: 0.5812 Explore P: 0.2125\n",
      "Episode: 529 Step: 159000 Total reward: 6.0 Training loss: 0.0240 Explore P: 0.2119\n",
      "Episode: 530 Step: 159300 Total reward: 9.0 Training loss: 0.0120 Explore P: 0.2113\n",
      "Episode: 531 Step: 159600 Total reward: 9.0 Training loss: 0.6976 Explore P: 0.2107\n",
      "Episode: 532 Step: 159900 Total reward: 8.0 Training loss: 0.0132 Explore P: 0.2101\n",
      "Episode: 533 Step: 160200 Total reward: 6.0 Training loss: 0.0061 Explore P: 0.2095\n",
      "Episode: 534 Step: 160500 Total reward: 10.0 Training loss: 0.0295 Explore P: 0.2089\n",
      "Episode: 535 Step: 160800 Total reward: 8.0 Training loss: 0.7124 Explore P: 0.2083\n",
      "Episode: 536 Step: 161100 Total reward: 7.0 Training loss: 0.0198 Explore P: 0.2077\n",
      "Episode: 537 Step: 161400 Total reward: 4.0 Training loss: 0.0169 Explore P: 0.2071\n",
      "Episode: 538 Step: 161700 Total reward: 0.0 Training loss: 0.6028 Explore P: 0.2065\n",
      "Episode: 539 Step: 162000 Total reward: 6.0 Training loss: 0.0147 Explore P: 0.2059\n",
      "Episode: 540 Step: 162300 Total reward: 5.0 Training loss: 0.0085 Explore P: 0.2053\n",
      "Episode: 541 Step: 162600 Total reward: 4.0 Training loss: 0.0124 Explore P: 0.2047\n",
      "Episode: 542 Step: 162900 Total reward: 5.0 Training loss: 0.0102 Explore P: 0.2042\n",
      "Episode: 543 Step: 163200 Total reward: 5.0 Training loss: 0.0160 Explore P: 0.2036\n",
      "Episode: 544 Step: 163500 Total reward: 2.0 Training loss: 0.0464 Explore P: 0.2030\n",
      "Episode: 545 Step: 163800 Total reward: 10.0 Training loss: 0.6347 Explore P: 0.2024\n",
      "Episode: 546 Step: 164100 Total reward: 4.0 Training loss: 0.6184 Explore P: 0.2018\n",
      "Episode: 547 Step: 164400 Total reward: 7.0 Training loss: 0.6446 Explore P: 0.2013\n",
      "Episode: 548 Step: 164700 Total reward: 6.0 Training loss: 0.0187 Explore P: 0.2007\n",
      "Episode: 549 Step: 165000 Total reward: 8.0 Training loss: 0.0086 Explore P: 0.2001\n",
      "Episode: 550 Step: 165300 Total reward: 12.0 Training loss: 0.0081 Explore P: 0.1996\n",
      "Episode: 551 Step: 165600 Total reward: 6.0 Training loss: 0.6848 Explore P: 0.1990\n",
      "Episode: 552 Step: 165900 Total reward: 13.0 Training loss: 0.0119 Explore P: 0.1984\n",
      "Episode: 553 Step: 166200 Total reward: 8.0 Training loss: 0.0275 Explore P: 0.1979\n",
      "Episode: 554 Step: 166500 Total reward: 10.0 Training loss: 0.0164 Explore P: 0.1973\n",
      "Episode: 555 Step: 166800 Total reward: 5.0 Training loss: 0.0366 Explore P: 0.1967\n",
      "Episode: 556 Step: 167100 Total reward: 6.0 Training loss: 0.0198 Explore P: 0.1962\n",
      "Episode: 557 Step: 167400 Total reward: 8.0 Training loss: 0.0313 Explore P: 0.1956\n",
      "Episode: 558 Step: 167700 Total reward: 10.0 Training loss: 0.0112 Explore P: 0.1951\n",
      "Episode: 559 Step: 168000 Total reward: 8.0 Training loss: 0.0131 Explore P: 0.1945\n",
      "Episode: 560 Step: 168300 Total reward: 4.0 Training loss: 0.0103 Explore P: 0.1940\n",
      "Episode: 561 Step: 168600 Total reward: 8.0 Training loss: 0.0115 Explore P: 0.1934\n",
      "Episode: 562 Step: 168900 Total reward: 4.0 Training loss: 0.0131 Explore P: 0.1929\n",
      "Episode: 563 Step: 169200 Total reward: 8.0 Training loss: 0.5842 Explore P: 0.1923\n",
      "Episode: 564 Step: 169500 Total reward: 15.0 Training loss: 0.6610 Explore P: 0.1918\n",
      "Episode: 565 Step: 169800 Total reward: 7.0 Training loss: 0.0154 Explore P: 0.1912\n",
      "Episode: 566 Step: 170100 Total reward: 7.0 Training loss: 0.6709 Explore P: 0.1907\n",
      "Episode: 567 Step: 170400 Total reward: 2.0 Training loss: 0.6515 Explore P: 0.1901\n",
      "Episode: 568 Step: 170700 Total reward: 15.0 Training loss: 0.9315 Explore P: 0.1896\n",
      "Episode: 569 Step: 171000 Total reward: 8.0 Training loss: 0.0224 Explore P: 0.1891\n",
      "Episode: 570 Step: 171300 Total reward: 10.0 Training loss: 0.0167 Explore P: 0.1885\n",
      "Episode: 571 Step: 171600 Total reward: 3.0 Training loss: 0.0082 Explore P: 0.1880\n",
      "Episode: 572 Step: 171900 Total reward: 9.0 Training loss: 0.0200 Explore P: 0.1875\n",
      "Episode: 573 Step: 172200 Total reward: 7.0 Training loss: 0.7831 Explore P: 0.1869\n",
      "Episode: 574 Step: 172500 Total reward: 5.0 Training loss: 0.7244 Explore P: 0.1864\n",
      "Episode: 575 Step: 172800 Total reward: 12.0 Training loss: 0.6164 Explore P: 0.1859\n",
      "Episode: 576 Step: 173100 Total reward: 10.0 Training loss: 0.0152 Explore P: 0.1853\n",
      "Episode: 577 Step: 173400 Total reward: 5.0 Training loss: 0.0100 Explore P: 0.1848\n",
      "Episode: 578 Step: 173700 Total reward: 16.0 Training loss: 0.0197 Explore P: 0.1843\n",
      "Episode: 579 Step: 174000 Total reward: 13.0 Training loss: 0.0175 Explore P: 0.1838\n",
      "Episode: 580 Step: 174300 Total reward: 12.0 Training loss: 0.0149 Explore P: 0.1832\n",
      "Episode: 581 Step: 174600 Total reward: 7.0 Training loss: 0.0108 Explore P: 0.1827\n",
      "Episode: 582 Step: 174900 Total reward: 8.0 Training loss: 0.0110 Explore P: 0.1822\n",
      "Episode: 583 Step: 175200 Total reward: 3.0 Training loss: 0.0093 Explore P: 0.1817\n",
      "Episode: 584 Step: 175500 Total reward: 10.0 Training loss: 0.0207 Explore P: 0.1812\n",
      "Episode: 585 Step: 175800 Total reward: 7.0 Training loss: 0.0300 Explore P: 0.1807\n",
      "Episode: 586 Step: 176100 Total reward: 4.0 Training loss: 0.0105 Explore P: 0.1802\n",
      "Episode: 587 Step: 176400 Total reward: 8.0 Training loss: 0.0123 Explore P: 0.1796\n",
      "Episode: 588 Step: 176700 Total reward: 3.0 Training loss: 0.0116 Explore P: 0.1791\n",
      "Episode: 589 Step: 177000 Total reward: 7.0 Training loss: 0.0077 Explore P: 0.1786\n",
      "Episode: 590 Step: 177300 Total reward: 6.0 Training loss: 0.0087 Explore P: 0.1781\n",
      "Episode: 591 Step: 177600 Total reward: 4.0 Training loss: 0.0483 Explore P: 0.1776\n",
      "Episode: 592 Step: 177900 Total reward: 6.0 Training loss: 0.0117 Explore P: 0.1771\n",
      "Episode: 593 Step: 178200 Total reward: 3.0 Training loss: 0.0067 Explore P: 0.1766\n",
      "Episode: 594 Step: 178500 Total reward: 13.0 Training loss: 0.0223 Explore P: 0.1761\n",
      "Episode: 595 Step: 178800 Total reward: 7.0 Training loss: 0.0177 Explore P: 0.1756\n",
      "Episode: 596 Step: 179100 Total reward: 5.0 Training loss: 0.0109 Explore P: 0.1751\n",
      "Episode: 597 Step: 179400 Total reward: 7.0 Training loss: 0.0136 Explore P: 0.1746\n",
      "Episode: 598 Step: 179700 Total reward: 10.0 Training loss: 0.0084 Explore P: 0.1741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 599 Step: 180000 Total reward: 8.0 Training loss: 0.0106 Explore P: 0.1736\n",
      "Episode: 600 Step: 180300 Total reward: 11.0 Training loss: 0.0314 Explore P: 0.1732\n",
      "[[6.8907027 6.689128  6.7615776 6.811649 ]\n",
      " [6.789767  6.4702835 6.6327047 6.820863 ]\n",
      " [7.555445  7.0636396 7.0939302 7.2217197]\n",
      " [6.945875  6.768117  6.8879795 6.757513 ]\n",
      " [6.558544  6.4925356 6.553587  6.4386463]\n",
      " [6.9100604 6.7527657 6.691964  6.8273664]\n",
      " [6.893442  6.6835794 6.742337  6.5538473]\n",
      " [7.245818  6.899577  6.9208975 6.8831763]\n",
      " [6.6127963 6.384124  6.5778027 6.606642 ]\n",
      " [6.621251  6.4734106 6.613615  6.6464643]\n",
      " [6.67159   6.5605288 6.669571  6.589736 ]\n",
      " [7.381959  6.9919844 6.8782988 7.0358915]\n",
      " [7.3520103 7.048824  6.836019  6.8938427]\n",
      " [6.586057  6.4531674 6.6112213 6.523432 ]\n",
      " [5.501063  6.2254124 6.111953  6.0229387]\n",
      " [7.129411  6.784399  6.7839537 6.7329874]\n",
      " [6.735307  6.6599026 6.633748  6.6885643]\n",
      " [6.6895094 6.631706  6.691147  6.642028 ]\n",
      " [6.7737584 6.5607357 6.646698  6.6695957]\n",
      " [6.73528   6.6520004 6.7671156 6.656037 ]\n",
      " [6.840086  6.6998234 6.5818696 6.8976088]\n",
      " [6.9212947 6.684296  6.779931  6.6522107]\n",
      " [6.5677633 6.3755155 6.5261574 6.611475 ]\n",
      " [6.8535185 6.757981  6.8437176 6.553599 ]\n",
      " [6.4993086 6.3948116 6.6161165 6.6040325]\n",
      " [6.839914  6.653699  6.8061395 6.7878075]\n",
      " [6.7250504 6.5431223 6.8579135 6.776056 ]\n",
      " [6.6138587 6.566081  6.6395106 6.5763874]\n",
      " [6.664847  6.585786  6.684312  6.6838455]\n",
      " [6.6960974 6.4472933 6.7417293 6.500831 ]\n",
      " [7.128639  6.7586565 6.9798512 6.7237387]\n",
      " [6.879506  6.610737  6.769042  6.617916 ]\n",
      " [7.362216  6.889409  6.873107  7.0784707]\n",
      " [6.6544228 6.492793  6.5150166 6.6219916]\n",
      " [6.681719  6.448839  6.6249156 6.726654 ]\n",
      " [6.9688287 6.7309494 7.0085716 6.8658476]\n",
      " [6.808306  6.5811095 6.809801  6.670926 ]\n",
      " [7.2214794 6.845686  6.9563026 6.8643217]\n",
      " [7.0059443 6.6938987 6.8146    6.793618 ]\n",
      " [6.728467  6.656434  6.573553  6.6527557]\n",
      " [6.524658  6.3553066 6.560896  6.50515  ]\n",
      " [6.766245  6.663648  6.77432   6.630133 ]\n",
      " [7.215293  6.715371  6.8434763 6.9448104]\n",
      " [7.3492174 7.04172   6.781429  6.8865905]\n",
      " [6.74658   6.548932  6.546566  6.8206453]\n",
      " [6.855223  6.5674524 6.811598  6.748937 ]\n",
      " [6.74481   6.564884  6.622848  6.6699686]\n",
      " [6.881803  6.647126  6.8892713 6.8687077]\n",
      " [7.1744165 6.8188925 6.8191767 6.698541 ]\n",
      " [6.6765704 6.513931  6.639266  6.6920633]\n",
      " [6.780898  6.575179  6.7503395 6.6327963]\n",
      " [6.729164  6.5327344 6.732462  6.8819213]\n",
      " [6.959428  6.7019634 6.8376803 6.8935614]\n",
      " [6.953159  6.771774  6.9782805 6.6669445]\n",
      " [6.6611314 6.582569  6.606428  6.579419 ]\n",
      " [6.565357  6.474084  6.4347415 6.5561843]\n",
      " [6.860424  6.664488  6.8311357 6.682327 ]\n",
      " [6.863044  6.660034  6.957119  6.6862173]\n",
      " [6.534573  6.4561067 6.6334906 6.544799 ]\n",
      " [6.6254425 6.5288873 6.651629  6.58168  ]\n",
      " [6.577195  6.5612054 6.6106086 6.612667 ]\n",
      " [6.677336  6.526584  6.7565002 6.4811144]\n",
      " [6.957548  6.7746334 6.870443  6.911669 ]\n",
      " [6.4947214 6.3689814 6.552477  6.4649105]]\n",
      "Episode: 601 Step: 180600 Total reward: 5.0 Training loss: 0.0248 Explore P: 0.1727\n",
      "Episode: 602 Step: 180900 Total reward: 6.0 Training loss: 0.0120 Explore P: 0.1722\n",
      "Episode: 603 Step: 181200 Total reward: 8.0 Training loss: 0.0078 Explore P: 0.1717\n",
      "Episode: 604 Step: 181500 Total reward: 6.0 Training loss: 0.0078 Explore P: 0.1712\n",
      "Episode: 605 Step: 181800 Total reward: 5.0 Training loss: 0.0095 Explore P: 0.1707\n",
      "Episode: 606 Step: 182100 Total reward: 8.0 Training loss: 0.0295 Explore P: 0.1702\n",
      "Episode: 607 Step: 182400 Total reward: 7.0 Training loss: 0.6382 Explore P: 0.1698\n",
      "Episode: 608 Step: 182700 Total reward: 5.0 Training loss: 0.6199 Explore P: 0.1693\n",
      "Episode: 609 Step: 183000 Total reward: 7.0 Training loss: 0.0101 Explore P: 0.1688\n",
      "Episode: 610 Step: 183300 Total reward: 6.0 Training loss: 0.6296 Explore P: 0.1683\n",
      "Episode: 611 Step: 183600 Total reward: 6.0 Training loss: 0.0270 Explore P: 0.1679\n",
      "Episode: 612 Step: 183900 Total reward: 13.0 Training loss: 0.0097 Explore P: 0.1674\n",
      "Episode: 613 Step: 184200 Total reward: 3.0 Training loss: 0.6481 Explore P: 0.1669\n",
      "Episode: 614 Step: 184500 Total reward: 7.0 Training loss: 0.0418 Explore P: 0.1664\n",
      "Episode: 615 Step: 184800 Total reward: 14.0 Training loss: 0.0248 Explore P: 0.1660\n",
      "Episode: 616 Step: 185100 Total reward: 6.0 Training loss: 0.0296 Explore P: 0.1655\n",
      "Episode: 617 Step: 185400 Total reward: 6.0 Training loss: 0.4953 Explore P: 0.1650\n",
      "Episode: 618 Step: 185700 Total reward: 10.0 Training loss: 0.0097 Explore P: 0.1646\n",
      "Episode: 619 Step: 186000 Total reward: 12.0 Training loss: 0.0259 Explore P: 0.1641\n",
      "Episode: 620 Step: 186300 Total reward: 11.0 Training loss: 0.0179 Explore P: 0.1637\n",
      "Episode: 621 Step: 186600 Total reward: 6.0 Training loss: 0.0171 Explore P: 0.1632\n",
      "Episode: 622 Step: 186900 Total reward: 9.0 Training loss: 0.0124 Explore P: 0.1627\n",
      "Episode: 623 Step: 187200 Total reward: 8.0 Training loss: 0.0110 Explore P: 0.1623\n",
      "Episode: 624 Step: 187500 Total reward: 5.0 Training loss: 0.0162 Explore P: 0.1618\n",
      "Episode: 625 Step: 187800 Total reward: 9.0 Training loss: 0.0258 Explore P: 0.1614\n",
      "Episode: 626 Step: 188100 Total reward: 14.0 Training loss: 0.6979 Explore P: 0.1609\n",
      "Episode: 627 Step: 188400 Total reward: 8.0 Training loss: 0.0127 Explore P: 0.1605\n",
      "Episode: 628 Step: 188700 Total reward: 10.0 Training loss: 0.0090 Explore P: 0.1600\n",
      "Episode: 629 Step: 189000 Total reward: 11.0 Training loss: 0.0084 Explore P: 0.1596\n",
      "Episode: 630 Step: 189300 Total reward: 6.0 Training loss: 0.0179 Explore P: 0.1591\n",
      "Episode: 631 Step: 189600 Total reward: 11.0 Training loss: 0.0183 Explore P: 0.1587\n",
      "Episode: 632 Step: 189900 Total reward: 2.0 Training loss: 1.2374 Explore P: 0.1582\n",
      "Episode: 633 Step: 190200 Total reward: 9.0 Training loss: 0.0110 Explore P: 0.1578\n",
      "Episode: 634 Step: 190500 Total reward: 8.0 Training loss: 0.0237 Explore P: 0.1573\n",
      "Episode: 635 Step: 190800 Total reward: 4.0 Training loss: 0.0127 Explore P: 0.1569\n",
      "Episode: 636 Step: 191100 Total reward: 6.0 Training loss: 0.0214 Explore P: 0.1565\n",
      "Episode: 637 Step: 191400 Total reward: 3.0 Training loss: 0.0081 Explore P: 0.1560\n",
      "Episode: 638 Step: 191700 Total reward: 5.0 Training loss: 0.0166 Explore P: 0.1556\n",
      "Episode: 639 Step: 192000 Total reward: 7.0 Training loss: 0.0125 Explore P: 0.1551\n",
      "Episode: 640 Step: 192300 Total reward: 5.0 Training loss: 0.0072 Explore P: 0.1547\n",
      "Episode: 641 Step: 192600 Total reward: 9.0 Training loss: 0.0091 Explore P: 0.1543\n",
      "Episode: 642 Step: 192900 Total reward: 2.0 Training loss: 0.0079 Explore P: 0.1538\n",
      "Episode: 643 Step: 193200 Total reward: 9.0 Training loss: 0.6744 Explore P: 0.1534\n",
      "Episode: 644 Step: 193500 Total reward: 5.0 Training loss: 0.0100 Explore P: 0.1530\n",
      "Episode: 645 Step: 193800 Total reward: 11.0 Training loss: 0.6398 Explore P: 0.1526\n",
      "Episode: 646 Step: 194100 Total reward: 10.0 Training loss: 0.0220 Explore P: 0.1521\n",
      "Episode: 647 Step: 194400 Total reward: 9.0 Training loss: 0.0196 Explore P: 0.1517\n",
      "Episode: 648 Step: 194700 Total reward: 1.0 Training loss: 0.0220 Explore P: 0.1513\n",
      "Episode: 649 Step: 195000 Total reward: 13.0 Training loss: 0.0253 Explore P: 0.1509\n",
      "Episode: 650 Step: 195300 Total reward: 7.0 Training loss: 0.6134 Explore P: 0.1504\n",
      "Episode: 651 Step: 195600 Total reward: 16.0 Training loss: 0.0203 Explore P: 0.1500\n",
      "Episode: 652 Step: 195900 Total reward: 4.0 Training loss: 0.0202 Explore P: 0.1496\n",
      "Episode: 653 Step: 196200 Total reward: 0.0 Training loss: 0.0210 Explore P: 0.1492\n",
      "Episode: 654 Step: 196500 Total reward: 9.0 Training loss: 0.0172 Explore P: 0.1488\n",
      "Episode: 655 Step: 196800 Total reward: 9.0 Training loss: 0.0170 Explore P: 0.1483\n",
      "Episode: 656 Step: 197100 Total reward: 10.0 Training loss: 0.0505 Explore P: 0.1479\n",
      "Episode: 657 Step: 197400 Total reward: 8.0 Training loss: 0.0165 Explore P: 0.1475\n",
      "Episode: 658 Step: 197700 Total reward: 6.0 Training loss: 0.0120 Explore P: 0.1471\n",
      "Episode: 659 Step: 198000 Total reward: 8.0 Training loss: 0.0081 Explore P: 0.1467\n",
      "Episode: 660 Step: 198300 Total reward: 12.0 Training loss: 0.0127 Explore P: 0.1463\n",
      "Episode: 661 Step: 198600 Total reward: 6.0 Training loss: 0.0129 Explore P: 0.1459\n",
      "Episode: 662 Step: 198900 Total reward: 10.0 Training loss: 0.7776 Explore P: 0.1455\n",
      "Episode: 663 Step: 199200 Total reward: 8.0 Training loss: 0.0209 Explore P: 0.1451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 664 Step: 199500 Total reward: 4.0 Training loss: 0.0069 Explore P: 0.1447\n",
      "Episode: 665 Step: 199800 Total reward: 10.0 Training loss: 0.0218 Explore P: 0.1443\n",
      "Episode: 666 Step: 200100 Total reward: 5.0 Training loss: 0.0130 Explore P: 0.1438\n",
      "Episode: 667 Step: 200400 Total reward: 12.0 Training loss: 0.0062 Explore P: 0.1434\n",
      "Episode: 668 Step: 200700 Total reward: 4.0 Training loss: 0.0099 Explore P: 0.1430\n",
      "Episode: 669 Step: 201000 Total reward: 8.0 Training loss: 0.0272 Explore P: 0.1426\n",
      "Episode: 670 Step: 201300 Total reward: 8.0 Training loss: 0.0151 Explore P: 0.1423\n",
      "Episode: 671 Step: 201600 Total reward: 1.0 Training loss: 0.0208 Explore P: 0.1419\n",
      "Episode: 672 Step: 201900 Total reward: 8.0 Training loss: 0.0093 Explore P: 0.1415\n",
      "Episode: 673 Step: 202200 Total reward: 10.0 Training loss: 0.0279 Explore P: 0.1411\n",
      "Episode: 674 Step: 202500 Total reward: 8.0 Training loss: 0.6322 Explore P: 0.1407\n",
      "Episode: 675 Step: 202800 Total reward: 7.0 Training loss: 1.3080 Explore P: 0.1403\n",
      "Episode: 676 Step: 203100 Total reward: 12.0 Training loss: 0.0102 Explore P: 0.1399\n",
      "Episode: 677 Step: 203400 Total reward: 6.0 Training loss: 0.0082 Explore P: 0.1395\n",
      "Episode: 678 Step: 203700 Total reward: 3.0 Training loss: 0.0393 Explore P: 0.1391\n",
      "Episode: 679 Step: 204000 Total reward: 10.0 Training loss: 0.0080 Explore P: 0.1387\n",
      "Episode: 680 Step: 204300 Total reward: 10.0 Training loss: 0.0444 Explore P: 0.1383\n",
      "Episode: 681 Step: 204600 Total reward: 6.0 Training loss: 0.6499 Explore P: 0.1380\n",
      "Episode: 682 Step: 204900 Total reward: 11.0 Training loss: 0.0258 Explore P: 0.1376\n",
      "Episode: 683 Step: 205200 Total reward: 12.0 Training loss: 0.0170 Explore P: 0.1372\n",
      "Episode: 684 Step: 205500 Total reward: 5.0 Training loss: 0.0113 Explore P: 0.1368\n",
      "Episode: 685 Step: 205800 Total reward: 11.0 Training loss: 0.0063 Explore P: 0.1364\n",
      "Episode: 686 Step: 206100 Total reward: 7.0 Training loss: 0.0087 Explore P: 0.1361\n",
      "Episode: 687 Step: 206400 Total reward: 9.0 Training loss: 0.0175 Explore P: 0.1357\n",
      "Episode: 688 Step: 206700 Total reward: 5.0 Training loss: 0.0310 Explore P: 0.1353\n",
      "Episode: 689 Step: 207000 Total reward: 2.0 Training loss: 0.0264 Explore P: 0.1349\n",
      "Episode: 690 Step: 207300 Total reward: 8.0 Training loss: 0.0110 Explore P: 0.1345\n",
      "Episode: 691 Step: 207600 Total reward: 2.0 Training loss: 0.0224 Explore P: 0.1342\n",
      "Episode: 692 Step: 207900 Total reward: 8.0 Training loss: 0.0337 Explore P: 0.1338\n",
      "Episode: 693 Step: 208200 Total reward: 14.0 Training loss: 0.6289 Explore P: 0.1334\n",
      "Episode: 694 Step: 208500 Total reward: 14.0 Training loss: 0.6639 Explore P: 0.1331\n",
      "Episode: 695 Step: 208800 Total reward: 8.0 Training loss: 0.0189 Explore P: 0.1327\n",
      "Episode: 696 Step: 209100 Total reward: 9.0 Training loss: 0.0120 Explore P: 0.1323\n",
      "Episode: 697 Step: 209400 Total reward: 7.0 Training loss: 0.0075 Explore P: 0.1320\n",
      "Episode: 698 Step: 209700 Total reward: 5.0 Training loss: 0.6210 Explore P: 0.1316\n",
      "Episode: 699 Step: 210000 Total reward: 2.0 Training loss: 0.0176 Explore P: 0.1312\n",
      "Episode: 700 Step: 210300 Total reward: 9.0 Training loss: 0.0085 Explore P: 0.1309\n",
      "[[6.606611  6.5836754 6.506805  6.528765 ]\n",
      " [6.4761915 6.5734773 6.5494595 6.4267077]\n",
      " [6.5682874 6.5206137 6.6296    6.5144324]\n",
      " [6.524038  6.6167073 6.74683   6.4554353]\n",
      " [6.9220614 6.7535486 6.747416  6.715609 ]\n",
      " [6.4264255 6.5284595 6.4852214 6.4945955]\n",
      " [6.584406  6.517368  6.5050783 6.4565024]\n",
      " [6.835212  6.8420486 6.773181  6.89982  ]\n",
      " [6.665269  6.6397696 6.574401  6.6956496]\n",
      " [6.649907  6.6008315 6.678911  6.5483174]\n",
      " [6.517069  6.424282  6.5155764 6.490188 ]\n",
      " [6.518799  6.435668  6.5274644 6.502334 ]\n",
      " [6.4151473 6.3710775 6.408459  6.417221 ]\n",
      " [6.584091  6.514287  6.539038  6.598591 ]\n",
      " [6.7736607 6.720235  6.7209997 6.7254086]\n",
      " [6.8314133 6.7795258 6.705453  6.937712 ]\n",
      " [6.531973  6.405154  6.530219  6.504165 ]\n",
      " [6.3953633 6.373418  6.451543  6.3950996]\n",
      " [6.4643946 6.654237  6.650406  6.467425 ]\n",
      " [6.465844  6.6030517 6.594968  6.625892 ]\n",
      " [7.2100563 7.0576544 7.0169563 6.838659 ]\n",
      " [7.4655614 7.222009  7.3524294 6.9730906]\n",
      " [6.7178316 6.577362  6.5392065 6.54273  ]\n",
      " [6.7292266 6.6896734 6.532646  6.671146 ]\n",
      " [6.5530195 6.527602  6.4915075 6.6343403]\n",
      " [6.626204  6.6457815 6.6651273 6.6151123]\n",
      " [6.4643917 6.550041  6.4475355 6.567143 ]\n",
      " [6.4258027 6.376239  6.4715824 6.422781 ]\n",
      " [6.52979   6.4054823 6.455367  6.5091596]\n",
      " [6.6373377 6.651542  6.6502204 6.615954 ]\n",
      " [6.5054893 6.512607  6.619904  6.459654 ]\n",
      " [6.465662  6.55844   6.5070066 6.4102936]\n",
      " [6.453757  6.4839225 6.522397  6.4775424]\n",
      " [7.172548  6.9101267 6.8401003 6.906159 ]\n",
      " [6.6165876 6.5330725 6.6266894 6.5720572]\n",
      " [6.504054  6.517107  6.4577756 6.559108 ]\n",
      " [6.5447416 6.610897  6.6194715 6.507075 ]\n",
      " [6.725221  6.634693  6.597277  6.5712085]\n",
      " [6.773082  6.615266  6.5750895 6.6600046]\n",
      " [6.600984  6.6833105 6.6301656 6.5008173]\n",
      " [7.473695  7.1015773 6.972863  7.2546215]\n",
      " [6.621709  6.5599613 6.483433  6.5623403]\n",
      " [6.6591926 6.4690957 6.538566  6.6007133]\n",
      " [6.4085217 6.3640504 6.485342  6.4451356]\n",
      " [6.4954686 6.4486737 6.4953966 6.5312066]\n",
      " [6.579973  6.5004787 6.533823  6.604276 ]\n",
      " [6.523072  6.5286355 6.509683  6.489199 ]\n",
      " [6.8515797 6.7997365 6.708993  6.8513074]\n",
      " [6.6418705 6.6536922 6.828001  6.49029  ]\n",
      " [6.846235  6.6927147 6.5773973 6.574568 ]\n",
      " [6.5982842 6.659395  6.7410164 6.5663896]\n",
      " [6.5255275 6.5728354 6.4528217 6.6071906]\n",
      " [6.4557285 6.431074  6.436989  6.3962293]\n",
      " [6.598917  6.6554947 6.661175  6.59631  ]\n",
      " [6.3695803 6.422384  6.401249  6.4016905]\n",
      " [6.9005466 6.727295  6.66743   6.7661905]\n",
      " [6.8369317 6.7575006 6.6896296 6.7544856]\n",
      " [6.3788176 6.467145  6.473775  6.425047 ]\n",
      " [6.778269  6.607648  6.651347  6.6861606]\n",
      " [6.440892  6.566899  6.5351696 6.5600553]\n",
      " [6.431506  6.386492  6.439248  6.4266515]\n",
      " [6.5000353 6.4975786 6.4880753 6.6619153]\n",
      " [6.380363  6.4519377 6.483122  6.454367 ]\n",
      " [6.7752004 6.720646  6.7460146 6.669436 ]]\n",
      "Episode: 701 Step: 210600 Total reward: 10.0 Training loss: 0.5530 Explore P: 0.1305\n",
      "Episode: 702 Step: 210900 Total reward: 10.0 Training loss: 0.0087 Explore P: 0.1301\n",
      "Episode: 703 Step: 211200 Total reward: 3.0 Training loss: 0.0165 Explore P: 0.1298\n",
      "Episode: 704 Step: 211500 Total reward: 5.0 Training loss: 0.0093 Explore P: 0.1294\n",
      "Episode: 705 Step: 211800 Total reward: 8.0 Training loss: 0.0203 Explore P: 0.1291\n",
      "Episode: 706 Step: 212100 Total reward: 7.0 Training loss: 0.6244 Explore P: 0.1287\n",
      "Episode: 707 Step: 212400 Total reward: 4.0 Training loss: 0.5530 Explore P: 0.1284\n",
      "Episode: 708 Step: 212700 Total reward: 11.0 Training loss: 0.0186 Explore P: 0.1280\n",
      "Episode: 709 Step: 213000 Total reward: 9.0 Training loss: 0.0173 Explore P: 0.1276\n",
      "Episode: 710 Step: 213300 Total reward: 7.0 Training loss: 0.0082 Explore P: 0.1273\n",
      "Episode: 711 Step: 213600 Total reward: 14.0 Training loss: 0.0102 Explore P: 0.1269\n",
      "Episode: 712 Step: 213900 Total reward: 2.0 Training loss: 0.0117 Explore P: 0.1266\n",
      "Episode: 713 Step: 214200 Total reward: 5.0 Training loss: 0.6859 Explore P: 0.1262\n",
      "Episode: 714 Step: 214500 Total reward: 12.0 Training loss: 0.0298 Explore P: 0.1259\n",
      "Episode: 715 Step: 214800 Total reward: 5.0 Training loss: 0.0115 Explore P: 0.1256\n",
      "Episode: 716 Step: 215100 Total reward: 9.0 Training loss: 0.0206 Explore P: 0.1252\n",
      "Episode: 717 Step: 215400 Total reward: 4.0 Training loss: 0.0306 Explore P: 0.1249\n",
      "Episode: 718 Step: 215700 Total reward: 8.0 Training loss: 0.8030 Explore P: 0.1245\n",
      "Episode: 719 Step: 216000 Total reward: 11.0 Training loss: 0.0089 Explore P: 0.1242\n",
      "Episode: 720 Step: 216300 Total reward: 11.0 Training loss: 0.6931 Explore P: 0.1238\n",
      "Episode: 721 Step: 216600 Total reward: 13.0 Training loss: 0.0159 Explore P: 0.1235\n",
      "Episode: 722 Step: 216900 Total reward: 7.0 Training loss: 0.0231 Explore P: 0.1231\n",
      "Episode: 723 Step: 217200 Total reward: 10.0 Training loss: 0.0136 Explore P: 0.1228\n",
      "Episode: 724 Step: 217500 Total reward: 12.0 Training loss: 0.0107 Explore P: 0.1225\n",
      "Episode: 725 Step: 217800 Total reward: 6.0 Training loss: 0.0117 Explore P: 0.1221\n",
      "Episode: 726 Step: 218100 Total reward: 1.0 Training loss: 0.0350 Explore P: 0.1218\n",
      "Episode: 727 Step: 218400 Total reward: 11.0 Training loss: 0.0095 Explore P: 0.1215\n",
      "Episode: 728 Step: 218700 Total reward: 6.0 Training loss: 0.6496 Explore P: 0.1211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 729 Step: 219000 Total reward: 7.0 Training loss: 0.0199 Explore P: 0.1208\n",
      "Episode: 730 Step: 219300 Total reward: 12.0 Training loss: 0.0226 Explore P: 0.1205\n",
      "Episode: 731 Step: 219600 Total reward: 7.0 Training loss: 0.0174 Explore P: 0.1201\n",
      "Episode: 732 Step: 219900 Total reward: 11.0 Training loss: 0.0181 Explore P: 0.1198\n",
      "Episode: 733 Step: 220200 Total reward: 9.0 Training loss: 0.0091 Explore P: 0.1195\n",
      "Episode: 734 Step: 220500 Total reward: 8.0 Training loss: 0.0086 Explore P: 0.1191\n",
      "Episode: 735 Step: 220800 Total reward: 11.0 Training loss: 0.0094 Explore P: 0.1188\n",
      "Episode: 736 Step: 221100 Total reward: 10.0 Training loss: 0.0244 Explore P: 0.1185\n",
      "Episode: 737 Step: 221400 Total reward: 8.0 Training loss: 0.0349 Explore P: 0.1182\n",
      "Episode: 738 Step: 221700 Total reward: 4.0 Training loss: 0.0097 Explore P: 0.1178\n",
      "Episode: 739 Step: 222000 Total reward: 3.0 Training loss: 0.0221 Explore P: 0.1175\n",
      "Episode: 740 Step: 222300 Total reward: 3.0 Training loss: 0.0379 Explore P: 0.1172\n",
      "Episode: 741 Step: 222600 Total reward: 12.0 Training loss: 0.0256 Explore P: 0.1169\n",
      "Episode: 742 Step: 222900 Total reward: 5.0 Training loss: 0.0438 Explore P: 0.1166\n",
      "Episode: 743 Step: 223200 Total reward: 12.0 Training loss: 0.0153 Explore P: 0.1162\n",
      "Episode: 744 Step: 223500 Total reward: 8.0 Training loss: 0.6150 Explore P: 0.1159\n",
      "Episode: 745 Step: 223800 Total reward: 8.0 Training loss: 0.0149 Explore P: 0.1156\n",
      "Episode: 746 Step: 224100 Total reward: 9.0 Training loss: 0.0163 Explore P: 0.1153\n",
      "Episode: 747 Step: 224400 Total reward: 7.0 Training loss: 0.0175 Explore P: 0.1150\n",
      "Episode: 748 Step: 224700 Total reward: 11.0 Training loss: 0.0299 Explore P: 0.1147\n",
      "Episode: 749 Step: 225000 Total reward: 7.0 Training loss: 0.0199 Explore P: 0.1143\n",
      "Episode: 750 Step: 225300 Total reward: 1.0 Training loss: 0.0132 Explore P: 0.1140\n",
      "Episode: 751 Step: 225600 Total reward: 7.0 Training loss: 0.0476 Explore P: 0.1137\n",
      "Episode: 752 Step: 225900 Total reward: 16.0 Training loss: 0.0107 Explore P: 0.1134\n",
      "Episode: 753 Step: 226200 Total reward: 8.0 Training loss: 0.0094 Explore P: 0.1131\n",
      "Episode: 754 Step: 226500 Total reward: 14.0 Training loss: 0.0104 Explore P: 0.1128\n",
      "Episode: 755 Step: 226800 Total reward: 9.0 Training loss: 0.0093 Explore P: 0.1125\n",
      "Episode: 756 Step: 227100 Total reward: 14.0 Training loss: 0.0063 Explore P: 0.1122\n",
      "Episode: 757 Step: 227400 Total reward: 9.0 Training loss: 0.0517 Explore P: 0.1119\n",
      "Episode: 758 Step: 227700 Total reward: 2.0 Training loss: 0.0098 Explore P: 0.1116\n",
      "Episode: 759 Step: 228000 Total reward: 11.0 Training loss: 0.6247 Explore P: 0.1113\n",
      "Episode: 760 Step: 228300 Total reward: 9.0 Training loss: 0.0107 Explore P: 0.1110\n",
      "Episode: 761 Step: 228600 Total reward: 15.0 Training loss: 0.0198 Explore P: 0.1107\n",
      "Episode: 762 Step: 228900 Total reward: 14.0 Training loss: 0.0184 Explore P: 0.1104\n",
      "Episode: 763 Step: 229200 Total reward: 4.0 Training loss: 0.0255 Explore P: 0.1101\n",
      "Episode: 764 Step: 229500 Total reward: 8.0 Training loss: 0.0244 Explore P: 0.1098\n",
      "Episode: 765 Step: 229800 Total reward: 0.0 Training loss: 0.0106 Explore P: 0.1095\n",
      "Episode: 766 Step: 230100 Total reward: 6.0 Training loss: 0.0145 Explore P: 0.1092\n",
      "Episode: 767 Step: 230400 Total reward: 4.0 Training loss: 0.6473 Explore P: 0.1089\n",
      "Episode: 768 Step: 230700 Total reward: 6.0 Training loss: 0.0131 Explore P: 0.1086\n",
      "Episode: 769 Step: 231000 Total reward: 10.0 Training loss: 0.0140 Explore P: 0.1083\n",
      "Episode: 770 Step: 231300 Total reward: 5.0 Training loss: 1.3096 Explore P: 0.1080\n",
      "Episode: 771 Step: 231600 Total reward: 9.0 Training loss: 0.0106 Explore P: 0.1077\n",
      "Episode: 772 Step: 231900 Total reward: 9.0 Training loss: 0.0155 Explore P: 0.1074\n",
      "Episode: 773 Step: 232200 Total reward: 1.0 Training loss: 0.0087 Explore P: 0.1071\n",
      "Episode: 774 Step: 232500 Total reward: 4.0 Training loss: 0.0257 Explore P: 0.1068\n",
      "Episode: 775 Step: 232800 Total reward: 11.0 Training loss: 0.0219 Explore P: 0.1065\n",
      "Episode: 776 Step: 233100 Total reward: 7.0 Training loss: 0.0264 Explore P: 0.1062\n",
      "Episode: 777 Step: 233400 Total reward: 2.0 Training loss: 0.0106 Explore P: 0.1059\n",
      "Episode: 778 Step: 233700 Total reward: 4.0 Training loss: 0.0086 Explore P: 0.1057\n",
      "Episode: 779 Step: 234000 Total reward: 3.0 Training loss: 0.7748 Explore P: 0.1054\n",
      "Episode: 780 Step: 234300 Total reward: 6.0 Training loss: 0.0136 Explore P: 0.1051\n",
      "Episode: 781 Step: 234600 Total reward: 10.0 Training loss: 0.0172 Explore P: 0.1048\n",
      "Episode: 782 Step: 234900 Total reward: 9.0 Training loss: 0.6618 Explore P: 0.1045\n",
      "Episode: 783 Step: 235200 Total reward: 12.0 Training loss: 0.0279 Explore P: 0.1042\n",
      "Episode: 784 Step: 235500 Total reward: 8.0 Training loss: 0.0152 Explore P: 0.1039\n",
      "Episode: 785 Step: 235800 Total reward: 12.0 Training loss: 0.0188 Explore P: 0.1037\n",
      "Episode: 786 Step: 236100 Total reward: 5.0 Training loss: 0.0165 Explore P: 0.1034\n",
      "Episode: 787 Step: 236400 Total reward: 8.0 Training loss: 0.0120 Explore P: 0.1031\n",
      "Episode: 788 Step: 236700 Total reward: 9.0 Training loss: 0.0091 Explore P: 0.1028\n",
      "Episode: 789 Step: 237000 Total reward: 10.0 Training loss: 0.0102 Explore P: 0.1025\n",
      "Episode: 790 Step: 237300 Total reward: 1.0 Training loss: 0.0119 Explore P: 0.1023\n",
      "Episode: 791 Step: 237600 Total reward: 6.0 Training loss: 0.0107 Explore P: 0.1020\n",
      "Episode: 792 Step: 237900 Total reward: 12.0 Training loss: 0.0202 Explore P: 0.1017\n",
      "Episode: 793 Step: 238200 Total reward: 13.0 Training loss: 0.6034 Explore P: 0.1014\n",
      "Episode: 794 Step: 238500 Total reward: 6.0 Training loss: 0.0182 Explore P: 0.1012\n",
      "Episode: 795 Step: 238800 Total reward: 6.0 Training loss: 0.0277 Explore P: 0.1009\n",
      "Episode: 796 Step: 239100 Total reward: 5.0 Training loss: 0.0099 Explore P: 0.1006\n",
      "Episode: 797 Step: 239400 Total reward: 7.0 Training loss: 0.0086 Explore P: 0.1004\n",
      "Episode: 798 Step: 239700 Total reward: 9.0 Training loss: 0.0131 Explore P: 0.1001\n",
      "Episode: 799 Step: 240000 Total reward: 8.0 Training loss: 0.0168 Explore P: 0.0998\n",
      "Episode: 800 Step: 240300 Total reward: 2.0 Training loss: 1.1897 Explore P: 0.0995\n",
      "[[6.0154476 6.0355964 6.1841817 6.0972447]\n",
      " [6.0037045 6.0710373 6.207693  6.201552 ]\n",
      " [5.885743  6.0175886 6.2170253 6.166556 ]\n",
      " [6.4016256 6.3273253 6.3677244 6.3071733]\n",
      " [6.0377407 6.042087  6.1986003 6.279927 ]\n",
      " [5.7594976 5.9139504 6.0428557 5.9693117]\n",
      " [6.321051  6.256843  6.1943784 6.4163494]\n",
      " [6.26192   6.2308526 6.105729  6.3285565]\n",
      " [5.7038507 5.8122263 5.8931575 6.0453076]\n",
      " [6.0268946 6.053346  6.091162  6.194733 ]\n",
      " [5.8130703 5.7922034 5.774063  6.0673127]\n",
      " [6.1111426 6.13579   5.993057  6.103593 ]\n",
      " [5.828055  5.902904  5.938026  6.112997 ]\n",
      " [5.917965  5.993253  5.9930024 6.1909857]\n",
      " [5.838695  5.994693  5.9023643 6.0980334]\n",
      " [6.615856  6.456567  6.3580785 6.7004647]\n",
      " [6.3247766 6.333644  6.446982  6.303012 ]\n",
      " [6.333661  6.3660717 6.365061  6.4262586]\n",
      " [6.014218  6.0530667 6.0083275 6.1619287]\n",
      " [5.90108   6.003595  6.1021986 6.2131314]\n",
      " [5.6113772 5.72673   5.7254806 5.86804  ]\n",
      " [6.014565  6.046587  6.1505637 6.3229012]\n",
      " [6.557017  6.3814197 6.2979703 6.5959716]\n",
      " [5.884     5.978525  5.942326  6.071632 ]\n",
      " [6.044517  6.092441  6.047898  6.276036 ]\n",
      " [5.743983  5.820791  5.9605417 6.0276284]\n",
      " [5.895916  6.0295086 6.0304084 6.197847 ]\n",
      " [5.7725697 5.908471  5.8796496 6.002478 ]\n",
      " [5.9812527 6.0270147 6.0112205 6.121698 ]\n",
      " [5.90067   5.93279   5.9641747 6.1272407]\n",
      " [6.097488  6.107654  6.2468963 6.23006  ]\n",
      " [5.7092366 5.860711  5.8798413 5.9164453]\n",
      " [5.9266677 6.001351  5.939018  6.124912 ]\n",
      " [6.364055  6.355008  6.2832646 6.3292813]\n",
      " [6.1164293 6.098693  6.1255345 6.2329054]\n",
      " [5.991329  6.1200023 6.0083404 6.178001 ]\n",
      " [6.328397  6.239812  6.3015976 6.2989073]\n",
      " [5.9935102 6.0541625 6.178551  6.2089047]\n",
      " [5.969255  6.0317345 6.1362214 6.239412 ]\n",
      " [5.8050904 5.934888  5.9197764 6.049063 ]\n",
      " [5.921344  6.0408926 6.099117  6.24435  ]\n",
      " [6.6180687 6.4630146 6.662645  6.5319624]\n",
      " [5.7070436 5.841558  5.8926835 5.956692 ]\n",
      " [6.043638  6.039462  6.1510854 6.166696 ]\n",
      " [5.875318  5.9090877 6.0079613 6.140984 ]\n",
      " [6.153233  6.1756005 6.143231  6.306824 ]\n",
      " [5.6687384 5.81243   5.8080373 5.992516 ]\n",
      " [5.8589272 6.002215  5.904834  6.086084 ]\n",
      " [5.879812  5.9235754 6.1099854 6.097042 ]\n",
      " [5.736946  5.914252  5.8549767 6.0476146]\n",
      " [5.7649064 5.8495493 5.8899674 6.017241 ]\n",
      " [5.8434935 5.995257  5.949347  6.1241   ]\n",
      " [6.4014473 6.4342403 6.5207286 6.3652816]\n",
      " [5.816765  5.9254904 5.9485083 6.0333943]\n",
      " [5.7380285 5.8513145 5.940501  6.0096145]\n",
      " [5.6359863 5.7231245 5.79078   5.8997483]\n",
      " [5.7494116 5.8899007 5.90148   6.0377946]\n",
      " [5.743589  5.873271  5.9218473 5.9291034]\n",
      " [5.8283706 5.906866  5.970076  6.0571156]\n",
      " [6.260712  6.2304845 6.1735473 6.4216695]\n",
      " [5.49118   5.8423944 5.7397766 5.893747 ]\n",
      " [6.1267476 6.2039933 6.19173   6.232324 ]\n",
      " [5.8313656 5.9591484 5.917349  6.2222166]\n",
      " [5.951988  6.0721035 5.951264  6.0632706]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 801 Step: 240600 Total reward: 15.0 Training loss: 0.0073 Explore P: 0.0993\n",
      "Episode: 802 Step: 240900 Total reward: 7.0 Training loss: 0.7205 Explore P: 0.0990\n",
      "Episode: 803 Step: 241200 Total reward: 10.0 Training loss: 0.0133 Explore P: 0.0987\n",
      "Episode: 804 Step: 241500 Total reward: 2.0 Training loss: 0.0183 Explore P: 0.0985\n",
      "Episode: 805 Step: 241800 Total reward: 10.0 Training loss: 0.0113 Explore P: 0.0982\n",
      "Episode: 806 Step: 242100 Total reward: 8.0 Training loss: 0.0081 Explore P: 0.0979\n",
      "Episode: 807 Step: 242400 Total reward: 8.0 Training loss: 0.7574 Explore P: 0.0977\n",
      "Episode: 808 Step: 242700 Total reward: 2.0 Training loss: 0.0080 Explore P: 0.0974\n",
      "Episode: 809 Step: 243000 Total reward: 4.0 Training loss: 0.0395 Explore P: 0.0972\n",
      "Episode: 810 Step: 243300 Total reward: 10.0 Training loss: 0.0274 Explore P: 0.0969\n",
      "Episode: 811 Step: 243600 Total reward: 7.0 Training loss: 0.0196 Explore P: 0.0966\n",
      "Episode: 812 Step: 243900 Total reward: 9.0 Training loss: 0.0649 Explore P: 0.0964\n",
      "Episode: 813 Step: 244200 Total reward: 11.0 Training loss: 0.0264 Explore P: 0.0961\n",
      "Episode: 814 Step: 244500 Total reward: 7.0 Training loss: 0.6676 Explore P: 0.0959\n",
      "Episode: 815 Step: 244800 Total reward: 11.0 Training loss: 0.0108 Explore P: 0.0956\n",
      "Episode: 816 Step: 245100 Total reward: 5.0 Training loss: 0.0236 Explore P: 0.0953\n",
      "Episode: 817 Step: 245400 Total reward: 3.0 Training loss: 0.0106 Explore P: 0.0951\n",
      "Episode: 818 Step: 245700 Total reward: 9.0 Training loss: 0.6868 Explore P: 0.0948\n",
      "Episode: 819 Step: 246000 Total reward: 4.0 Training loss: 0.0379 Explore P: 0.0946\n",
      "Episode: 820 Step: 246300 Total reward: 8.0 Training loss: 0.0236 Explore P: 0.0943\n",
      "Episode: 821 Step: 246600 Total reward: 9.0 Training loss: 0.0110 Explore P: 0.0941\n",
      "Episode: 822 Step: 246900 Total reward: 3.0 Training loss: 0.0255 Explore P: 0.0938\n",
      "Episode: 823 Step: 247200 Total reward: 7.0 Training loss: 0.0136 Explore P: 0.0936\n",
      "Episode: 824 Step: 247500 Total reward: 8.0 Training loss: 0.0148 Explore P: 0.0933\n",
      "Episode: 825 Step: 247800 Total reward: 10.0 Training loss: 0.0147 Explore P: 0.0931\n",
      "Episode: 826 Step: 248100 Total reward: 13.0 Training loss: 0.0111 Explore P: 0.0928\n",
      "Episode: 827 Step: 248400 Total reward: 6.0 Training loss: 0.6752 Explore P: 0.0926\n",
      "Episode: 828 Step: 248700 Total reward: 11.0 Training loss: 0.6498 Explore P: 0.0923\n",
      "Episode: 829 Step: 249000 Total reward: 9.0 Training loss: 0.0217 Explore P: 0.0921\n",
      "Episode: 830 Step: 249300 Total reward: 10.0 Training loss: 0.0127 Explore P: 0.0918\n",
      "Episode: 831 Step: 249600 Total reward: 12.0 Training loss: 0.0274 Explore P: 0.0916\n",
      "Episode: 832 Step: 249900 Total reward: 10.0 Training loss: 0.0082 Explore P: 0.0913\n",
      "Episode: 833 Step: 250200 Total reward: 9.0 Training loss: 0.0068 Explore P: 0.0911\n",
      "Episode: 834 Step: 250500 Total reward: 9.0 Training loss: 0.0097 Explore P: 0.0909\n",
      "Episode: 835 Step: 250800 Total reward: 4.0 Training loss: 0.0194 Explore P: 0.0906\n",
      "Episode: 836 Step: 251100 Total reward: 4.0 Training loss: 0.0105 Explore P: 0.0904\n",
      "Episode: 837 Step: 251400 Total reward: 10.0 Training loss: 0.0136 Explore P: 0.0901\n",
      "Episode: 838 Step: 251700 Total reward: 8.0 Training loss: 0.6103 Explore P: 0.0899\n",
      "Episode: 839 Step: 252000 Total reward: 12.0 Training loss: 0.0233 Explore P: 0.0897\n",
      "Episode: 840 Step: 252300 Total reward: 9.0 Training loss: 0.0119 Explore P: 0.0894\n",
      "Episode: 841 Step: 252600 Total reward: 5.0 Training loss: 0.0123 Explore P: 0.0892\n",
      "Episode: 842 Step: 252900 Total reward: 10.0 Training loss: 0.0305 Explore P: 0.0889\n",
      "Episode: 843 Step: 253200 Total reward: 8.0 Training loss: 0.0207 Explore P: 0.0887\n",
      "Episode: 844 Step: 253500 Total reward: 4.0 Training loss: 0.0211 Explore P: 0.0885\n",
      "Episode: 845 Step: 253800 Total reward: 5.0 Training loss: 0.0063 Explore P: 0.0882\n",
      "Episode: 846 Step: 254100 Total reward: 11.0 Training loss: 0.0134 Explore P: 0.0880\n",
      "Episode: 847 Step: 254400 Total reward: 18.0 Training loss: 0.0252 Explore P: 0.0878\n",
      "Episode: 848 Step: 254700 Total reward: 8.0 Training loss: 0.0223 Explore P: 0.0875\n",
      "Episode: 849 Step: 255000 Total reward: 9.0 Training loss: 0.0108 Explore P: 0.0873\n",
      "Episode: 850 Step: 255300 Total reward: 4.0 Training loss: 0.0227 Explore P: 0.0871\n",
      "Episode: 851 Step: 255600 Total reward: 5.0 Training loss: 0.6249 Explore P: 0.0868\n",
      "Episode: 852 Step: 255900 Total reward: 6.0 Training loss: 0.0196 Explore P: 0.0866\n",
      "Episode: 853 Step: 256200 Total reward: 13.0 Training loss: 0.5581 Explore P: 0.0864\n",
      "Episode: 854 Step: 256500 Total reward: 6.0 Training loss: 0.0078 Explore P: 0.0861\n",
      "Episode: 855 Step: 256800 Total reward: 11.0 Training loss: 0.0073 Explore P: 0.0859\n",
      "Episode: 856 Step: 257100 Total reward: 7.0 Training loss: 0.0530 Explore P: 0.0857\n",
      "Episode: 857 Step: 257400 Total reward: 8.0 Training loss: 0.0100 Explore P: 0.0855\n",
      "Episode: 858 Step: 257700 Total reward: 9.0 Training loss: 0.0106 Explore P: 0.0852\n",
      "Episode: 859 Step: 258000 Total reward: 10.0 Training loss: 0.0156 Explore P: 0.0850\n",
      "Episode: 860 Step: 258300 Total reward: 11.0 Training loss: 0.0111 Explore P: 0.0848\n",
      "Episode: 861 Step: 258600 Total reward: 18.0 Training loss: 0.0324 Explore P: 0.0846\n",
      "Episode: 862 Step: 258900 Total reward: 9.0 Training loss: 0.0156 Explore P: 0.0843\n",
      "Episode: 863 Step: 259200 Total reward: 5.0 Training loss: 0.0125 Explore P: 0.0841\n",
      "Episode: 864 Step: 259500 Total reward: 3.0 Training loss: 0.6061 Explore P: 0.0839\n",
      "Episode: 865 Step: 259800 Total reward: 8.0 Training loss: 0.0218 Explore P: 0.0837\n",
      "Episode: 866 Step: 260100 Total reward: 9.0 Training loss: 0.0211 Explore P: 0.0835\n",
      "Episode: 867 Step: 260400 Total reward: 8.0 Training loss: 0.0135 Explore P: 0.0832\n",
      "Episode: 868 Step: 260700 Total reward: 10.0 Training loss: 0.0111 Explore P: 0.0830\n",
      "Episode: 869 Step: 261000 Total reward: 8.0 Training loss: 0.0085 Explore P: 0.0828\n",
      "Episode: 870 Step: 261300 Total reward: 8.0 Training loss: 0.0062 Explore P: 0.0826\n",
      "Episode: 871 Step: 261600 Total reward: 8.0 Training loss: 0.0118 Explore P: 0.0824\n",
      "Episode: 872 Step: 261900 Total reward: 6.0 Training loss: 0.0056 Explore P: 0.0821\n",
      "Episode: 873 Step: 262200 Total reward: 3.0 Training loss: 0.0087 Explore P: 0.0819\n",
      "Episode: 874 Step: 262500 Total reward: 8.0 Training loss: 0.0259 Explore P: 0.0817\n",
      "Episode: 875 Step: 262800 Total reward: 11.0 Training loss: 0.7142 Explore P: 0.0815\n",
      "Episode: 876 Step: 263100 Total reward: 9.0 Training loss: 0.0057 Explore P: 0.0813\n",
      "Episode: 877 Step: 263400 Total reward: 6.0 Training loss: 0.0412 Explore P: 0.0811\n",
      "Episode: 878 Step: 263700 Total reward: 4.0 Training loss: 0.0121 Explore P: 0.0809\n",
      "Episode: 879 Step: 264000 Total reward: 12.0 Training loss: 0.0119 Explore P: 0.0806\n",
      "Episode: 880 Step: 264300 Total reward: 4.0 Training loss: 0.0254 Explore P: 0.0804\n",
      "Episode: 881 Step: 264600 Total reward: 10.0 Training loss: 0.0199 Explore P: 0.0802\n",
      "Episode: 882 Step: 264900 Total reward: 11.0 Training loss: 0.0220 Explore P: 0.0800\n",
      "Episode: 883 Step: 265200 Total reward: 10.0 Training loss: 0.0214 Explore P: 0.0798\n",
      "Episode: 884 Step: 265500 Total reward: 8.0 Training loss: 0.0092 Explore P: 0.0796\n",
      "Episode: 885 Step: 265800 Total reward: 1.0 Training loss: 0.0065 Explore P: 0.0794\n",
      "Episode: 886 Step: 266100 Total reward: 7.0 Training loss: 0.0263 Explore P: 0.0792\n",
      "Episode: 887 Step: 266400 Total reward: 12.0 Training loss: 0.0126 Explore P: 0.0790\n",
      "Episode: 888 Step: 266700 Total reward: 7.0 Training loss: 0.0069 Explore P: 0.0788\n",
      "Episode: 889 Step: 267000 Total reward: 9.0 Training loss: 0.6461 Explore P: 0.0786\n",
      "Episode: 890 Step: 267300 Total reward: 3.0 Training loss: 0.0088 Explore P: 0.0784\n",
      "Episode: 891 Step: 267600 Total reward: 11.0 Training loss: 0.0193 Explore P: 0.0781\n",
      "Episode: 892 Step: 267900 Total reward: 10.0 Training loss: 0.0218 Explore P: 0.0779\n",
      "Episode: 893 Step: 268200 Total reward: 8.0 Training loss: 0.0134 Explore P: 0.0777\n",
      "Episode: 894 Step: 268500 Total reward: 7.0 Training loss: 0.0157 Explore P: 0.0775\n",
      "Episode: 895 Step: 268800 Total reward: 17.0 Training loss: 0.0064 Explore P: 0.0773\n",
      "Episode: 896 Step: 269100 Total reward: 6.0 Training loss: 0.0257 Explore P: 0.0771\n",
      "Episode: 897 Step: 269400 Total reward: 2.0 Training loss: 0.0100 Explore P: 0.0769\n",
      "Episode: 898 Step: 269700 Total reward: 4.0 Training loss: 0.0133 Explore P: 0.0767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 899 Step: 270000 Total reward: 8.0 Training loss: 0.0237 Explore P: 0.0765\n",
      "Episode: 900 Step: 270300 Total reward: 8.0 Training loss: 0.0230 Explore P: 0.0763\n",
      "[[6.2342644 6.2780814 6.3764734 6.304111 ]\n",
      " [7.0287304 6.946236  6.884027  6.9791727]\n",
      " [6.732742  6.651694  6.5572276 6.706773 ]\n",
      " [6.484576  6.4636035 6.6676445 6.5562277]\n",
      " [6.709205  6.648992  6.5243134 6.7498736]\n",
      " [6.3245296 6.4577303 6.47974   6.4938655]\n",
      " [6.4721847 6.5750275 6.557279  6.5728745]\n",
      " [6.7713146 6.7090626 6.757108  6.6826115]\n",
      " [6.4175267 6.4988494 6.539616  6.497197 ]\n",
      " [6.3582    6.4573517 6.460789  6.5458055]\n",
      " [6.504364  6.5195293 6.6154866 6.5664034]\n",
      " [6.165149  6.2635145 6.240793  6.343864 ]\n",
      " [6.450278  6.489879  6.634521  6.507358 ]\n",
      " [6.4561906 6.5473213 6.701424  6.496718 ]\n",
      " [6.0114517 6.344222  6.2417293 6.3820825]\n",
      " [6.24392   6.330599  6.353925  6.5063515]\n",
      " [6.736348  6.779067  6.577601  6.9418015]\n",
      " [6.48073   6.5300736 6.509212  6.5505404]\n",
      " [6.2600794 6.303651  6.3366656 6.4237075]\n",
      " [6.3734145 6.476549  6.43461   6.4709945]\n",
      " [6.477938  6.5985565 6.5610285 6.719152 ]\n",
      " [6.549292  6.6018105 6.533923  6.5969777]\n",
      " [6.377092  6.485533  6.5219083 6.5226197]\n",
      " [7.0119123 6.852249  6.63709   6.9732   ]\n",
      " [6.1780963 6.273761  6.4033556 6.3986783]\n",
      " [6.0820265 6.218103  6.2241178 6.3136873]\n",
      " [6.2587585 6.425817  6.497064  6.60253  ]\n",
      " [6.310718  6.426515  6.471434  6.497807 ]\n",
      " [6.6478333 6.6358614 6.54325   6.6772523]\n",
      " [6.1208186 6.2757344 6.343653  6.3846354]\n",
      " [6.197604  6.2831383 6.293075  6.354297 ]\n",
      " [6.118984  6.2684984 6.3410497 6.311255 ]\n",
      " [6.227202  6.3192816 6.4045453 6.3789744]\n",
      " [6.1950583 6.349338  6.4116387 6.3244004]\n",
      " [7.0948796 6.9805827 6.943967  6.8874683]\n",
      " [6.3785367 6.4996433 6.60921   6.524223 ]\n",
      " [6.2277675 6.325622  6.421593  6.273148 ]\n",
      " [6.1733527 6.316207  6.375768  6.307305 ]\n",
      " [6.3383985 6.4387794 6.654899  6.388925 ]\n",
      " [6.976406  6.778933  6.765722  6.747754 ]\n",
      " [6.2747865 6.3722606 6.463883  6.4932957]\n",
      " [6.1739626 6.267335  6.370901  6.3690515]\n",
      " [6.292894  6.3762207 6.5231147 6.3606663]\n",
      " [6.1436925 6.2428465 6.290385  6.3658185]\n",
      " [6.1789064 6.3526707 6.4329305 6.4196353]\n",
      " [6.3326154 6.370028  6.3953295 6.4655976]\n",
      " [6.351593  6.4061913 6.555468  6.5141377]\n",
      " [6.7256055 6.6641464 6.5909986 6.684832 ]\n",
      " [6.2296762 6.257603  6.3203015 6.435116 ]\n",
      " [6.2780967 6.397521  6.444182  6.4482093]\n",
      " [6.4772763 6.5251307 6.4783106 6.752213 ]\n",
      " [6.3896694 6.4339123 6.4205394 6.5471196]\n",
      " [6.8367767 6.7254295 6.6717153 6.628341 ]\n",
      " [6.286221  6.3871183 6.357931  6.4473176]\n",
      " [6.4091673 6.5153966 6.5891604 6.4326444]\n",
      " [6.935463  6.811146  6.7726955 6.8776093]\n",
      " [6.7162232 6.70171   6.641996  6.547876 ]\n",
      " [7.0243516 6.966661  6.932878  6.850741 ]\n",
      " [7.0080094 6.8759584 7.02311   6.8485394]\n",
      " [6.366486  6.4949694 6.4530396 6.495554 ]\n",
      " [6.1985526 6.2791    6.257108  6.3933854]\n",
      " [6.0301857 6.364855  6.289675  6.375815 ]\n",
      " [6.2357383 6.3456316 6.298788  6.425534 ]\n",
      " [7.005721  6.9156103 6.74898   6.935841 ]]\n",
      "Episode: 901 Step: 270600 Total reward: 17.0 Training loss: 0.0157 Explore P: 0.0761\n",
      "Episode: 902 Step: 270900 Total reward: 11.0 Training loss: 0.0253 Explore P: 0.0759\n",
      "Episode: 903 Step: 271200 Total reward: 6.0 Training loss: 0.0068 Explore P: 0.0757\n",
      "Episode: 904 Step: 271500 Total reward: 12.0 Training loss: 1.1363 Explore P: 0.0755\n",
      "Episode: 905 Step: 271800 Total reward: 9.0 Training loss: 0.0164 Explore P: 0.0753\n",
      "Episode: 906 Step: 272100 Total reward: 9.0 Training loss: 0.0057 Explore P: 0.0752\n",
      "Episode: 907 Step: 272400 Total reward: -2.0 Training loss: 0.6430 Explore P: 0.0750\n",
      "Episode: 908 Step: 272700 Total reward: 12.0 Training loss: 0.0116 Explore P: 0.0748\n",
      "Episode: 909 Step: 273000 Total reward: 4.0 Training loss: 0.0077 Explore P: 0.0746\n",
      "Episode: 910 Step: 273300 Total reward: 3.0 Training loss: 0.0230 Explore P: 0.0744\n",
      "Episode: 911 Step: 273600 Total reward: 15.0 Training loss: 0.0225 Explore P: 0.0742\n",
      "Episode: 912 Step: 273900 Total reward: 10.0 Training loss: 0.0173 Explore P: 0.0740\n",
      "Episode: 913 Step: 274200 Total reward: 5.0 Training loss: 0.0253 Explore P: 0.0738\n",
      "Episode: 914 Step: 274500 Total reward: 8.0 Training loss: 0.0229 Explore P: 0.0736\n",
      "Episode: 915 Step: 274800 Total reward: 13.0 Training loss: 0.0110 Explore P: 0.0734\n",
      "Episode: 916 Step: 275100 Total reward: 11.0 Training loss: 0.8949 Explore P: 0.0732\n",
      "Episode: 917 Step: 275400 Total reward: 8.0 Training loss: 0.0106 Explore P: 0.0730\n",
      "Episode: 918 Step: 275700 Total reward: 10.0 Training loss: 0.0113 Explore P: 0.0728\n",
      "Episode: 919 Step: 276000 Total reward: 13.0 Training loss: 0.0364 Explore P: 0.0727\n",
      "Episode: 920 Step: 276300 Total reward: 8.0 Training loss: 0.0109 Explore P: 0.0725\n",
      "Episode: 921 Step: 276600 Total reward: 3.0 Training loss: 0.0065 Explore P: 0.0723\n",
      "Episode: 922 Step: 276900 Total reward: 8.0 Training loss: 0.0091 Explore P: 0.0721\n",
      "Episode: 923 Step: 277200 Total reward: 8.0 Training loss: 0.0137 Explore P: 0.0719\n",
      "Episode: 924 Step: 277500 Total reward: 3.0 Training loss: 0.0224 Explore P: 0.0717\n",
      "Episode: 925 Step: 277800 Total reward: 9.0 Training loss: 0.0086 Explore P: 0.0715\n",
      "Episode: 926 Step: 278100 Total reward: 6.0 Training loss: 0.0162 Explore P: 0.0714\n",
      "Episode: 927 Step: 278400 Total reward: 8.0 Training loss: 0.0105 Explore P: 0.0712\n",
      "Episode: 928 Step: 278700 Total reward: 8.0 Training loss: 0.0292 Explore P: 0.0710\n",
      "Episode: 929 Step: 279000 Total reward: 11.0 Training loss: 0.0097 Explore P: 0.0708\n",
      "Episode: 930 Step: 279300 Total reward: 10.0 Training loss: 0.0148 Explore P: 0.0706\n",
      "Episode: 931 Step: 279600 Total reward: 6.0 Training loss: 0.0156 Explore P: 0.0704\n",
      "Episode: 932 Step: 279900 Total reward: 6.0 Training loss: 0.0416 Explore P: 0.0703\n",
      "Episode: 933 Step: 280200 Total reward: 11.0 Training loss: 0.0176 Explore P: 0.0701\n",
      "Episode: 934 Step: 280500 Total reward: 5.0 Training loss: 0.0081 Explore P: 0.0699\n",
      "Episode: 935 Step: 280800 Total reward: 2.0 Training loss: 0.0145 Explore P: 0.0697\n",
      "Episode: 936 Step: 281100 Total reward: 8.0 Training loss: 0.0146 Explore P: 0.0695\n",
      "Episode: 937 Step: 281400 Total reward: 14.0 Training loss: 0.6154 Explore P: 0.0694\n",
      "Episode: 938 Step: 281700 Total reward: 13.0 Training loss: 0.6718 Explore P: 0.0692\n",
      "Episode: 939 Step: 282000 Total reward: 4.0 Training loss: 0.0175 Explore P: 0.0690\n",
      "Episode: 940 Step: 282300 Total reward: 5.0 Training loss: 0.0131 Explore P: 0.0688\n",
      "Episode: 941 Step: 282600 Total reward: 14.0 Training loss: 0.0191 Explore P: 0.0687\n",
      "Episode: 942 Step: 282900 Total reward: 9.0 Training loss: 0.0061 Explore P: 0.0685\n",
      "Episode: 943 Step: 283200 Total reward: 4.0 Training loss: 0.0156 Explore P: 0.0683\n",
      "Episode: 944 Step: 283500 Total reward: 6.0 Training loss: 0.0121 Explore P: 0.0681\n",
      "Episode: 945 Step: 283800 Total reward: 16.0 Training loss: 0.6731 Explore P: 0.0680\n",
      "Episode: 946 Step: 284100 Total reward: 4.0 Training loss: 0.0108 Explore P: 0.0678\n",
      "Episode: 947 Step: 284400 Total reward: 7.0 Training loss: 0.0290 Explore P: 0.0676\n",
      "Episode: 948 Step: 284700 Total reward: 12.0 Training loss: 0.0133 Explore P: 0.0674\n",
      "Episode: 949 Step: 285000 Total reward: 4.0 Training loss: 0.0106 Explore P: 0.0673\n",
      "Episode: 950 Step: 285300 Total reward: 16.0 Training loss: 0.0288 Explore P: 0.0671\n",
      "Episode: 951 Step: 285600 Total reward: 2.0 Training loss: 0.0088 Explore P: 0.0669\n",
      "Episode: 952 Step: 285900 Total reward: 8.0 Training loss: 0.0092 Explore P: 0.0668\n",
      "Episode: 953 Step: 286200 Total reward: 6.0 Training loss: 1.4616 Explore P: 0.0666\n",
      "Episode: 954 Step: 286500 Total reward: 11.0 Training loss: 0.0202 Explore P: 0.0664\n",
      "Episode: 955 Step: 286800 Total reward: 7.0 Training loss: 0.0133 Explore P: 0.0662\n",
      "Episode: 956 Step: 287100 Total reward: 13.0 Training loss: 0.0119 Explore P: 0.0661\n",
      "Episode: 957 Step: 287400 Total reward: 8.0 Training loss: 0.6413 Explore P: 0.0659\n",
      "Episode: 958 Step: 287700 Total reward: 13.0 Training loss: 0.0128 Explore P: 0.0657\n",
      "Episode: 959 Step: 288000 Total reward: 13.0 Training loss: 0.0089 Explore P: 0.0656\n",
      "Episode: 960 Step: 288300 Total reward: 3.0 Training loss: 0.0112 Explore P: 0.0654\n",
      "Episode: 961 Step: 288600 Total reward: 11.0 Training loss: 0.7420 Explore P: 0.0652\n",
      "Episode: 962 Step: 288900 Total reward: 6.0 Training loss: 1.3575 Explore P: 0.0651\n",
      "Episode: 963 Step: 289200 Total reward: 5.0 Training loss: 0.0245 Explore P: 0.0649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 964 Step: 289500 Total reward: 6.0 Training loss: 0.0607 Explore P: 0.0647\n",
      "Episode: 965 Step: 289800 Total reward: 7.0 Training loss: 0.0365 Explore P: 0.0646\n",
      "Episode: 966 Step: 290100 Total reward: 14.0 Training loss: 0.0136 Explore P: 0.0644\n",
      "Episode: 967 Step: 290400 Total reward: 9.0 Training loss: 0.0438 Explore P: 0.0643\n",
      "Episode: 968 Step: 290700 Total reward: 12.0 Training loss: 0.0181 Explore P: 0.0641\n",
      "Episode: 969 Step: 291000 Total reward: 5.0 Training loss: 0.6210 Explore P: 0.0639\n",
      "Episode: 970 Step: 291300 Total reward: 7.0 Training loss: 0.0121 Explore P: 0.0638\n",
      "Episode: 971 Step: 291600 Total reward: 11.0 Training loss: 0.0126 Explore P: 0.0636\n",
      "Episode: 972 Step: 291900 Total reward: 2.0 Training loss: 0.7874 Explore P: 0.0634\n",
      "Episode: 973 Step: 292200 Total reward: 17.0 Training loss: 0.0323 Explore P: 0.0633\n",
      "Episode: 974 Step: 292500 Total reward: 9.0 Training loss: 0.6886 Explore P: 0.0631\n",
      "Episode: 975 Step: 292800 Total reward: 3.0 Training loss: 0.0232 Explore P: 0.0630\n",
      "Episode: 976 Step: 293100 Total reward: -2.0 Training loss: 0.0110 Explore P: 0.0628\n",
      "Episode: 977 Step: 293400 Total reward: 7.0 Training loss: 0.0079 Explore P: 0.0627\n",
      "Episode: 978 Step: 293700 Total reward: 13.0 Training loss: 0.0108 Explore P: 0.0625\n",
      "Episode: 979 Step: 294000 Total reward: 11.0 Training loss: 0.6787 Explore P: 0.0623\n",
      "Episode: 980 Step: 294300 Total reward: 1.0 Training loss: 0.0231 Explore P: 0.0622\n",
      "Episode: 981 Step: 294600 Total reward: 8.0 Training loss: 0.0111 Explore P: 0.0620\n",
      "Episode: 982 Step: 294900 Total reward: 12.0 Training loss: 0.0073 Explore P: 0.0619\n",
      "Episode: 983 Step: 295200 Total reward: 7.0 Training loss: 0.0100 Explore P: 0.0617\n",
      "Episode: 984 Step: 295500 Total reward: 10.0 Training loss: 0.0278 Explore P: 0.0616\n",
      "Episode: 985 Step: 295800 Total reward: 8.0 Training loss: 0.0130 Explore P: 0.0614\n",
      "Episode: 986 Step: 296100 Total reward: 1.0 Training loss: 0.0156 Explore P: 0.0612\n",
      "Episode: 987 Step: 296400 Total reward: 8.0 Training loss: 0.0153 Explore P: 0.0611\n",
      "Episode: 988 Step: 296700 Total reward: 15.0 Training loss: 0.0319 Explore P: 0.0609\n",
      "Episode: 989 Step: 297000 Total reward: 5.0 Training loss: 0.0437 Explore P: 0.0608\n",
      "Episode: 990 Step: 297300 Total reward: 15.0 Training loss: 0.0323 Explore P: 0.0606\n",
      "Episode: 991 Step: 297600 Total reward: 5.0 Training loss: 0.0145 Explore P: 0.0605\n",
      "Episode: 992 Step: 297900 Total reward: 7.0 Training loss: 0.0255 Explore P: 0.0603\n",
      "Episode: 993 Step: 298200 Total reward: 4.0 Training loss: 0.6112 Explore P: 0.0602\n",
      "Episode: 994 Step: 298500 Total reward: 10.0 Training loss: 0.0186 Explore P: 0.0600\n",
      "Episode: 995 Step: 298800 Total reward: 5.0 Training loss: 0.0141 Explore P: 0.0599\n",
      "Episode: 996 Step: 299100 Total reward: 3.0 Training loss: 0.0180 Explore P: 0.0597\n",
      "Episode: 997 Step: 299400 Total reward: 8.0 Training loss: 0.0107 Explore P: 0.0596\n",
      "Episode: 998 Step: 299700 Total reward: 9.0 Training loss: 0.0172 Explore P: 0.0594\n",
      "Episode: 999 Step: 300000 Total reward: 5.0 Training loss: 0.0408 Explore P: 0.0593\n",
      "episode: 1 Score: 1.0\n",
      "episode: 2 Score: 0.0\n",
      "episode: 3 Score: 0.0\n",
      "episode: 4 Score: 0.0\n",
      "episode: 5 Score: 0.0\n",
      "episode: 6 Score: 0.0\n",
      "episode: 7 Score: 0.0\n",
      "episode: 8 Score: 0.0\n",
      "episode: 9 Score: 0.0\n",
      "episode: 10 Score: 1.0\n",
      "episode: 11 Score: 0.0\n",
      "episode: 12 Score: 0.0\n",
      "episode: 13 Score: 0.0\n",
      "episode: 14 Score: 0.0\n",
      "episode: 15 Score: 0.0\n",
      "episode: 16 Score: 0.0\n",
      "episode: 17 Score: 0.0\n",
      "episode: 18 Score: 0.0\n",
      "episode: 19 Score: 0.0\n",
      "episode: 20 Score: 0.0\n",
      "episode: 21 Score: 0.0\n",
      "episode: 22 Score: 0.0\n",
      "episode: 23 Score: 0.0\n",
      "episode: 24 Score: 0.0\n",
      "episode: 25 Score: 1.0\n",
      "episode: 26 Score: 0.0\n",
      "episode: 27 Score: 1.0\n",
      "episode: 28 Score: 0.0\n",
      "episode: 29 Score: 0.0\n",
      "episode: 30 Score: 0.0\n",
      "episode: 31 Score: 1.0\n",
      "episode: 32 Score: 0.0\n",
      "episode: 33 Score: 0.0\n",
      "episode: 34 Score: 0.0\n",
      "episode: 35 Score: 0.0\n",
      "episode: 36 Score: 0.0\n",
      "episode: 37 Score: 0.0\n",
      "episode: 38 Score: 0.0\n",
      "episode: 39 Score: 0.0\n",
      "episode: 40 Score: 0.0\n",
      "episode: 41 Score: 0.0\n",
      "episode: 42 Score: 1.0\n",
      "episode: 43 Score: 0.0\n",
      "episode: 44 Score: 0.0\n",
      "episode: 45 Score: 1.0\n",
      "episode: 46 Score: -1.0\n",
      "episode: 47 Score: 0.0\n",
      "episode: 48 Score: 0.0\n",
      "episode: 49 Score: 0.0\n",
      "episode: 50 Score: 0.0\n",
      "episode: 51 Score: 0.0\n",
      "episode: 52 Score: 0.0\n",
      "episode: 53 Score: 0.0\n",
      "episode: 54 Score: -1.0\n",
      "episode: 55 Score: 0.0\n",
      "episode: 56 Score: 0.0\n",
      "episode: 57 Score: 0.0\n",
      "episode: 58 Score: 0.0\n",
      "episode: 59 Score: 0.0\n",
      "episode: 60 Score: 0.0\n",
      "episode: 61 Score: 0.0\n",
      "episode: 62 Score: 0.0\n",
      "episode: 63 Score: 0.0\n",
      "episode: 64 Score: -1.0\n",
      "episode: 65 Score: 0.0\n",
      "episode: 66 Score: 0.0\n",
      "episode: 67 Score: 1.0\n",
      "episode: 68 Score: 0.0\n",
      "episode: 69 Score: 1.0\n",
      "episode: 70 Score: 0.0\n",
      "episode: 71 Score: 0.0\n",
      "episode: 72 Score: 0.0\n",
      "episode: 73 Score: 0.0\n",
      "episode: 74 Score: 0.0\n",
      "episode: 75 Score: 0.0\n",
      "episode: 76 Score: 0.0\n",
      "episode: 77 Score: 0.0\n",
      "episode: 78 Score: 0.0\n",
      "episode: 79 Score: 0.0\n",
      "episode: 80 Score: 0.0\n",
      "episode: 81 Score: 0.0\n",
      "episode: 82 Score: 0.0\n",
      "episode: 83 Score: 0.0\n",
      "episode: 84 Score: 0.0\n",
      "episode: 85 Score: 0.0\n",
      "episode: 86 Score: 0.0\n",
      "episode: 87 Score: 0.0\n",
      "episode: 88 Score: 0.0\n",
      "episode: 89 Score: 1.0\n",
      "episode: 90 Score: 0.0\n",
      "episode: 91 Score: 0.0\n",
      "episode: 92 Score: 0.0\n",
      "episode: 93 Score: 0.0\n",
      "episode: 94 Score: -1.0\n",
      "episode: 95 Score: 0.0\n",
      "episode: 96 Score: 0.0\n",
      "episode: 97 Score: -1.0\n",
      "episode: 98 Score: 0.0\n",
      "episode: 99 Score: 0.0\n",
      "episode: 100 Score: 0.0\n",
      "Average score in 100 episodes: 0.05\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "#with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Needs to be same weights between main and target in start\n",
    "    #copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
    "    #sess.run(copy_ops)\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(0, train_episodes):\n",
    "    #for ep in range(0, 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            \n",
    "            #Select an action with probability e\n",
    "            action, ex_p = predict_action(explore_start, explore_stop, decay_rate, step, state)\n",
    "            \n",
    "            #Execute action\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            #Observe reward and next state\n",
    "            reward = env_info.rewards[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                next_state = np.zeros(state_size)\n",
    "            \n",
    "            #Store transition (st, at, rt, st+1, done) in D\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            \n",
    "            #states = np.array([each[0].reshape(1, *each[0].shape) for each in batch])\n",
    "            states = [each[0] for each in batch]\n",
    "            actions = [each[1] for each in batch]\n",
    "            rewards = [each[2] for each in batch]\n",
    "            next_states = [each[3] for each in batch]\n",
    "            dones = [each[4] for each in batch]\n",
    "            targets = []\n",
    "\n",
    "            #next_Qs = sess.run(targetDQN.output, feed_dict={targetDQN.inputs: next_states})\n",
    "            next_Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: next_states})\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                if dones[i]:\n",
    "                    target = rewards[i]\n",
    "                else:\n",
    "                    target = rewards[i] + gamma * np.max(next_Qs[i])                    \n",
    "                targets.append(target)\n",
    "\n",
    "            loss, _ = sess.run([mainDQN.loss, mainDQN.opt], feed_dict={mainDQN.inputs: states, mainDQN.target_Q: targets, mainDQN.actions: actions})                   \n",
    "\n",
    "            \"\"\"\n",
    "            if step % C == 0:\n",
    "                copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
    "                sess.run(copy_ops)\n",
    "            \"\"\"\n",
    "            if done:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Step: {}'.format(step),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(ex_p))\n",
    "                if ep % 100 == 0:\n",
    "                    #Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: states})\n",
    "                    #print(Qs)\n",
    "                    validation()\n",
    "                rewards_list.append((ep, total_reward))              \n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_file = './layer1_512_weight.ckpt'\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, safe_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 1\n",
    "   - hidden size : 512, performance : \n",
    "Layer 2\n",
    "   - hidden size : \n",
    "Dueling Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation\n",
    "  - The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:    \n",
    "    scores = []\n",
    "    for ep in range(100):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "\n",
    "        score = 0                                          # initialize the score        \n",
    "        while True:\n",
    "            Qs = sess.run(mainDQN.output, feed_dict={mainDQN.inputs: state.reshape((1, *state.shape))})\n",
    "            print(Qs)\n",
    "            action = np.asscalar(np.argmax(Qs[0]))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            #Observe reward and next state\n",
    "            reward = env_info.rewards[0]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        print(\"episode: {}\".format(ep+1), \"Score: {}\".format(score))\n",
    "        scores.append(score)\n",
    "    print(\"Average score in 100 episodes: {}\".format(np.sum(scores) / 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(4)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1",
   "language": "python",
   "name": "p1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
